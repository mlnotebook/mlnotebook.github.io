<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning Notebook</title>
    <link>/index.xml</link>
    <description>Recent content on Machine Learning Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Mar 2017 10:43:07 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Transfer Functions</title>
      <link>/post/transfer-functions/</link>
      <pubDate>Wed, 08 Mar 2017 10:43:07 +0000</pubDate>
      
      <guid>/post/transfer-functions/</guid>
      <description>

&lt;p&gt;As promised in the previous post, we&amp;rsquo;ll take a look at some of the different activation functions that could be used in our nodes. Again &lt;strong&gt;please&lt;/strong&gt; let me know if there&amp;rsquo;s anything I&amp;rsquo;ve gotten totally wrong - I&amp;rsquo;m very much learning too.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#linear&#34;&gt;Linear Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sigmoid&#34;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tanh&#34;&gt;Hyperbolic Tangent Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian&#34;&gt;Gaussian Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step&#34;&gt;Heaviside (step) Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ramp&#34;&gt;Ramp Function&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#relu&#34;&gt;Rectified Linear Unit (ReLU)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;linear&#34;&gt; Linear (Identity) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig1&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/linear.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dlinear.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: The linear function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
f \left( x_{i} \right) = x_{i}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def linear(x, Derivative=False):
    if not Derivative:
        return x
    else:
        return 1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;If there&amp;rsquo;s a situation where we want a node to give its output without applying any thresholds, then the identity (or linear) function is the way to go.&lt;/p&gt;

&lt;p&gt;Hopefully you can see why it is used in the final output layer nodes as we only want these nodes to do the $ \text{input} \times \text{weight}$ operations before giving us its answer without any further modifications.&lt;/p&gt;

&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The linear function is not used in the hidden layers. We must use non-linear transfer functions in the hidden layer nodes or else the output will only ever end up being a linearly separable solution.&lt;/p&gt;

&lt;p&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt; The Sigmoid (or Fermi) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-1&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig2&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/sigmoid.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dsigmoid.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The sigmoid function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-1&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \frac{1}{1 + e^{  - x_{i}  }}, \ \
f^{\prime}\left( x_{i} \right) = \sigma(x_{i}) \left( 1 -  \sigma(x_{i}) \right)
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-1&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(x,Derivative=False):
    if not Derivative:
        return 1 / (1 + np.exp (-x))
    else:
        out = sigmoid(x)
        return out * (1 - out)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-1&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;This function maps the input to a value between 0 and 1 (but not equal to 0 or 1). This means the output from the node will be a high signal (if the input is positive) or a low one (if the input is negative). This function is often chosen as it is one of the easiest to hard-code in terms of its derivative. The simplicity of its derivative allows us to efficiently perform back propagation without using any fancy packages or approximations. The fact that this function is smooth, continuous (differentiable), monotonic and bounded means that back propagation will work well.&lt;/p&gt;

&lt;p&gt;The sigmoid&amp;rsquo;s natural threshold is 0.5, meaning that any input that maps to a value above 0.5 will be considered high (or 1) in binary terms.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;tanh&#34;&gt; Hyperbolic Tangent Function ( $\tanh(x)$ ) &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-2&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig3&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/tanh.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dtanh.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The hyperbolic tangent function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-2&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \tanh\left(x_{i}\right),
f^{\prime}\left(x_{i} \right) = 1 - \tanh\left(x_{i}\right)^{2}
$$&lt;/div&gt;

&lt;h3 id=&#34;why-is-it-used-2&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;This is a very similar function to the previous sigmoid function and has much of the same properties: even its derivative is straight forward to compute. However, this function allows us to map the input to any value between -1 and 1 (but not inclusive of those). In effect, this allows us to apply a plenalty to the node (negative) rather than just have the node not fire at all. It also gives us a larger range of output to play with in the positive end of the scale meaning finer adjustments can be made.&lt;/p&gt;

&lt;p&gt;This function has a natural threshold of 0, meaning that any input which maps to a value greater than 0 is considered high (or 1) in binary terms.&lt;/p&gt;

&lt;p&gt;Again, the fact that this function is smooth, continuous (differentiable), monotonic and bounded means that back propagation will work well. The subsequent functions don&amp;rsquo;t all have these properties which makes them more difficult to use in back propagation (though it is done).
&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-s-the-difference-between-the-sigmoid-and-hyperbolic-tangent&#34;&gt;What&amp;rsquo;s the difference between the sigmoid and hyperbolic tangent?&lt;/h2&gt;

&lt;p&gt;They both achieve a similar mapping, are both continuous, smooth, monotonic and differentiable, but give out different values. For a sigmoid function, a large negative input generates an almost zero output. This lack of output will affect all subsequent weights in the network which may not be desirable - effectively stopping the next nodes from learning. In contrast, the $\tanh$ function supplies -1 for negative values, maintaining the output of the node and allowing subsequent nodes to learn from it.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;gaussian&#34;&gt; Gaussian Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-3&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig4&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/gaussian.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dgaussian.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 4&lt;/font&gt;: The gaussian function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-3&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left( x_{i}\right ) = e^{ -x_{i}^{2}}, \ \
f^{\prime}\left( x_{i}\right ) = - 2x e^{ - x_{i}^{2}}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-2&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gaussian(x, Derivative=False):
    if not Derivative:
        return np.exp(-x**2)
    else:
        return -2 * x * np.exp(-x**2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-3&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;The gaussian function is an even function, thus is gives the same output for equally positive and negative values of input. It gives its maximal output when there is no input and has decreasing output with increasing distance from zero. We can perhaps imagine this function is used in a node where the input feature is less likely to contribute to the final result.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;step&#34;&gt; Step (or Heaviside) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-4&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig5&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/step.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 5&lt;/font&gt;: The Heaviside function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-4&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
    f(x)= 
\begin{cases}
\begin{align}
    0  \ &amp;: \ x_{i} \leq T\\
    1 \ &amp;: \ x_{i} &gt; T\\
    \end{align}
\end{cases}
$$&lt;/div&gt;

&lt;h3 id=&#34;why-is-it-used-4&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;Some cases call for a function which applies a hard thresold: either the output is precisely a single value, or not. The other functions we&amp;rsquo;ve looked at have an intrinsic probablistic output to them i.e. a higher output in decimal format implying a greater probability of being 1 (or a high output). The step function does away with this opting for a definite high or low output depending on some threshold on the input $T$.&lt;/p&gt;

&lt;p&gt;However, the step-function is discontinuous and therefore non-differentiable (its derivative is the Dirac-delta function). Therefore use of this function in practice is not done with back-propagation.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;ramp&#34;&gt; Ramp Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-5&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig6&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/ramp.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dramp.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 6&lt;/font&gt;: The ramp function (left) and its derivative (right) with $T1=-2$ and $T2=3$.
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-5&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
    f(x)= 
\begin{cases}
\begin{align}
    0 \ &amp;: \ x_{i} \leq T_{1}\\[0.5em]
    \frac{\left( x_{i} - T_{1} \right)}{\left( T_{2} - T_{1} \right)} \ &amp;: \ T_{1} \leq x_{i} \leq T_{2}\\[0.5em]
    1 \ &amp;: \ x_{i} &gt; T_{2}\\
    \end{align}
\end{cases}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-3&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ramp(x, Derivative=False, T1=0, T2=np.max(x)):
    out = np.ones(x.shape)
    ids = ((x &amp;lt; T1) | (x &amp;gt; T2))
    if not Derivative:
        out = ((x - T1)/(T2-T1))
        out[(x &amp;lt; T1)] = 0
        out[(x &amp;gt; T2)] = 1
        return out
    else:
        out[ids]=0
        return out
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-5&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;The ramp function is a truncated version of the linear function. From its shape, the ramp function looks like a more definitive version of the sigmoid function in that its maps a range of inputs to outputs over the range (0 1) but this time with definitive cut off points $T1$ and $T2$. This gives the function the ability to fire the node very definitively above a threshold, but still have some uncertainty in the lower regions. It may not be common to see $T1$ in the negative region unless the ramp is equally distributed about $0$.&lt;/p&gt;

&lt;h3 id=&#34;relu&#34;&gt; 6.1 Rectified Linear Unit (ReLU) &lt;/h3&gt;

&lt;p&gt;There is a popular, special case of the ramp function in use in the powerful &lt;em&gt;convolutional neural network&lt;/em&gt; (CNN) architecture called a &lt;em&gt;&lt;strong&gt;Re&lt;/strong&gt;ctifying &lt;strong&gt;L&lt;/strong&gt;inear &lt;strong&gt;U&lt;/strong&gt;nit&lt;/em&gt; (ReLU). In a ReLU, $T1=0$ and $T2$ is the maximum of the input giving a linear function with no negative values as below:&lt;/p&gt;

&lt;div  id=&#34;fig7&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/relu.png&#34; width=&#34;45%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/drelu.png&#34; width=&#34;45%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 7&lt;/font&gt;: The Rectified Linear Unit (ReLU) (left) with its derivative (right).
        &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;and in Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def relu(x, Derivative=False):
    if not Derivative:
        return np.maximum(0,x)
    else:
        out = np.ones(x.shape)
        out[(x &amp;lt; 0)]=0
        return out
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Simple Neural Network - Mathematics</title>
      <link>/post/neuralnetwork/</link>
      <pubDate>Mon, 06 Mar 2017 17:04:53 +0000</pubDate>
      
      <guid>/post/neuralnetwork/</guid>
      <description>&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;p&gt;Tutorials on neural networks (NN) can be found all over the internet. Though many of them are the same, each is written (or recorded) slightly differently. This means that I always feel like I learn something new or get a better understanding of things with every tutorial I see. I&amp;rsquo;d like to make this tutorial as clear as I can, so sometimes the maths may be simplistic, but hopefully it&amp;rsquo;ll give you a good unserstanding of what&amp;rsquo;s going on. &lt;strong&gt;Please&lt;/strong&gt; let me know if any of the notation is incorrect or there are any mistakes - either comment or use the contact page on the left.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#nnarchitecture&#34;&gt;Neural Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferFunction&#34;&gt;Transfer Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedforward&#34;&gt;Feed-forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#error&#34;&gt;Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationGrads&#34;&gt;Back Propagation - the Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationAlgorithm&#34;&gt;Back Propagaton - the Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;nnarchitecture&#34;&gt;1. Neural Network Architecture &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By now, you may well have come across diagrams which look very similar to the one below. It shows some input node, connected to some output node via an intermediate node in what is called a &amp;lsquo;hidden layer&amp;rsquo; - &amp;lsquo;hidden&amp;rsquo; because in the use of NN only the input and output is of concern to the user, the &amp;lsquo;under-the-hood&amp;rsquo; stuff may not be interesting to them. In real, high-performing NN there are usually more hidden layers.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; width=40% src=&#34;/img/simpleNN/simpleNN.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: A simple 2-layer NN with 2 features in the input layer, 3 nodes in the hidden layer and two nodes in the output layer.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;When we train our network, the nodes in the hidden layer each perform a calculation using the values from the input nodes. The output of this is passed on to the nodes of the next layer. When the output hits the final layer, the &amp;lsquo;output layer&amp;rsquo;, the results are compared to the real, known outputs and some tweaking of the network is done to make the output more similar to the real results. This is done with an algorithm called &lt;em&gt;back propagation&lt;/em&gt;. Before we get there, lets take a closer look at these calculations being done by the nodes.&lt;/p&gt;

&lt;h2 id=&#34;transferFunction&#34;&gt;2. Transfer Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At each node in the hidden and output layers of the NN, an &lt;em&gt;activation&lt;/em&gt; or &lt;em&gt;transfer&lt;/em&gt; function is executed. This function takes in the output of the previous node, and multiplies it by some &lt;em&gt;weight&lt;/em&gt;. These weights are the lines which connect the nodes. The weights that come out of one node can all be different, that is they will &lt;em&gt;activate&lt;/em&gt; different neurons. There can be many forms of the transfer function, we will first look at the &lt;em&gt;sigmoid&lt;/em&gt; transfer function as it seems traditional.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;The sigmoid function&#34; width=50% src=&#34;/img/simpleNN/sigmoid.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The sigmoid function.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As you can see from the figure, the sigmoid function takes any real-valued input and maps it to a real number in the range $(0 \ 1)$ - i.e. between, but not equal to, 0 and 1. We can think of this almost like saying &amp;lsquo;if the value we have maps to an output near 1, this node fires, if it maps to an output near 0, the node does not fire&amp;rsquo;. The equation for this sigmoid function is:&lt;/p&gt;

&lt;div id=&#34;eqsigmoidFunction&#34;&gt;$$
\sigma ( x ) = \frac{1}{1 + e^{-x}}
$$&lt;/div&gt;

&lt;p&gt;We need to have the derivative of this transfer function so that we can perform back propagation later on. This is the process where by the connections in the network are updated to tune the performance of the NN. We&amp;rsquo;ll talk about this in more detail later, but let&amp;rsquo;s find the derivative now.&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\frac{d}{dx}\sigma ( x ) &amp;= \frac{d}{dx} \left( 1 + e^{ -x }\right)^{-1}\\
&amp;=  -1 \times -e^{-x} \times \left(1 + e^{-x}\right)^{-2}= \frac{ e^{-x} }{ \left(1 + e^{-x}\right)^{2} } \\
&amp;= \frac{\left(1 + e^{-x}\right) - 1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{\left(1 + e^{-x}\right) }{\left(1 + e^{-x}\right)^{2}} - \frac{1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{1}{\left(1 + e^{-x}\right)} - \left( \frac{1}{\left(1 + e^{-x}\right)} \right)^{2} \\[0.5em]
&amp;= \sigma ( x ) - \sigma ( x ) ^ {2}
\end{align*}
$$&lt;/div&gt;

&lt;p&gt;Therefore, we can write the derivative of the sigmoid function as:&lt;/p&gt;

&lt;div id=&#34;eqdsigmoid&#34;&gt;$$
\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)
$$&lt;/div&gt;

&lt;p&gt;The sigmoid function has the nice property that its derivative is very simple: a bonus when we want to hard-code this into our NN later on. Now that we have our activation or transfer function selected, what do we do with it?&lt;/p&gt;

&lt;h2 id=&#34;feedforward&#34;&gt;3. Feed-forward &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;During a feed-forward pass, the network takes in the input values and gives us some output values. To see how this is done, let&amp;rsquo;s first consider a 2-layer neural network like the one in Figure 1. Here we are going to refer to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$i$ - the $i^{\text{th}}$ node of the input layer $I$&lt;/li&gt;
&lt;li&gt;$j$ - the $j^{\text{th}}$ node of the hidden layer $J$&lt;/li&gt;
&lt;li&gt;$k$ - the $k^{\text{th}}$ node of the input layer $K$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The activation function at a node $j$ in the hidden layer takes the value:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{j} &amp;= \xi_{1} w_{1j} + \xi_{2} w_{2j} \\[0.5em]
&amp;= \sum_{i \in I} \xi_{i} w_{i j}

\end{align}
$$&lt;/div&gt;

&lt;p&gt;where $\xi_{i}$ is the value of the $i^{\text{th}}$ input node and $w_{i j}$ is the weight of the connection between $i^{\text{th}}$ input node and the $j^{\text{th}}$ hidden node. &lt;strong&gt;In short:&lt;/strong&gt; at each hidden layer node, multiply each input value by the connection received by that node and add them together.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the weights are initisliased when the network is setup. Sometimes they are all set to 1, or often they&amp;rsquo;re set to some small random value.&lt;/p&gt;

&lt;p&gt;We apply the activation function on $x_{j}$ at the $j^{\text{th}}$ hidden node and get:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{j} &amp;= \sigma(x_{j}) \\
&amp;= \sigma(  \xi_{1} w_{1j} + \xi_{2} w_{2j})
\end{align}
$$&lt;/div&gt;

&lt;p&gt;$\mathcal{O}_{j}$ is the output of the $j^{\text{th}}$ hidden node. This is calculated for each of the $j$ nodes in the hidden layer. The resulting outputs now become the input for the next layer in the network. In our case, this is the final output later. So for each of the $k$ nodes in $K$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{k} &amp;= \sigma(x_{k}) \\
&amp;= \sigma \left( \sum_{j \in J}  \mathcal{O}_{j} w_{jk}  \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;As we&amp;rsquo;ve reached the end of the network, this is also the end of the feed-foward pass. So how well did our network do at getting the correct result $\mathcal{O}_{k}$? As this is the training phase of our network, the true results will be known an we cal calculate the error.&lt;/p&gt;

&lt;h2 id=&#34;error&#34;&gt;4. Error &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We measure error at the end of each foward pass. This allows us to quantify how well our network has performed in getting the correct output. Let&amp;rsquo;s define $t_{k}$ as the expected or &lt;em&gt;target&lt;/em&gt; value of the $k^{\text{th}}$ node of the output layer $K$. Then the error $E$ on the entire output is:&lt;/p&gt;

&lt;div id=&#34;eqerror&#34;&gt;$$
\text{E} = \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;Dont&amp;rsquo; be put off by the random &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; in front there, it&amp;rsquo;s been manufactured that way to make the upcoming maths easier. The rest of this should be easy enough: get the residual (difference between the target and output values), square this to get rid of any negatives and sum this over all of the nodes in the output layer.&lt;/p&gt;

&lt;p&gt;Good! Now how does this help us? Our aim here is to find a way to tune our network such that when we do a forward pass of the input data, the output is exactly what we know it should be. But we can&amp;rsquo;t change the input data, so there are only two other things we can change:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the weights going into the activation function&lt;/li&gt;
&lt;li&gt;the activation function itself&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will indeed consider the second case in another post, but the magic of NN is all about the &lt;em&gt;weights&lt;/em&gt;. Getting each weight i.e. each connection between nodes, to be just the perfect value, is what back propagation is all about. The back propagation algorithm we will look at in the next section, but lets go ahead and set it up by considering the following: how much of this error $E$ has come from each of the weights in the network?&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re asking, what is the proportion of the error coming from each of the $W_{jk}$ connections between the nodes in layer $J$ and the output layer $K$. Or in mathematical terms:&lt;/p&gt;

&lt;div&gt;$$
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{\partial{}}{\partial{W_{jk}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;If you&amp;rsquo;re not concerned with working out the derivative, skip this highlighted section.&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

To tackle this we can use the following bits of knowledge: the derivative of the sum is equal to the sum of the derivatives i.e. we can move the derivative term inside of the summation:

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \sum_{k \in K} \frac{\partial{}}{\partial{W_{jk}}} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the weight $w_{1k}$ does not affect connection $w_{2k}$ therefore the change in $W_{jk}$ with respect to any node other than the current $k$ is zero. Thus the summation goes away:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;apply the power rule knowing that $t_{k}$ is a constant:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}} &amp;=  \frac{1}{2} \times 2 \times \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right) \\
 &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right)
\end{align}
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the leftover derivative is the chage in the output values with respect to the weights. Substituting $ \mathcal{O}_{k} = \sigma(x_{k}) $ and the sigmoid derivative $\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)$:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( x_{k}\right)
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the final derivative, the input value $x_{k}$ is just $\mathcal{O}_{j} W_{jk}$ i.e. output of the previous layer times the weight to this layer. So the change in  $\mathcal{O}_{j} w_{jk}$ with respect to $w_{jk}$ just gives us the output value of the previous layer $ \mathcal{O}_{j} $ and so the full derivative becomes:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{j} W_{jk} \right) \\[0.5em]
&amp;=\left( \mathcal{O}_{k} - t_{k} \right) \sigma (x )  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j} 
\end{align}
$$&lt;/div&gt;

&lt;p&gt;We can replace the sigmoid function with the output of the layer
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;The derivative of the error function with respect to the weights is then:&lt;/p&gt;

&lt;div id=&#34;derror&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  =\left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j}
$$&lt;/div&gt;

&lt;p&gt;We group the terms involving $k$ and define:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;

&lt;p&gt;And therefore:&lt;/p&gt;

&lt;div id=&#34;derrorjk&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  = \mathcal{O}_{j} \delta_{k} 
$$&lt;/div&gt;

&lt;p&gt;So we have an expression for the amount of error, called &amp;lsquo;deta&amp;rsquo; ($\delta_{k}$), on the weights from the nodes in $J$ to each node $k$ in $K$. But how does this help us to improve out network? We need to back propagate the error.&lt;/p&gt;

&lt;h2 id=&#34;backPropagationGrads&#34;&gt;5. Back Propagation - the gradients&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Back propagation takes the error function we found in the previous section, uses it to calculate the error on the current layer and updates the weights to that layer by some amount.&lt;/p&gt;

&lt;p&gt;So far we&amp;rsquo;ve only looked at the error on the output layer, what about the hidden layer? This also has an error, but the error here depends on the output layer&amp;rsquo;s error too (because this is where the difference between the target $t_{k}$ and output $\mathcal{O}_{k}$ can be calculated). Lets have a look at the error on the weights of the hidden layer $W_{ij}$:&lt;/p&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{ij}}} =  \frac{\partial{}}{\partial{W_{ij}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;p&gt;Now, unlike before, we cannot just drop the summation as the derivative is not directly acting on a subscript $k$ in the summation. We should be careful to note that the output from every node in $J$ is actually connected to each of the nodes in $K$ so the summation should stay. But we can still use the same tricks as before: lets use the power rule again and move the derivative inside (because the summation is finite):&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;=  \frac{1}{2} \times 2 \times  \frac{\partial{}}{\partial{W_{ij}}}   \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)  \mathcal{O}_{k} \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} \mathcal{O}_{k}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Again, we substitute $\mathcal{O}_{k} = \sigma( x_{k})$ and its derivative and revert back to our output notation:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (\sigma(x_{k}) )\\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \sigma(x_{k}) \left( 1 - \sigma(x_{k}) \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k}) \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k})
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;This still looks familar from the output layer derivative, but now we&amp;rsquo;re struggling with the derivative of the input to $k$ i.e. $x_{k}$ with respect to the weights from $I$ to $J$. Let&amp;rsquo;s use the chain rule to break apart this derivative in terms of the output from $J$:&lt;/p&gt;

&lt;div&gt; $$
\frac{\partial{ x_{k}}}{\partial{W_{ij}}} = \frac{\partial{ x_{k}}}{\partial{\mathcal{O}_{j}}}\frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}
$$&lt;/div&gt;

&lt;p&gt;The change of the input to the $k^{\text{th}}$ node with respect to the output from the $j^{\text{th}}$ node is down to a product with the weights, therefore this derivative just becomes the weights $W_{jk}$. The final derivative has nothing to do with the subscript $k$ anymore, so we&amp;rsquo;re free to move this around - lets put it at the beginning:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Lets finish the derivatives, remembering that the output of the node $j$ is just $\mathcal{O}_{j} = \sigma(x_{j}) $ and we know the derivative of this function too:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{}}{\partial{W_{ij}}}\sigma(x_{j})  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \sigma(x_{j}) \left( 1 - \sigma(x_{j}) \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;The final derivative is straightforward too, the derivative of the input to $j$ with repect to the weights is just the previous input, which in our case is $\mathcal{O}_{i}$,&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Almost there! Recall that we defined $\delta_{k}$ earlier, lets sub that in:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \delta_{k} W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;To clean this up, we now define the &amp;lsquo;delta&amp;rsquo; for our hidden layer:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;

&lt;p&gt;Thus, the amount of error on each of the weights going into our hidden layer:&lt;/p&gt;

&lt;div id=&#34;derrorij&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{ij}}}  = \mathcal{O}_{i} \delta_{j} 
$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the reason for the name &lt;em&gt;back&lt;/em&gt; propagation is that we must calculate the errors at the far end of the network and work backwards to be able to calculate the weights at the front.&lt;/p&gt;

&lt;h2 id=&#34;bias&#34;&gt;6.  Bias &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets remind ourselves what happens inside our hidden layer nodes:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInsideNoBias.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Each feature $\xi_{i}$ from the input layer $I$ is multiplied by some weight $w_{ij}$&lt;/li&gt;
&lt;li&gt;These are added together to get $x_{i}$ the total, weighted input from the nodes in $I$&lt;/li&gt;
&lt;li&gt;$x_{i}$ is passed through the activation, or transfer, function $\sigma(x_{i})$&lt;/li&gt;
&lt;li&gt;This gives the output $\mathcal{O}_{j}$ for each of the $j$ nodes in hidden layer $J$&lt;/li&gt;
&lt;li&gt;$\mathcal{O}_{j}$ from each of the $J$ nodes becomes $\xi_{j}$ for the next layer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we talk about the &lt;em&gt;bias&lt;/em&gt; term in NN, we are talking about an additional parameter that is inluded in the summation of step 2 above. The bias term is usually denoted with the symbol $\theta$ (theta). It&amp;rsquo;s function is to act as a threshold for the activation (transfer) function. It is given the value of 1 and is not connected to anything else. As such, this means that any derivative of the node&amp;rsquo;s output with respect to the bias term would just give a constant, 1. This allows us to just think of the bias term as an output from the node with the value of 1. This will be updated later during backpropagation to change the threshold at which the node fires.&lt;/p&gt;

&lt;p&gt;Lets update the equation for $x_{i}$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{i} &amp;= \xi_{1j} w_{1j} + \xi_{2j} w_{2j} + \theta_{j} \\[0.5em]
\sigma( x_{i} ) &amp;= \sigma \left( \sum_{i \in I} \left( \xi_{ij} w_{ij} \right) + \theta_{j} \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;and put it on the diagram:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInside.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;backPropagationAlgorithm&#34;&gt;7. Back Propagation - the algorithm&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now we have all of the pieces! We&amp;rsquo;ve got the initial outputs after our feed-forward, we have the equations for the delta terms (the amount by which the error is based on the different weights) and we know we need to update our bias term too. So what does it look like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Input the data into the network and feed-forward&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;output&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;hidden layer&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calculate the changes that need to be made to the weights and bias terms:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\Delta W &amp;= -\eta \ \delta_{l} \ \mathcal{O}_{l-1} \\
\Delta\theta &amp;= -\eta \ \delta_{l}
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update the weights and biases across the network:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
W + \Delta W &amp;\rightarrow W \\
\theta + \Delta\theta &amp;\rightarrow \theta
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, $\eta$ is just a small number that limit the size of the deltas that we compute: we don&amp;rsquo;t want the network jumping around everywhere. The $l$ subscript denotes the deltas and output for that layer $l$. That is, we compute the delta for each of the nodes in a layer and vectorise them. Thus we can compute the element-wise product with the output values of the previous layer and get our update $\Delta W$ for the weights of the current later. Similarly with the bias term.&lt;/p&gt;

&lt;p&gt;This algorithm is looped over and over until the error between the output and the target values is below some set threshold. Depending on the size of the network i.e. the number of layers and number of nodes per layer, it can take a long time to complete one &amp;lsquo;epoch&amp;rsquo; or run through of this algorithm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some of the ideas notation in this tutorial comes from the good videos by &lt;a href=&#34;https://www.youtube.com/playlist?list=PL29C61214F2146796&#34; title=&#34; NN Videos&#34;&gt;Ryan Harris&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Design Wisdom</title>
      <link>/post/webdesign/</link>
      <pubDate>Sat, 04 Mar 2017 17:21:15 +0000</pubDate>
      
      <guid>/post/webdesign/</guid>
      <description>&lt;p&gt;I thought I&amp;rsquo;d give an overview of some of the wisdom I&amp;rsquo;ve gained from creating MLNotebook - my adventures in markdown&amp;hellip; and the rest!&lt;/p&gt;

&lt;h2 id=&#34;hugo&#34;&gt; Hugo &lt;/h2&gt;

&lt;h3 id=&#34;hugoSetup&#34;&gt; Setup &lt;/h3&gt;

&lt;p&gt;Hugo was relatively easy to setup, but I think some of the guides around could be a lot clearer particularly when it comes to hosting on Githib Pages. Firstly, make sure that you download Hugo &lt;a href=&#34;https://github.com/spf13/hugo/releases&#34; title=&#34;Hugo Github&#34;&gt;here&lt;/a&gt; and extract it to &lt;code&gt;/usr/local/bin&lt;/code&gt;. I renamed mine to &amp;ldquo;hugo&amp;rdquo;. Check whether its properly installed with the command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will provide the version number. If not, add &lt;code&gt;/usr/local/bin&lt;/code&gt; to your system path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ PATH=$PATH:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating a new site called &amp;ldquo;newsite&amp;rdquo; from scratch is the easy bit:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo new site ./newsite
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;themeAndOverrides&#34;&gt;Theme and overrides &lt;/h3&gt;

&lt;p&gt;To get my theme to work, I simply cloned the repository (as shown &lt;a href=&#34;https://themes.gohugo.io/blackburn/&#34; title=&#34;Blackburn theme&#34;&gt;here&lt;/a&gt;) directly into ./newsite/themes/blackburn. Be sure to copy the &lt;code&gt;config.toml&lt;/code&gt; file to &lt;code&gt;./newsite&lt;/code&gt;. That&amp;rsquo;s all there is to it!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir themes
$ cd themes
$ git clone https://github.com/yoshiharuyamashita/blackburn.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Customising this theme was really easy as it is mostly done in config.toml. What I wish I knew about Hugo straight off the bat is that the tree structure is important. So anything in the &amp;ldquo;themes&amp;rdquo; folder is a fall-back for anything that &lt;strong&gt;isn&amp;rsquo;t&lt;/strong&gt; present in the root folder of the site. That means if you have your own template for a post in &lt;code&gt;./newsite/layouts/single.html&lt;/code&gt; it will be used instead of the themes one in &lt;code&gt;./newsite/themes/layouts/single.html&lt;/code&gt;. Thus if you want to edit the layout, copy the theme&amp;rsquo;s one into your sites layout folder and edit it from there.&lt;/p&gt;

&lt;p&gt;The index page is the same deal, just copy it to your sites root and it will take precident over the default theme&amp;rsquo;s one.&lt;/p&gt;

&lt;h3 id=&#34;partials&#34;&gt;Partials&lt;/h3&gt;

&lt;p&gt;The partials bit can be a little confusing if you&amp;rsquo;re not too familiar with how the site is put together. Effectively, the page you&amp;rsquo;re loooking at right now is made up of lots of different parts (partials) that have been edited separately, put through a parser, turned into HTML and pasted together into a single HTML page. The head and footer don&amp;rsquo;t have much in them but are important for adding calls to Javascripts as they are stitched into each and every page on the website. Don&amp;rsquo;t confuse the head.html and header.html files, the latter is the actual title/banner at the top of the homepage (it is another partial that is stitched into index.html.&lt;/p&gt;

&lt;h3 id=&#34;socialMediaButtons&#34;&gt;Social Media Buttons&lt;/h3&gt;

&lt;p&gt;I spend a while trying to figure out how to get my social media buttons to actually take the url of the page they were on and share that exact post. I tried a hosted service which gave me a script that pulled down the buttons from them and allowed me to edit them via their interface, but it wasn&amp;rsquo;t content-specific. To dynamically get the url and get some nice-looking icons, I actually used the site &lt;a href=&#34;https://simplesharingbuttons.com/&#34; title=&#34;Simple Sharing Buttons&#34;&gt;Simple Sharing Buttons&lt;/a&gt;, chose the sites I wanted and theyprovided the icons along with the HTML. In comparisson to other sites and methods, this seems to work the best (except for the reddit one really).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;ul class=&amp;quot;share-buttons&amp;quot;&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmlnotebook.github.io&amp;amp;t=&amp;quot; title=&amp;quot;Share on Facebook&amp;quot; target=&amp;quot;_blank&amp;quot; onclick=&amp;quot;window.open(&#39;https://www.facebook.com/sharer/sharer.php?u=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;t=&#39; + encodeURIComponent(document.URL),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Share on facebook&amp;quot; src=&amp;quot;/img/facebook.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;https://twitter.com/intent/tweet?source=https%3A%2F%2Fmlnotebook.github.io&amp;amp;text=:%20https%3A%2F%2Fmlnotebook.github.io&amp;amp;via=mlnotebook&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Tweet&amp;quot; onclick=&amp;quot;window.open(&#39;https://twitter.com/intent/tweet?text=&#39; + encodeURIComponent(document.title) + &#39;:%20&#39;  + encodeURIComponent(document.URL),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Tweet&amp;quot; src=&amp;quot;/img/twitter.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;http://www.reddit.com/submit?url=https%3A%2F%2Fmlnotebook.github.io&amp;amp;title=&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Submit to Reddit&amp;quot; onclick=&amp;quot;window.open(&#39;http://www.reddit.com/submit?url=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;title=&#39; +  encodeURIComponent(document.title),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Submit to Reddit&amp;quot; src=&amp;quot;/img/reddit.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https%3A%2F%2Fmlnotebook.github.io&amp;amp;title=&amp;amp;summary=&amp;amp;source=https%3A%2F%2Fmlnotebook.github.io&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Share on LinkedIn&amp;quot; onclick=&amp;quot;window.open(&#39;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;title=&#39; +  encodeURIComponent(document.title),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Share on LinkedIn&amp;quot; src=&amp;quot;/img/linkedin.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;githubPages&#34;&gt; Hosting on Peronal Github Pages &lt;/h3&gt;

&lt;p&gt;Again, some of the tutorials out there aren&amp;rsquo;t great at properly explaining how to get your pages hosted on your &lt;strong&gt;personal&lt;/strong&gt; Github pages, rather than project ones (i.e. &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;) I&amp;rsquo;ll try to give you another version here.&lt;/p&gt;

&lt;p&gt;Firstly, login to Github and create the repository &lt;code&gt;&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;. This is important as the master branch will be used to locate your website at exactly &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;. Initialise it with the &lt;code&gt;README.md&lt;/code&gt;. Create a new branch called &lt;code&gt;hugo&lt;/code&gt; and initialise this with the &lt;code&gt;README.md&lt;/code&gt; too.&lt;/p&gt;

&lt;p&gt;In your &lt;code&gt;./newsite&lt;/code&gt; directory you&amp;rsquo;ll need to build the site, initialise the git respository and add the remote:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo
$
$ git init
$ git remote add origin git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;re having trouble adding the remote because of &lt;em&gt;permissions&lt;/em&gt; it could be that you&amp;rsquo;re using a different Git account for your website than normal. Have a look at the &lt;code&gt;git config&lt;/code&gt; options to change the username/password. If that fails, it could be that you need to sort an &lt;code&gt;ssh&lt;/code&gt; key - instructions for that are on your account settings page.&lt;/p&gt;

&lt;p&gt;From here, I managed to find and adapt two scripts from &lt;a href=&#34;https://hjdskes.github.io/blog/deploying-hugo-on-personal-gh-pages/&#34; title=&#34;hjdskes&#34;&gt;here&lt;/a&gt;. The first is &lt;code&gt;setup.sh&lt;/code&gt; (&lt;a href=&#34;/docs/setup.sh&#34; title=&#34;setup.sh&#34;&gt;download&lt;/a&gt;) and only needs to be executed once. It does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deletes the master branch (perfectly safe)&lt;/li&gt;
&lt;li&gt;Creates a new orphaned master branch&lt;/li&gt;
&lt;li&gt;Takes the &lt;code&gt;README.md&lt;/code&gt; from &lt;code&gt;hugo&lt;/code&gt; and makes an initial commit to &lt;code&gt;master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Changes back to &lt;code&gt;hugo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Removes the existing &lt;code&gt;./public&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;Sets the &lt;code&gt;master&lt;/code&gt; branch as a subtree for the &lt;code&gt;./public&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;Pulls the commited &lt;code&gt;master&lt;/code&gt; back into &lt;code&gt;./public&lt;/code&gt; to stop merge conflicts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;warn&#34;&gt;Make sure that you edit the `USERNAME` field in `setup.sh` before executing.&lt;/div&gt;

&lt;p&gt;After that, whenever you want to upload your site, just run the second script &lt;code&gt;deploy.sh&lt;/code&gt; which I&amp;rsquo;ve altered slightly (&lt;a href=&#34;/docs/deploy.sh&#34; title=&#34;deploy.sh&#34;&gt;download&lt;/a&gt;) with an optional argument which will be your commit message: missing out the argument submits a default message.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;deploy.sh&lt;/code&gt; commits and pushes all of your changes to the &lt;code&gt;hugo&lt;/code&gt; source branch before putting the &lt;code&gt;./public&lt;/code&gt; folder on &lt;code&gt;master&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&#34;warn&#34;&gt;Make sure that you edit the `USERNAME` field in `deploy.sh` before executing&lt;/div&gt;

&lt;p&gt;And that&amp;rsquo;s it! If the website doesn&amp;rsquo;t load when you go to &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt; you may need to hit &lt;code&gt;settings&lt;/code&gt; in your repo (top right of the menu bar), scroll down to &amp;ldquo;Github Pages&amp;rdquo; and select &lt;code&gt;master&lt;/code&gt; as your source.&lt;/p&gt;

&lt;h2 id=&#34;htmlCss&#34;&gt;HTML / CSS&lt;/h2&gt;

&lt;h3 id=&#34;contactForm&#34;&gt;Contact Form&lt;/h3&gt;

&lt;p&gt;The first part of the site I altered was the contant page. I added a contact form which largely involves &lt;code&gt;html&lt;/code&gt; formatted with &lt;code&gt;css&lt;/code&gt;. The magic that makes it work comes from the free service called &lt;a href=&#34;https://formspree.io/&#34; title=&#34;Formspree&#34;&gt;Formspree&lt;/a&gt;. Essentially, the submit button sends the information to formspree and they forward it on to me directly. It uses a hidden field to give the forwarded emails the same subject, this makes for easy filtering. It also provides a free &amp;ldquo;I&amp;rsquo;m not a robot&amp;rdquo; page after clicking submit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div id=&amp;quot;contactform&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
&amp;lt;form action=&amp;quot;https://formspree.io/your@email.com method=&amp;quot;POST&amp;quot; name=&amp;quot;sentMessage&amp;quot; id=&amp;quot;contactForm&amp;quot; novalidate&amp;gt;
	&amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;name&amp;quot; placeholder=&amp;quot;Name&amp;quot; id=&amp;quot;name&amp;quot; required data-validation-required-message=&amp;quot;Please enter your name.&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
	&amp;lt;input type=&amp;quot;email&amp;quot; name=&amp;quot;_replyto&amp;quot; placeholder=&amp;quot;Email Address&amp;quot; id=&amp;quot;email&amp;quot; required data-validation-required-message=&amp;quot;Please enter your email address.&amp;quot; &amp;gt;&amp;lt;br&amp;gt;

	&amp;lt;input type=&amp;quot;hidden&amp;quot;  name=&amp;quot;_subject&amp;quot; value=&amp;quot;Message from MLNotebook&amp;quot;&amp;gt;
	&amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;_gotcha&amp;quot; style=&amp;quot;display:none&amp;quot; /&amp;gt;
	&amp;lt;textarea rows=&amp;quot;10&amp;quot; name=&amp;quot;message&amp;quot; class=&amp;quot;form-control&amp;quot; placeholder=&amp;quot;Message&amp;quot; id=&amp;quot;message&amp;quot; required data-validation-required-message=&amp;quot;Please enter a message.&amp;quot;&amp;gt;&amp;lt;/textarea&amp;gt;&amp;lt;br&amp;gt;
	&amp;lt;input type=&amp;quot;submit&amp;quot; value=&amp;quot;Send&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The formatting was a pain as I&amp;rsquo;d never used the box-size argument before - this is what I found made the boxes all the same size and have the same alignment. I added for all browsers too.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;
input[type=text], input[type=email], textarea {
	display: inline-block;
  	border: 1px solid transparent;
  	border-top: none;
  	border-bottom: 1px solid #DDD;
  	box-shadow: inset 0 1px 2px rgba(0,0,0,.39), 0 -1px 1px #FFF, 0 1px 0 #FFF;
	border-radius: 4px;
	margin: 2px 2px 2px 2px;
	resize:none;
	float: left;
	width: 100%;
}

textarea, input {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    box-sizing: border-box;
}

input[type=submit] {
	width: 100%;
}

.center {
	margin: auto;
}

input {
	height:50px;
}

textarea {
	height: 200px;
	padding-left: 0px;
}

input, textarea::-webkit-input-placeholder {
   padding-left: 10px;
}
input, textarea::-moz-placeholder {
   padding-left: 10px;
}
input, textarea:-ms-input-placeholder {
   padding-left: 10px;
}
input, textarea:-moz-placeholder {
   padding-left: 10px;
}
  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resizing&#34;&gt;Resizing for Small Screens&lt;/h3&gt;

&lt;p&gt;One of my final hurdles in getting the site setup was making the homepage a little more friendly that just showing the recent posts. So I decided to add my &lt;a href=&#34;https://twitter.com/mlnotebook&#34; title=&#34;@MLNotebook&#34;&gt;twitter&lt;/a&gt; feed to the side. Twitter has an easy code to embed this, and I just put it into its own partial in &lt;code&gt;layouts/partials/twitterfeed.html&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;My problem here though was that when I viewed my site on my phone, or resized the web-browser on the computer, the content would shrink and be almost unreadable - I wanted the feed to move below the text if the screen was below a certain size. So I created the usual &lt;code&gt;div&lt;/code&gt; containers within my &lt;code&gt;index.html&lt;/code&gt; file and added the shortcode to include my &lt;code&gt;twitterfeed.html&lt;/code&gt; in the right-hand side.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div id=&amp;quot;container&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
	&amp;lt;div id=&amp;quot;left_content&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
		&amp;lt;div class=&amp;quot;content&amp;quot;&amp;gt;
		  {{ range ( .Paginate (where .Data.Pages &amp;quot;Type&amp;quot; &amp;quot;post&amp;quot;)).Pages }}
		    {{ .Render &amp;quot;summary&amp;quot;}}
		  {{ end }}

		  {{ partial &amp;quot;pagination.html&amp;quot; . }}
		&amp;lt;/div&amp;gt;
	&amp;lt;/div&amp;gt;
	&amp;lt;div id=&amp;quot;right_content&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
		&amp;lt;center&amp;gt;{{ partial &amp;quot;twitterfeed.html&amp;quot; . }}&amp;lt;/center&amp;gt;
	&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then used &lt;code&gt;css&lt;/code&gt; to give the &lt;code&gt;div&lt;/code&gt; containers their own properties for different screen sizes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;#container {
	position: relative;
	width:auto;

}

#right_content {
	float:left;
	overflow:hidden;
	display:block;
	padding-right:1%;

}

#left_content {
	float:left;
	width:80%;
	display:block;
	margin:auto;
	min-width=600px;

}

pre &amp;gt; code {
	font-size:11pt;
}

@media screen and (max-width: 1000px) {

#left_content {
	width: 100%;
	}
	
	.content {
	max-width:100%;
	}



#right_content {
	width:100%;
}

pre &amp;gt; code {
	font-size:8pt;
}

}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this allows the size of the font in the code-snippets to shrink when the screensize is small - I find that it reads more easily.&lt;/p&gt;

&lt;h2 id=&#34;syntaxHighlighting&#34;&gt;Syntax highlighting&lt;/h2&gt;

&lt;p&gt;So actually getting code into the website was trickier than I thought. The in-built markdown codeblocks seem to work just fine by adding code between backticks: &lt;code&gt;`&amp;lt;code here&amp;gt;`&lt;/code&gt;. Markdown doesn&amp;rsquo;t do syntax highlightsing right out of the box though. So I&amp;rsquo;m using &lt;code&gt;highlight.js&lt;/code&gt;. My theme does come with a highlight shortcode option, but I found that I couldn&amp;rsquo;t customise it how I wanted - particuarly, the font size was just too big. I tried everything, even adding extra &lt;code&gt;&amp;lt;pre&amp;gt; &amp;lt;/pre&amp;gt;&lt;/code&gt; tags around it and using &lt;code&gt;css&lt;/code&gt; to format them. In the end, I found that using &lt;code&gt;highlight.js&lt;/code&gt; was much simpler - I just loaded the script straight off their server and voila! The link just needed editing to select the theme I wanted, but I opted for the standard &lt;code&gt;monokai&lt;/code&gt; anyway. I placed this in my site&amp;rsquo;s &lt;code&gt;head&lt;/code&gt; partial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai.min.css&amp;quot;&amp;gt;
&amp;lt;script src=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;hljs.initHighlightingOnLoad();&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mathsRendering&#34;&gt;Maths Rendering&lt;/h2&gt;

&lt;p&gt;Being a site on machine learning, I&amp;rsquo;m going to need to be able to include some mathematics sometimes. I&amp;rsquo;m very familiar with $\rm\LaTeX$ and I&amp;rsquo;ve written-up a lot of formulae already, so I looked into getting $\rm\LaTeX$ formatting into markdown/Hugo. A few math rendering engines are around, but not all are simple to implement. The best option I found was &lt;a href=&#34;https://www.mathjax.org/&#34; title=&#34;MathJax&#34;&gt;MathJax&lt;/a&gt; which literally required me to add these few lines to my &lt;code&gt;head&lt;/code&gt; partial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;

&amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &amp;quot;AMS&amp;quot; },
         extensions: [&amp;quot;AMSmath.js&amp;quot;, &amp;quot;AMSsymbols.js&amp;quot;] }
  }
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From there, it allows me to put inline math into my websites such as $ c = \sqrt{a^{2} + b^{2}} $ by enclosing them in the normal \$ symbols like so: &lt;code&gt;\$ some math \$&lt;/code&gt;. MathJax also provides display-style input with enclosing &lt;code&gt;&amp;lt;div&amp;gt;\$\$ code \$\$&amp;lt;/div&amp;gt;&lt;/code&gt; e.g.:&lt;/p&gt;

&lt;div&gt;$$ c = \sqrt{a^{2} + b^{2}}  $$&lt;/div&gt;

&lt;p&gt;The formatting is done by some css&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;code.has-jax {
	font: inherit;
	font-size: 100%;
	background: inherit;
	border: inherit;
	color: #515151;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/td&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Surface Distance Function</title>
      <link>/post/surface-distance-function/</link>
      <pubDate>Wed, 01 Mar 2017 19:27:27 +0000</pubDate>
      
      <guid>/post/surface-distance-function/</guid>
      <description>

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Recently, I have been doing a &lt;strong&gt;lot&lt;/strong&gt; of segmentation evaluation - seeing how good a segmentation done by a machine compares with one that&amp;rsquo;s done manual, a &amp;lsquo;ground truth&amp;rsquo; (GT). Traditionally, such verification is done by comparing the overlap between the two e.g. Dice Simlarity Coefficient (DSC) [1]. There are a few different calculations that can be done (there&amp;rsquo;ll be a longer post on just that) and &amp;lsquo;surface distance&amp;rsquo; calculations are one of them.&lt;/p&gt;

&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;

&lt;p&gt;For this calculation, we need to be able to find the outline of the segmentation and compare it to the outline of the GT. We can then take measurements of how far each segmentation pixel is from its corresponding pixel in the GT outline.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at the maths. Surface distance metrics estimate the error between the outer surfaces $S$ and $S^{\prime}$ of the segmentations $X$ and $X^{\prime}$. The distance between a point $p$ on surface $S$ and the surface $S^{\prime}$ is given by the minimum of the Euclidean norm:&lt;/p&gt;

&lt;div&gt;$$ d(p, S^{\prime}) = \min_{p^{\prime} \in S^{\prime}} \left|\left| p - p^{\prime} \right|\right|_{2} $$&lt;/div&gt;

&lt;p&gt;Doing this for all pixels in the surface gives the total surface distance between $S$ and $S^{\prime}$: $d(S, S^{\prime})$:&lt;/p&gt;

&lt;p&gt;Now I&amp;rsquo;ve seen MATLAB code that can do this, though often its not entirely accurate. Plus I wanted to do this calculation on-the-fly as part of my program which was written in Python. So I came up with this function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology

def surfd(input1, input2, sampling=1, connectivity=1):
    
    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
    

    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)

    
    dta = morphology.distance_transform_edt(~S,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
       
    
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets go through it bit-by-bit. The function &lt;em&gt;surfd&lt;/em&gt; is defined to take in four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input1&lt;/em&gt; - the segmentation that has been created. It can be a multi-class segmentation, but this function will make the image binary. We&amp;rsquo;ll talk about how to use this function on individual classes later.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;input2&lt;/em&gt; - the GT segmentation against which we wish to compare &lt;em&gt;input1&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sampling&lt;/em&gt; - the pixel resolution or pixel size. This is entered as an &lt;em&gt;n&lt;/em&gt;-vector where &lt;em&gt;n&lt;/em&gt; is equal to the number of dimensions in the segmentation i.e. 2D or 3D. The default value is 1 which means pixels (or rather voxels) are 1 x 1 x 1 mm in size.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;connectivity&lt;/em&gt; - creates either a 2D (3 x 3) or 3D (3 x 3 x 3) matrix defining the neighbourhood around which the function looks for neighbouring pixels. Typically, this is defined as a six-neighbour kernel which is the default behaviour of this function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we&amp;rsquo;ll be making use of simple numpy operations, but we&amp;rsquo;ll also need the &lt;em&gt;morphology&lt;/em&gt; module from &lt;em&gt;scipy&lt;/em&gt;&amp;rsquo;s &lt;em&gt;dnimage&lt;/em&gt; package. These are imported first. More information on this module can be found &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.18.1/reference/ndimage.html&#34; title=&#34;Scipy _ndimage_ package&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two inputs are checked for their size and made binary. Any value greater than zero is made 1 (true).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the the &lt;em&gt;morphology.generate_binary_structure&lt;/em&gt; function, along with the number of dimensions of the segmentation, to create the kernel that will be used to detect the edges of the segmentations. This could be done just by hard-coding the kernel itself: &lt;code&gt;[[0 0 0],[0 1 0],[0 0 0]; [0 1 0], [1 1 1], [0 1 0]; [0 0 0], [0 1 0], [0 0 0]]&lt;/code&gt;. This kernel &amp;lsquo;&lt;em&gt;conn&lt;/em&gt;&amp;rsquo; is supplied to the &lt;em&gt;morphology.binary_erosion&lt;/em&gt; function which strips the outermost pixel from the edge of the segmentation. Subtracting this result from the segmentation itself leaves only the single-pixel-wide surface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we again use the &lt;em&gt;morphology&lt;/em&gt; module. This time we give the &lt;em&gt;distance_transform_edt&lt;/em&gt; function our pixel-size (&lt;em&gt;samping&lt;/em&gt;) and also the inverted surface-image. The inversion is used such that the surface itself is given the value of zero i.e. any pixel at this location, will have zero surface-distance. The transform increases the value/error/penalty of the remaining pixels with increasing distance away from the surface.&lt;/p&gt;

&lt;p&gt;Each pixel of the opposite segmentation-surface is then laid upon this &amp;lsquo;map&amp;rsquo; of penalties and both results are concatenated into a vector which is as long as the number of pixels in the surface of each segmentation. This vector of &lt;em&gt;surface distances&lt;/em&gt; is returned. Note that this is technically the &lt;em&gt;symmetric&lt;/em&gt; surface distance as we are not assuming that just doing this for &lt;em&gt;one&lt;/em&gt; of the surfaces is enough. It may be that the distance between a pixel in A and in B is not the same as between the pixel in B and in A. i.e. $d(S, S^{\prime}) \neq d(S^{\prime}, S)$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    dta = morphology.distance_transform_edt(~input1_border,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
        
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-is-it-used&#34;&gt;How is it used?&lt;/h2&gt;

&lt;p&gt;The function example below takes two segmentations (which both have multiple classes). The sampling vector is a typical pixel-size from an MRI scan and the 1 indicated I&amp;rsquo;d like a 6 neighbour (cross-shaped) kernel for finding the edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg, GT_seg, [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By specifcing the value of the voxel-label I&amp;rsquo;m interested in (assuming we&amp;rsquo;re talking about classes which are contiguous and not spread out), we can find the surface accuracy of that class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg(test_seg==1), \
                   GT_seg(GT_seg==1), [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-do-the-results-mean&#34;&gt;What do the results mean?&lt;/h2&gt;

&lt;p&gt;The returned surface distances can be used to calculate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Mean Surface Distance (MSD)&lt;/em&gt; - the mean of the vector is taken. This tell us how much, on average, the surface varies between the segmentation and the GT (in mm).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$     \text{MSD} = \frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime}) + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S) \right) $$ &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Residual Mean Square Distance (RMS)&lt;/em&gt; - as it says, the mean is taken from each of the points in the vector, these residuals are squared (to remove negative signs), summated, weighted by the mean and then the square-root is taken. Measured in mm.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \text{RMS} = \sqrt{\frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime})^{2} + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S)^{2} \right) }\ $$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hausdorff Distance (HD)&lt;/em&gt; - the maximum of the vector. The largest difference between the surface distances. Also measured in mm. We calculate the &lt;em&gt;symmetric Hausdorff distance&lt;/em&gt; as:&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$\text{HD} = \max \left[ d(S, S^{\prime}) , d(S^{\prime}, S) \right]$$&lt;/div&gt;

&lt;p&gt;Or in Python:
&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    msd = surface_distance.mean()
    rms = np.sqrt((surface_distance**2).mean())
    hd  = surface_distance.max()
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1]     Dice, L. R. (1945). Measures of the Amount of Ecologic Association Between Species. Ecology, 26(3), 297–302. &lt;a href=&#34;https://doi.org/10.2307/1932409&#34;&gt;https://doi.org/10.2307/1932409&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Wed, 01 Mar 2017 12:46:58 +0000</pubDate>
      
      <guid>/contact/</guid>
      <description>&lt;p&gt;Please leave any comments via Twitter or Github (links are on the left) or for anything else, use the form below.&lt;/p&gt;

&lt;!-- Contact Section --&gt;

&lt;div id=&#34;contactform&#34; class=&#34;center&#34;&gt;
&lt;form action=&#34;https://formspree.io/r.robinson16@imperial.ac.uk&#34; method=&#34;POST&#34; name=&#34;sentMessage&#34; id=&#34;contactForm&#34; novalidate&gt;
    &lt;input type=&#34;text&#34; name=&#34;name&#34; placeholder=&#34;Name&#34; id=&#34;name&#34; required data-validation-required-message=&#34;Please enter your name.&#34;&gt;&lt;br&gt;
    &lt;input type=&#34;email&#34; name=&#34;_replyto&#34; placeholder=&#34;Email Address&#34; id=&#34;email&#34; required data-validation-required-message=&#34;Please enter your email address.&#34; &gt;&lt;br&gt;

    &lt;input type=&#34;hidden&#34;  name=&#34;_subject&#34; value=&#34;Message from MLNotebook&#34;&gt;
    &lt;input type=&#34;text&#34; name=&#34;_gotcha&#34; style=&#34;display:none&#34; /&gt;
    &lt;textarea rows=&#34;10&#34; name=&#34;message&#34; class=&#34;form-control&#34; placeholder=&#34;Message&#34; id=&#34;message&#34; required data-validation-required-message=&#34;Please enter a message.&#34;&gt;&lt;/textarea&gt;&lt;br&gt;
    &lt;input type=&#34;submit&#34; value=&#34;Send&#34;&gt;
&lt;/form&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MLNotebook First Post</title>
      <link>/post/my-first-post/</link>
      <pubDate>Wed, 01 Mar 2017 10:38:47 +0000</pubDate>
      
      <guid>/post/my-first-post/</guid>
      <description>&lt;p&gt;This is the first post on the Machine Learning Notebook (MLN)!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m a n00b when it comes to Github Pages and relatively new to working with Github at all! It took me a while to decide what platform to use for creating this humble resource. I was initially looking at writing everything in iPython notebooks, but it was confusing to figure out how to keep the scripts live rather than coverting them to static sites.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve read several blogs, but &lt;a href=&#34;http://bruceeckel.github.io/2014/11/19/using-github-pages/&#34; title=&#34;Using Github Pages&#34;&gt;this one&lt;/a&gt; seemed to be written to my liking and was pretty much a document of trail and error, but perhaps more imporantly, &lt;strong&gt;solutions&lt;/strong&gt;. I&amp;rsquo;m actually writing this first post after reading &lt;a href=&#34;https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/&#34; title=&#34;Build a Blog with Jekyll and Github Pages&#34;&gt;this&lt;/a&gt; where I&amp;rsquo;ve settled on using Jekyll. Though I must say that I was initially pushed away from this idea after seeing that Jekyll was written for Ruby (in which I have no experience). But here it goes - hopefully it&amp;rsquo;ll be straight-forward.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/em&gt;: Scratch that - after playing around with &lt;a href=&#34;https://gohugo.io/&#34; title=&#34;Hugo Homepage&#34;&gt;Hugo&lt;/a&gt; and reading the &amp;lsquo;start from scratch&amp;rsquo; post &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-and-use-hugo-a-static-site-generator-on-ubuntu-14-04#adjusting-the-initial-configuration-for-your-site&#34; title=&#34;Quickstart Hugo&#34;&gt;here&lt;/a&gt; I think I&amp;rsquo;m a hugo-convert!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m (still) going to have to play about with writing in markdown just to get used to the command structure. Luckily, I found a good cheatsheet over &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34; title=&#34;Markdown Cheatsheet&#34;&gt;here&lt;/a&gt; and another &lt;a href=&#34;https://sourceforge.net/p/hugo-generator/wiki/markdown_syntax/&#34; title=&#34;Sourceforge Markdown&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Eventually, I want this site to build up into small sampling of what I&amp;rsquo;ve been learning to do whilst learning Python and applying it to Machine Learning problems. It may at times become quite specialised with my research in medical imaging, but the general processes (I hope) should benefit at least one other person in the world.&lt;/p&gt;

&lt;p&gt;Here goes!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/em&gt;: Just to test out the code-block highlighting&amp;hellip;
&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;def hello_world():
    print &amp;ldquo;Hello there!&amp;rdquo;
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>/about/</link>
      <pubDate>Wed, 01 Mar 2017 10:06:20 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>

&lt;p&gt;Welcome to my Machine Learning Notebook.&lt;/p&gt;

&lt;h2 id=&#34;my-aim&#34;&gt;My Aim&lt;/h2&gt;

&lt;p&gt;Primarily, this website is being created to keep track of the things that I&amp;rsquo;ve been learning over the course of my PhD research. I use Python to create Machine Learning algorithms with a focus on Biomedical Image Analysis - but the concepts are just as applicable elsewhere.&lt;/p&gt;

&lt;p&gt;After noticing that I kept forgetting the little things and sometimes feeling like I wasn&amp;rsquo;t really understanding what was going on, I decided to keep a record of the small problems I&amp;rsquo;ve bene having (and their solutions) as well as providing code snippets and tutorials to remind myself in the future. I hope that they will prove useful to at least one other person in the world too.&lt;/p&gt;

&lt;h2 id=&#34;my-research&#34;&gt;My Research&lt;/h2&gt;

&lt;p&gt;Whilst a lot of my work will remain the intellectual property of my current organisation, the things that I post on this site are of my own creation and unrelated to my actual research; the basics of what I&amp;rsquo;m writing here can be found scattered throughout the web, but I just wanted to pull a more consistent source of information together.&lt;/p&gt;

&lt;h2 id=&#34;copyright-and-intellectual-property&#34;&gt;Copyright and Intellectual Property&lt;/h2&gt;

&lt;p&gt;Whilst I&amp;rsquo;m writing my tutorials and code snippets for my own benefit and providing them on an opensource platform, it would be nice to see my name or webaddress where others have found use for them. Equally, anything I use on this stie I will try to properly credit and cite.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;banner&#34; src=&#34;/img/banner.jpg&#34; title=&#34;CCImage&#34; &lt;p style=&#34;text-align: right&#34;&gt;&lt;a href=&#34;https://pixabay.com/en/users/geralt-9301/&#34;&gt;&lt;em&gt;pixabay:geralt&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/</guid>
      <description>partial &#34;header.html&#34; . }}

&lt;div class=&#34;header&#34;&gt;
  &lt;h1&gt;{{ .Title }}&lt;/h1&gt;
  &lt;h2&gt;{{ .Site.Params.subtitle }}&lt;/h2&gt;
&lt;/div&gt;

&lt;br&gt;
&lt;div align=&#34;center&#34;&gt;
  &lt;img title=&#34;pixabay:geralt&#34; class=&#34;banner&#34; src=&#34;/images/banner.jpg&#34;&gt;
&lt;/div&gt;

&lt;div class=&#34;content&#34;&gt;
  {{ range ( .Paginate (where .Data.Pages &#34;Type&#34; &#34;post&#34;)).Pages }}
    {{ .Render &#34;summary&#34;}}
  {{ end }}

  {{ partial &#34;pagination.html&#34; . }}

&lt;/div&gt;

{{ partial &#34;footer.html&#34; . }}
</description>
    </item>
    
  </channel>
</rss>