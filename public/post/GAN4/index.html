<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="The GAN Class and Data Functions">
  <meta name="generator" content="Hugo 0.19" />

  <title>Generative Adversarial Network (GAN) in TensorFlow - Part 4 &middot; Machine Learning Notebook</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  

  
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokai-sublime.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  



  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  
    <link rel="stylesheet" href="/css/additional.css">
  
    <link rel="stylesheet" href="/css/toc.css">
  
  
    <script src="/js/toc.js"></script>
  
  
  <meta name="google-site-verification" content="9K5gUhw2zLi94y-8-ZbWmpsZW1Ke4J3zvl62FYxm-pY" />

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="/">MLNotebook</a>



  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/contact/"><i class='fa fa-phone fa-fw'></i>Contact</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/mlnotebook" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://facebook.com/machineln" target="_blank"><i class="fa fa-facebook-square fa-fw"></i>Facebook</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/robdrobinson" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/mlnotebook" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small></small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>


</div>


  <div id="main">


<div class="header">
  <h1>Generative Adversarial Network (GAN) in TensorFlow - Part 4</h1>
  <h2>The GAN Class and Data Functions</h2>
</div>
<div class="content">

<div class="header_container">

<div class="featured_image_container">

  <img class="featured_image" src="/img/featgan4.png">

 </div>
 
 <div class="post_meta_container">
  
  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>17 Jul 2017, 09:37</time>
  </div>

  

  
  
  
  <div>
    <i class="fa fa-folder fa-fw"></i>
    
      <a class="post-taxonomy-topic" href="/topics/tutorial">tutorial</a>
    
  </div>
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="/tags/gan">GAN</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="/tags/cnn">CNN</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="/tags/machine-learning">machine learning</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="/tags/generative">generative</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="/tags/tensorflow">tensorflow</a>
    
  </div>
  
  
  
<ul class="share-buttons"><li><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmlnotebook.github.io&t=" title="Share on Facebook" target="_blank" onclick="window.open('https://www.facebook.com/sharer/sharer.php?u=' + encodeURIComponent(document.URL) + '&t=' + encodeURIComponent(document.URL),'','width=500,height=300'); return false;"><img alt="Share on facebook" src="/img/facebook.png"></a></li>
  <li><a href="https://twitter.com/intent/tweet?source=https%3A%2F%2Fmlnotebook.github.io&text=:%20https%3A%2F%2Fmlnotebook.github.io&via=mlnotebook" target="_blank" title="Tweet" onclick="window.open('https://twitter.com/intent/tweet?text=' + encodeURIComponent(document.title) + ':%20'  + encodeURIComponent(document.URL),'','width=500,height=300'); return false;"><img alt="Tweet" src="/img/twitter.png"></a></li>
  <li><a href="http://www.reddit.com/submit?url=https%3A%2F%2Fmlnotebook.github.io&title=" target="_blank" title="Submit to Reddit" onclick="window.open('http://www.reddit.com/submit?url=' + encodeURIComponent(document.URL) + '&title=' +  encodeURIComponent(document.title),'','width=500,height=300'); return false;"><img alt="Submit to Reddit" src="/img/reddit.png"></a></li>
  <li><a href="http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fmlnotebook.github.io&title=&summary=&source=https%3A%2F%2Fmlnotebook.github.io" target="_blank" title="Share on LinkedIn" onclick="window.open('http://www.linkedin.com/shareArticle?mini=true&url=' + encodeURIComponent(document.URL) + '&title=' +  encodeURIComponent(document.title),'','width=500,height=300'); return false;"><img alt="Share on LinkedIn" src="/img/linkedin.png"></a></li></ul>	
  
</div>


</div>

</div>


  <p>Now that we&rsquo;re able to import images into our network, we really need to build the GAN iteself. This tuorial will build the GAN <code>class</code> including the methods needed to create the generator and discriminator. We&rsquo;ll also be looking at some of the data functions needed to make this work.</p>

<p></p>

<p>*Note: This table of contents does not follow the order in the post. The contents is grouped by the methods in the GAN <code>class</code> and the functions in <code>gantut_imgfuncs.py</code>.</p>

<div id="toctop"></div>

<ol>
<li><a href="#intro">Introduction</a></li>
<li><a href="#gan">The GAN</a>

<ul>
<li><a href="#datasetfiles">dataset_files()</a></li>
<li><a href="#dcgan">GAN Class</a>

<ul>
<li><a href="#init">__init__()</a></li>
<li><a href="#discriminator">discriminator()</a></li>
<li><a href="#generator">generator()</a></li>
<li><a href="#buildmodel">build_model()</a></li>
<li><a href="#save">save()</a></li>
<li><a href="#load">load()</a></li>
<li><a href="#train">train()</a><br /></li>
</ul></li>
<li><a href="#batchnorm">Data Functions</a>

<ul>
<li><a href="#batchnorm">batch_norm()</a></li>
<li><a href="#conv2d">conv2d()</a></li>
<li><a href="#relu">relu()</a></li>
<li><a href="#linear">linear()</a></li>
<li><a href="#conv2dtrans">conv2d_transpose()</a><br /></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<h2 id="intro"> Introduction </h2>

<p>In the last tutorial, we build the functions in <code>gantut_imgfuncs.py</code>which allow us to import data into our networks. The completed file is <a href="/docs/GAN/gantut_imgfuncs_complete.py" title="gantut_imgfuncs_complete.py">here</a>. In this tutorial we will be working on the final two code skeletons:</p>

<ul>
<li><a href="/docs/GAN/gantut_gan.py" title="gantut_gan.py"><code>gantut_gan.py</code></a></li>
<li><a href="/docs/GAN/gantut_datafuncs.py" title="gantut_datafuncs.py"><code>gantut_datafuncs.py</code></a></li>
</ul>

<p>First, let&rsquo;s take a look at the various parts of our GAN in the <code>gantut_gan.py</code> file and see what they&rsquo;re going to do.</p>

<h2 id="gan"> The GAN </h2>

<p>We&rsquo;re going to import a number of modules for this file including those from our own <code>gantut_datafuncs.py</code> and <code>gantut_imgfuncs.py</code>:</p>

<pre><code class="language-python">from __future__ import division
import os
import time
import math
import itertools
from glob import glob
import tensorflow as tf
import numpy as np
from six.moves import xrange

#IMPORT OUR IMAGE AND DATA FUNCTIONS
from gantut_datafuncs import *
from gantut_imgfuncs import *
</code></pre>

<h3 id="datasetfiles"> dataset_files() </h3>

<p>The initial part of this file is a little housekeeping - ensuring that we are only dealing with supported filetypes. This way of doing things I liked in <a href="http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks" title="B. Amos">B. Amos blog</a>. We define accepted file-extensions and then return a list of all of the possible files we can use for training purposes. the <code>itertools.chain.from_iterable</code> function is useful for create a single <code>list</code> of all of the files found in the folders and subfolders of a particular <code>root</code> with an appropriate <code>ext</code>. Notice that it doesn&rsquo;t really matter what we call the images, so this will work for all datasets.</p>

<pre><code class="language-python">SUPPORTED_EXTENSIONS = [&quot;png&quot;, &quot;jpg&quot;, &quot;jpeg&quot;]

&quot;&quot;&quot; Returns the list of all SUPPORTED image files in the directory
&quot;&quot;&quot;
def dataset_files(root):
    return list(itertools.chain.from_iterable(
    glob(os.path.join(root, &quot;*.{}&quot;.format(ext))) for ext in SUPPORTED_EXTENSIONS))
</code></pre>

<hr>

<h3 id="dcgan"> DCGAN() </h3>

<p>This is where the hard work begins. We&rsquo;re going to build the DCGAN <code>class</code> (i.e. Deep Convolutional Generative Adversarial Network). The skeleton code already has the necessary method names for our model, let&rsquo;s have a look at what we&rsquo;ve got to create:</p>

<ul>
<li><code>__init__</code>:  &emsp;to initialise the model and set parameters</li>
<li><code>build_model</code>: &emsp;creates the model (or &lsquo;graph&rsquo; in TensorFlow-speak) by calling&hellip;</li>
<li><code>generator</code>: &emsp;defines the generator network</li>
<li><code>discriminator</code>: &emsp;defines the discriminator network</li>
<li><code>train</code>: &emsp;is called to begin the training of the network with data</li>
<li><code>save</code>: &emsp;saves the TensorFlow checkpoints of the GAN</li>
<li><code>load</code>: &emsp;loads the TensorFlow checkpoints of the GAN</li>
</ul>

<p>We create an instance of our GAN class with <code>DCGAN(args)</code> and be returned a DCGAN object with the above methods. Let&rsquo;s code.</p>

<h4 id="init"> __init__() </h4>

<p>To initialise our GAN object, we need some initial parameters. It looks like this:</p>

<pre><code class="language-python">def __init__(self, sess, image_size=64, is_crop=False, batch_size=64, sample_size=64, z_dim=100,
             gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, checkpoint_dir=None, lam=0.1):
</code></pre>

<p>The parameters are:</p>

<ul>
<li><code>sess</code>: &emsp; the TensorFlow session to run in</li>
<li><code>image_size</code>: &emsp; the width of the images, which should be the same as the height as we like square inputs</li>
<li><code>is_crop</code>: &emsp; whether to crop the images or leave them as they are</li>
<li><code>batch_size</code>: &emsp; number of images to use in each run</li>
<li><code>sample_size</code>: &emsp; number of z samples to take on each run, should be equal to batch_size</li>
<li><code>z_dim</code>: &emsp; number of samples to take for each z</li>
<li><code>gf_dim</code>: &emsp; dimension of generator filters in first conv layer</li>
<li><code>df_dim</code>: &emsp; dimenstion of discriminator filters in first conv layer</li>
<li><code>gfc_dim</code>: &emsp; dimension of generator units for fully-connected layer</li>
<li><code>dfc_gim</code>: &emsp; dimension of discriminator units for fully-connected layer</li>
<li><code>c_dim</code>: &emsp; number of image cannels (gray=1, RGB=3)</li>
<li><code>checkpoint_dir</code>: &emsp; where to store the TensorFlow checkpoints</li>
<li><code>lam</code>: &emsp;small constant weight for the sum of contextual and perceptual loss</li>
</ul>

<p>These are the controllable parameters for the GAN. As this is the initialising function, we need to transfer these inputs to the <code>self</code> of the class so they are accessible later on. We will also add two new lines:</p>

<ul>
<li><p>Let&rsquo;s add a check that the <code>image_size</code> is a power of 2 (to make the convolution work well). This clever &lsquo;bit-wise-and&rsquo; operator <code>&amp;</code> will do the job for us. It uses the unique property of all power of 2 numbers have only one bit set to <code>1</code> and all others to <code>0</code>. Let&rsquo;s also check that the image is bigger than $[8  \times 8]$ to we don&rsquo;t convolve too far:</p></li>

<li><p>Get the <code>image_shape</code> which is the width and height of the image along with the number of channels (gray or RBG).</p></li>
</ul>

<pre><code class="language-python">#image_size must be power of 2 and 8+
assert(image_size &amp; (image_size - 1) == 0 and image_size &gt;= 8)

self.sess = sess
self.is_crop = is_crop
self.batch_size = batch_size
self.image_size = image_size
self.sample_size = sample_size
self.image_shape = [image_size, image_size, c_dim]

self.z_dim = z_dim
self.gf_dim = gf_dim
self.df_dim = df_dim        
self.gfc_dim = gfc_dim
self.dfc_dim = dfc_dim

self.lam = lam
self.c_dim = c_dim
</code></pre>

<p>Later on, we will want to do &lsquo;batch normalisation&rsquo; on our data to make sure non of our images are extremely different to the others. We will need a batch-norm layer for each of the conv layers in our generator and discriminator. We will initialise the layers here, but define them in our <code>gantut_datafuncs.py</code> file shortly.</p>

<pre><code class="language-python">#batchnorm (from funcs.py)
self.d_bns = [batch_norm(name='d_bn{}'.format(i,)) for i in range(4)]

log_size = int(math.log(image_size) / math.log(2))
self.g_bns = [batch_norm(name='g_bn{}'.format(i,)) for i in range(log_size)]
</code></pre>

<p>This shows that we will be using 4 layers in our discriminator. But we will need more in our generator: our generator starts with a simple vector <em>z</em> and needs to upscale to the size of <code>image_size</code>. It does this by a factor of 2 in each layer, thus $\log(\mathrm{image \ size})/\log(2)$ is equal to the number of upsamplings to be done i.e. $2^{\mathrm{num \ of \ layers}} = 64$ in our case. Also note that we&rsquo;ve created these objects (layers) with an iterator so that each has the name <code>g_bn1</code>, <code>g_bn1</code> etc.</p>

<p>To finish <code>__init__()</code> we set the checkpoint directory for TensorFlow saves, instruct the class to build the model and name it &lsquo;DCGAN.model&rsquo;.</p>

<pre><code class="language-python">self.checkpoint_dir = checkpoint_dir
self.build_model()

self.model_name=&quot;DCGAN.model&quot;
</code></pre>

<hr>

<h4 id="batchnorm"> batch_norm() </h4>

<p>This is the first of our <code>gantut_datafuncs.py</code> functions.</p>

<p>If some of our images are very different to the others then the network will not learn the features correctly. To avoid this, we add batch normalisation (as described in <a href="http://arxiv.org/abs/1502.03167" title="Batch Normalization: Sergey Ioffe, Christian Szegedy">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Ioffe &amp; Szegedy (2015)</a>. We effectively redistribute the intensities of the images around a common mean with a set variance.</p>

<p>This is a <code>class</code> that will be instantiated with set parameters when called. Then, the method will perform batch normalisation whenever the object is called on the set of images <code>x</code>. We are using Tensorflow&rsquo;s built-in <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm" title="tf.contrib.layers.batch_norm">tf.contrib.layers.batch_norm()</a> layer for this which implements the method from the paper above.</p>

<p><em>Parameters</em></p>

<ul>
<li><code>epsilon</code>:    &lsquo;small float added to variance [of the input data] to avoid division by 0&rsquo;</li>
<li><code>momentum</code>:   &lsquo;decay value for the moving average, usually 0.999, 0.99, 0.9&rsquo;</li>
</ul>

<p><em>Inputs</em></p>

<ul>
<li><code>x</code>:      the set of input images to be normalised</li>
<li><code>train</code>:  whether or not the network is in training mode [True or False]</li>
</ul>

<p><em>Returns</em></p>

<ul>
<li>A batch_norm &lsquo;object&rsquo; on instantiation</li>
<li>A tensor representing the output of the batch_norm operation</li>
</ul>

<pre><code class="language-python">&quot;&quot;&quot;Batch normalisation function to standardise the input
Initialises an object with all of the batch norm properties
When called, performs batch norm on input 'x'
&quot;&quot;&quot;
class batch_norm(object):
    def __init__(self, epsilon=1e-5, momentum = 0.9, name=&quot;batch_norm&quot;):
        with tf.variable_scope(name):
            self.epsilon = epsilon
            self.momentum = momentum

            self.name = name

    def __call__(self, x, train):
        return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon,
                                            center=True, scale=True, is_training=train, scope=self.name)
</code></pre>

<hr>

<h4 id="discriminator"> discriminator() </h4>

<p>As the discriminator is a simple <a href="/post/CNN1" title="MLNotebook: Convolutional Neural Network">convolutional neural network (CNN)</a> this will not take many lines. We will have to create a couple of wrapper functions that will perform the actual convolutions, but let&rsquo;s get the method written in <code>gantut_gan.py</code> first.</p>

<p>We want our discriminator to check a real <code>image</code>, save varaibles and then use the same variables to check a fake <code>image</code>. This way, if the images are fake, but fool the discriminator, we know we&rsquo;re on the right track. Thus we use the variable <code>reuse</code> when calling the <code>discriminator()</code> method - we will set it to <code>True</code> when we&rsquo;re using the fake images.</p>

<p>We add <code>tf.variable_scope()</code> to our functions so that when we visualise our graph in TensorBoard we can recognise the various pieces of our GAN.</p>

<p>Next are the definitions of the 4 layers of our discriminator. each one takes in the images, the kernel (filter) dimensions and has a name to identify it later on. Notice that we also call our <code>d_bns</code> objects which are the batch-norm objects that were set-up during instantiation of the GAN. These act on the result of the convolution before being passed through the non-linear <code>lrelu</code> function. The last layer is just a <code>linear</code> layer that outputs the unbounded results from the network.</p>

<p>As this is a classificaiton task (real or fake) we finish by returning the probabilities in the range $[0 \ 1]$ by applying the sigmoid function. The full output is also returned.</p>

<pre><code class="language-python">def discriminator(self, image, reuse=False):
	with tf.variable_scope(&quot;discriminator&quot;) as scope:
	    if reuse:
		scope.reuse_variables()
	   	    
	    h0 = lrelu(conv2d(image, self.df_dim, name='d_h00_conv'))
	    h1 = lrelu(self.d_bns[0](conv2d(h0, self.df_dim*2, name='d_h1_conv'), self.is_training))
	    h2 = lrelu(self.d_bns[1](conv2d(h1, self.df_dim*4, name='d_h2_conv'), self.is_training))
	    h3 = lrelu(self.d_bns[2](conv2d(h2, self.df_dim*8, name='d_h3_conv'), self.is_training))
	    h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h4_lin')
	    
	    return tf.nn.sigmoid(h4), h4
</code></pre>

<p>This method calls a couple of functions that we haven&rsquo;t defined yet: <code>cov2d</code>, <code>lrelu</code> and <code>linear</code> so lets do those now.</p>

<hr>

<h4 id="conv2d"> conv2d() </h4>

<p>This function we&rsquo;ve seen before in our <a href="/post/CNN1" title="MLNotebook: Convolutional Neural Networks">CNN</a> tutorial. We&rsquo;ve defined the weights <code>w</code> for each kernel which is <code>[k_h x k_w x number of images x number of kernels]</code>not forgetting that different weights are learned for different images. We&rsquo;ve initialised these weights using a standard, random sampling from a normal distribution with standard deviation <code>stddev</code>.</p>

<p>The convolution is done by TensorFlow&rsquo;s [tf.nn.conv2d]( &ldquo;tf.nn.conv2d&rdquo;) function using the weights <code>w</code> we&rsquo;ve already defined. The padding option <code>SAME</code> makes sure that we end up with output that is the same size as the input. Biases are added (the same size as the number of kernels and initialised at a constant value) before the result is returned.</p>

<p><em>Inputs</em></p>

<ul>
<li><code>input_</code>:     the input images (full batch)</li>
<li><code>output_dim</code>: the number of kernels/filters to be learned</li>
<li><code>k_h</code>, <code>k_w</code>:   height and width of the kernels to be learned</li>
<li><code>d_h</code>, <code>d_w</code>:   stride of the kernel horizontally and vertically</li>
<li><code>stddev</code>:     standard deviation for the normal func in weight-initialiser</li>
</ul>

<p><em>Returns</em></p>

<ul>
<li>the convolved images for each kernel</li>
</ul>

<pre><code class="language-python">&quot;&quot;&quot;Defines how to perform the convolution for the discriminator,
i.e. traditional conv rather than reverse conv for the generator
&quot;&quot;&quot;
def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;conv2d&quot;):
    with tf.variable_scope(name):
        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],
                            initializer=tf.truncated_normal_initializer(stddev=stddev))
        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')

        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))
        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        conv = tf.nn.bias_add(conv, biases)

        return conv 
</code></pre>

<hr>

<h4 id="relu"> relu() </h4>

<p>The network need to be able to learn complex functions, so we add some non-linearity to the output of our convolution layers. We&rsquo;ve seen this before in our tutorial on <a href="/post/transfer_functions" title="Transfer Functions">transfer functions</a>. Here we use the leaky rectified linear unit (lReLU).</p>

<p><em>Parameters</em></p>

<ul>
<li><code>leak</code>:   the &lsquo;leakiness&rsquo; of the lrelu</li>
</ul>

<p><em>Inputs</em></p>

<ul>
<li><code>x</code>: some data with a wide range</li>
</ul>

<p><em>Returns</em></p>

<ul>
<li>the transformed input data</li>
</ul>

<pre><code class="language-python">&quot;&quot;&quot;Neural nets need this non-linearity to build complex functions
&quot;&quot;&quot;
def lrelu(x, leak=0.2, name=&quot;lrelu&quot;):
    with tf.variable_scope(name):
        f1 = 0.5 * (1 + leak)
        f2 = 0.5 * (1 - leak)
        return f1 * x + f2 * abs(x)
</code></pre>

<hr>

<h4 id="linear"> linear() </h4>

<p>This linear layer takes the outputs from the convolution and does a linear transform using some randomly initialised weights. This does not have the same non-linear property as the <code>lrelu</code> function because we will use this output to calcluate probabilities for classification. We return the result of <code>input_ x matrix</code> by default, but if we also need the weights, we also output <code>matrix</code> and <code>bias</code> through the <code>if</code> statement.</p>

<p><em>Parameters</em></p>

<ul>
<li><code>stddev</code>:     standard deviation for weight initialiser</li>
<li><code>bias_start</code>: for the bias initialiser (constant value)</li>
<li><code>with_w</code>:     return the weight matrix (and biases) as well as the output if True</li>
</ul>

<p><em>Inputs</em></p>

<ul>
<li><code>input_</code>:         input data (shape is used to define weight/bias matrices)</li>
<li><code>output_size</code>:    desired output size of the linear layer</li>
</ul>

<pre><code class="language-python">&quot;&quot;&quot;For the final layer of the discriminator network to get the
full detail (probabilities etc.) from the output
&quot;&quot;&quot;
def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):
    shape = input_.get_shape().as_list()

    with tf.variable_scope(scope or &quot;Linear&quot;):
        matrix = tf.get_variable(&quot;Matrix&quot;, [shape[1], output_size], tf.float32,
                                 tf.random_normal_initializer(stddev=stddev))
        bias = tf.get_variable(&quot;bias&quot;, [output_size],
            initializer=tf.constant_initializer(bias_start))
        if with_w:
            return tf.matmul(input_, matrix) + bias, matrix, bias
        else:
            return tf.matmul(input_, matrix) + bias
</code></pre>

<hr>

<h4 id="generator"> generator() </h4>

<p>Finally! We&rsquo;re going to write the code for the generative part of the GAN. This method will take a single input - the randomly-sampled vector $z$ from the well known distribution $p_z$.</p>

<p>Remember that the generator is effectively a reverse discriminator in that it is a CNN that works backwards. Thus we start with the &lsquo;values&rsquo; and must perform the linear transformation on them before feeding them through the other layers of the network. As we do not know the weights or biases yet in this network, we need to make sure we output these from the linear layer with <code>with_w=True</code>.</p>

<p>This first hidden layer <code>hs[0]</code> needs reshaping to be the small image-shaped array that we can send through the network to become the upscaled $[64 \times 64]$ image at the end. So we take the linearly-transformed z-values and reshape to $[4 x 4 x num_kernels]$. Don&rsquo;t forget the <code>-1</code> to do this for all images in the batch. As before, we must batch-norm the result and pass it through the non-linearity.</p>

<p>The number of layers in this network has been calculated earlier (using the logarithm ratio of image size to downsampling factor. We can therefore do the next part of the generator in a loop.</p>

<p>In each loop/layer we are going to:</p>

<ol>
<li>give the layer a name</li>
<li>perform the <em>inverse</em> convolution</li>
<li>apply non-linearity</li>
</ol>

<p>1 and 3 are self-explanatory, but the inverse convolution function still needs to be written. This is the function that will take in the small square image and upsample it to a larger image using some weights that are being learnt. We start at layer <code>i=1</code> where we want the image to go to <code>size=8</code> from <code>size=4</code> at layer <code>i=0</code>. This will increase by a factor of 2 at each layer. As with a regular CNN we want to learn fewer kernels on the larger images, so we need to decrease the <code>depth_mul</code> by a factor of 2 at each layer. Note that the <code>while</code> loop will terminate when the size gets to the size of the input images <code>image_size</code>.</p>

<p>The final layer is added which takes the last output and does the inverse convolution to get the final fake image (that will be tested with the discriminator.</p>

<pre><code class="language-python">def generator(self, z):
	with tf.variable_scope(&quot;generator&quot;) as scope:
	    self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=True)

	    hs = [None]
	    hs[0] = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])
	    hs[0] = tf.nn.relu(self.g_bns[0](hs[0], self.is_training))
	    
	    i=1             #iteration number
	    depth_mul = 8   #depth decreases as spatial component increases
	    size=8          #size increases as depth decreases
	    
	    while size &lt; self.image_size:
		hs.append(None)
		name='g_h{}'.format(i)
		hs[i], _, _ = conv2d_transpose(hs[i-1], [self.batch_size, size, size, self.gf_dim*depth_mul],
		                                name=name, with_w=True)
		hs[i] = tf.nn.relu(self.g_bns[i](hs[i], self.is_training))
		
		i += 1
		depth_mul //= 2
		size *= 2
		
	    hs.append(None)
	    name = 'g_h{}'.format(i)
	    hs[i], _, _ = conv2d_transpose(hs[i-1], [self.batch_size, size, size, 3], name=name, with_w=True)
	    
	    return tf.nn.tanh(hs[i])           
</code></pre>

<hr>

<h4 id="conv2dtrans"> conv2d_transpose() </h4>

<p>The inverse convolution function looks very similar to the forward convolution function. We&rsquo;ve had to make sure that different versions of TensorFlow work here - in newer versions, the correct function is located at <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" title="tf.nn.conv2d_transpose">tf.nn.conv2d_transpose</a> where as in older ones we must use <code>tf.nn.deconv2d</code>.</p>

<p><em>Inputs</em></p>

<ul>
<li><code>input_</code>:         a vector (of noise) with dim=batch_size x z_dim</li>
<li><code>output_shape</code>:   the final shape of the generated image</li>
<li><code>k_h</code>, <code>k_w</code>:       the height and width of the kernels</li>
<li><code>d_h</code>, <code>d_w</code>:       the stride of the kernel horiz and vert.<br /></li>
</ul>

<p><em>Returns</em></p>

<ul>
<li>an image (upscaled from the initial data)</li>
</ul>

<pre><code class="language-python">&quot;&quot;&quot;Deconv isn't an accurate word, but is a handy shortener,
so we'll use that. This is for the generator that has to make
the image from some randomly sampled data
&quot;&quot;&quot;
def conv2d_transpose(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
                     name=&quot;conv2d_transpose&quot;, with_w=False):
    with tf.variable_scope(name):
        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],
                            initializer=tf.random_normal_initializer(stddev=stddev))

        try:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        # Support for verisons of TensorFlow before 0.7.0
        except AttributeError:
            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))
        # deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        deconv = tf.nn.bias_add(deconv, biases)

        if with_w:
            return deconv, w, biases
        else:
            return deconv    
</code></pre>

<hr>

<h4 id="buildmodel"> build_model() </h4>

<p>The <code>build_model()</code> method bring together the image data and the generator and discriminator methods. This is the &lsquo;graph&rsquo; for TensorFlow to follow. It contains some <code>tf.placeholder</code> pieces which we must supply attributes to when we finally train the model.</p>

<p>We will need to know whether the model is in training or inference mode throughout our code, so we have a placeholder for that variable. We also need a placeholder for the image data itself because there will be a different batch of data being injected at each epoch. These are our <code>real_images</code>.</p>

<p>When we inject the <code>z</code> vectors into the GAN (served by another palceholder) we will also produce some monitoring output for TensorBoard. By adding <code>tf.summary.histogram()</code> we are able to keep track of how the different <code>z</code> vectors look at each epoch.</p>

<pre><code class="language-python">    def build_model(self):
        self.is_training = tf.placeholder(tf.bool, name='is_training')
        self.images = tf.placeholder(
            tf.float32, [None] + self.image_shape, name='real_images')
        self.lowres_images = tf.reduce_mean(tf.reshape(self.images,
            [self.batch_size, self.lowres_size, self.lowres,
             self.lowres_size, self.lowres, self.c_dim]), [2, 4])
        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')
        self.z_sum = tf.summary.histogram(&quot;z&quot;, self.z)
</code></pre>

<p>Next, lets tell the graph to take the injected <code>z</code> vector an turn it into an image with our <code>generator</code>. We&rsquo;ll also produce a lowres version of this image. Now, put the &lsquo;real_images&rsquo; into the <code>discriminator</code>, which gives back our probabilities and the final-layer data (the logits). We then <code>reuse</code> the same discriminator parameters to test the fake image from the generator. Here we also output some histograms of the probabilities of the &lsquo;real_image&rsquo; and the fake image. We will also output the current fake image from the generator to TensorBoard.</p>

<pre><code class="language-python">        self.G = self.generator(self.z)
        self.lowres_G = tf.reduce_mean(tf.reshape(self.G,
            [self.batch_size, self.lowres_size, self.lowres,
             self.lowres_size, self.lowres, self.c_dim]), [2, 4])
        self.D, self.D_logits = self.discriminator(self.images)

        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)

        self.d_sum = tf.summary.histogram(&quot;d&quot;, self.D)
        self.d__sum = tf.summary.histogram(&quot;d_&quot;, self.D_)
        self.G_sum = tf.summary.image(&quot;G&quot;, self.G)
</code></pre>

<p>Now for some of the necessary calculations needed to be able to update the network. Let&rsquo;s find the &lsquo;loss&rsquo; on the current outputs. We will utilise a very efficient loss function here the <a href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits" title="tf.nn.sigmoid_cross_entropy_with_logits">tf.nn.sigmoid_cross_entropy_with_logits</a>. We want to calculate a few things:</p>

<ol>
<li>how well did the discriminator do at letting <em>true</em> images through (i.e. comparing <code>D</code> to <code>1</code>)</li>
<li>how often was the discriminator fooled by the generator  (i.e. comparing <code>D_</code> to <code>1</code>)</li>
<li>how often did the generator fail at making realistic images (i.e. comparing <code>D_</code> to <code>0</code>).</li>
</ol>

<p>We&rsquo;ll add the discriminator losses up (1 + 2) and create a TensorBoard summary statistic (a <code>scalar</code> value) for the discriminator and generator losses in this epoch. These are what we will optimise during training.</p>

<p>To keep everything tidy, we&rsquo;ll group the discriminator and generator variables into <code>d_vars</code> and <code>g_vars</code> respectively.</p>

<pre><code class="language-python">        self.d_loss_real = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,
                                                    labels=tf.ones_like(self.D)))
        self.d_loss_fake = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                                    labels=tf.zeros_like(self.D_)))
        self.g_loss = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                                    labels=tf.ones_like(self.D_)))

        self.d_loss_real_sum = tf.summary.scalar(&quot;d_loss_real&quot;, self.d_loss_real)
        self.d_loss_fake_sum = tf.summary.scalar(&quot;d_loss_fake&quot;, self.d_loss_fake)

        self.d_loss = self.d_loss_real + self.d_loss_fake

        self.g_loss_sum = tf.summary.scalar(&quot;g_loss&quot;, self.g_loss)
        self.d_loss_sum = tf.summary.scalar(&quot;d_loss&quot;, self.d_loss)

        t_vars = tf.trainable_variables()

        self.d_vars = [var for var in t_vars if 'd_' in var.name]
        self.g_vars = [var for var in t_vars if 'g_' in var.name]
</code></pre>

<p>We don&rsquo;t want t lose our progress, so lets make sure we setup the <code>tf.Saver()</code> function just keeping the most recent variables each time.</p>

<pre><code class="language-python">        self.saver = tf.train.Saver(max_to_keep=1)
</code></pre>

<hr>

<h4 id="save"> save() </h4>

<p>When we want to save a checkpoint (i.e. save all of the weights we&rsquo;ve learned) we will call this function. It will check whether the output directory exists, if not it will create it. Then it wll call the <a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#save" title="tf.train.Saver.save"><code>tf.train.Saver.save()</code></a> function which takes in the current session <code>sess</code>, the save directory, model name and keeps track of the number of steps that&rsquo;ve been done.</p>

<pre><code class="language-python">    def save(self, checkpoint_dir, step):
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
            
        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name), global_step=step)
</code></pre>

<hr>

<h4 id="load"> load() </h4>

<p>Equally, if we&rsquo;ve already spent a long time learning weights, we don&rsquo;t want to start from scratch every time we want to push the network further. This function will load the most recent checkpoint in the save directory. TensorFlow has build-in functions for checking out the most recent checkpoint. If there is no checkpoint available, the function returns false and the appropriate action is taken by the main method that called it.</p>

<pre><code class="language-python">    def load(self, checkpoint_dir):
        print(&quot; [*] Reading checkpoints...&quot;)
        
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            self.saver.restore(self.sess, ckpt.model_checkpoint_path)
            return True
        else:
            return False
</code></pre>

<hr>

<h4 id="train"> train() </h4>

<p>The all-important <code>train()</code> method. This is where the magic happens. When we call <code>DCGAN.train(config)</code> the networks will begin their fight and train. We will discuss the <code>config</code> argument later on, but succinctly: it&rsquo;s a list of all hyperparameters TensorFlow will use in the network. Here&rsquo;s how <code>train()</code> works:</p>

<p>First we give the trainer the data (using our <code>dataset_files</code> function) and make sure that it&rsquo;s randomly shuffled. We want to make sure that the images next to each other have nothing in common so that we can truly randomly sample them. There&rsquo;s also a check here `<code>assert(len(data) &gt; 0)</code> to make sure that we don&rsquo;t pass in an empty directory&hellip; that wouln&rsquo;t be useful to learn from.</p>

<pre><code class="language-python">def train(self, config):
	data = dataset_files(config.dataset)
	np.random.shuffle(data)
	assert(len(data) &gt; 0)
</code></pre>

<p>We&rsquo;re going to use the adaptive non-convex optimization method <a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" title="tf.train.AdamOptimizer"><code>tf.train.AdamOptimizer()</code></a> from <a href="https://arxiv.org/pdf/1412.6980.pdf" title="Adam: A Method for Stochastic Optimization">Kingma <em>et al</em> (2014)</a> to train out networks. Let&rsquo;s set this up for the discriminator (<code>d_optim</code>) and the generator (<code>g_optim</code>).</p>

<pre><code class="language-python">	d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.d_loss, var_list=self.d_vars)
	g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.g_loss, var_list=self.g_vars)
</code></pre>

<p>Next we will initialize all variables in the network (depending on TensorFlow version) and generate some <code>tf.summary</code> variables for TensorBoard which group together all of the summaries that we want to keep track of.</p>

<pre><code class="language-python">	try:
	    tf.global_variables_initializer().run()
	except:
	    tf.initialize_all_variables().run()
	    
	self.g_sum = tf.summary.merge([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])
	self.d_sum = tf.summary.merge([self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])
	self.writer = tf.summary.FileWriter(&quot;./logs&quot;, self.sess.graph)
</code></pre>

<p>So here&rsquo;s the part where we now sample this well-known distribution $p_z$ to get the noise vector $z$. We&rsquo;re using a <code>np.random.uniform</code> distribution. Keep a look out for this when we&rsquo;re watching the network in TensorBoard, we told the GAN <code>class</code> to output the histogram of $z$ vectors that are sampled from $p_z$. So they should all approximate to a uniform distribution.</p>

<p>We&rsquo;re also going to sample the input <em>real</em> image files we shuffled earlier taking <code>sample_size</code> images through to the training process. We will use these later on to assess the loss functions every now and again when we output some examples.</p>

<p>We need to load in the data using the function <code>get_image()</code> that we wrote into <code>gantut_imgfuncs.py</code> during the <a href="/post/GAN3" title="MLNotebook: GAN3">last tutorial</a>. After loading the images, lets make sure that they&rsquo;re all in one <code>np.array</code> ready to be used.</p>

<pre><code class="language-python">	sample_z = np.random.uniform(-1, 1, size=(self.sample_size, self.z_dim))

	sample_files = data[0:self.sample_size]
	sample = [get_image(sample_file, self.image_size, is_crop=self.is_crop) for sample_file in sample_files]
	sample_images = np.array(sample).astype(np.float32)
</code></pre>

<p>Set the epoch counter and get the start time (it can be frustrating if we can&rsquo;t see how long things are taking). We also want to be sure to load any previous checkpoints from TensorFlow before we start again from scratch.</p>

<pre><code class="language-python">	counter = 1
	start_time = time.time()

	if self.load(self.checkpoint_dir):
	    print(&quot;&quot;&quot; An existing model was found - delete the directory or specify a new one with --checkpoint_dir &quot;&quot;&quot;)
	else:
	    print(&quot;&quot;&quot; No model found - initializing a new one&quot;&quot;&quot;)
</code></pre>

<p>Here&rsquo;s the actual training bit taking place.  <code>For</code> each <code>epoch</code> that we&rsquo;ve assigned in <code>config</code>, we create two minibatches: a sampling of real images, and those generated from the $z$ vector. We then update the <code>discriminator</code> network before updating the <code>generator</code>. We also write these loss values to the TensorBoard summary. There are two things to notice:</p>

<ul>
<li><p>By calling <code>sess.run()</code> with specified variables in the first (or <code>fetch</code> attribute) we are able to keep the generator steady whilst updating the discriminator, and vice versa.</p></li>

<li><p>The generator is updated twice. This is to make sure that the discriminator loss function does not just converge to zero very quickly.</p></li>
</ul>

<pre><code class="language-python">	for epoch in xrange(config.epoch):
	    data = dataset_files(config.dataset)
	    batch_idxs = min(len(data), config.train_size) // self.batch_size
	    
	    for idx in xrange(0, batch_idxs):
		batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]
		batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop) for batch_file in batch_files]
		batch_images = np.array(batch).astype(np.float32)
		
		batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]).astype(np.float32)
		
		#update D network
		_, summary_str = self.sess.run([d_optim, self.d_sum],
		                               feed_dict={self.images: batch_images, self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)
		
		#update G network
		_, summary_str = self.sess.run([g_optim, self.g_sum],
		                               feed_dict={self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)
		
		#run g_optim twice to make sure that d_loss does not go to zero
		_, summary_str = self.sess.run([g_optim, self.g_sum],
		                               feed_dict={self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)

</code></pre>

<p>To get the errors needed for backpropagation, we evaluate <code>d_loss_fake</code>, <code>d_loss_real</code> and <code>g_loss</code>. We run the $z$ vector through the graph to get the fake loss and the generator loss, and use the real <code>batch_images</code> for the real loss.</p>

<pre><code class="language-python">		errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.is_training: False})
		errD_real = self.d_loss_real.eval({self.images: batch_images, self.is_training: False})
		errG = self.g_loss.eval({self.z: batch_z, self.is_training: False})
</code></pre>

<p>Let&rsquo;s get some output to <code>stdout</code> for the user. The current epoch and progress through the minibatches is output at each new minibatch. Every 100 minibatches we&rsquo;re going to evaluate the current generator <code>self.G</code> and calculate the loss against the small set of images we sampled earlier. We will output the result of the generator and use our <code>save_images()</code> function to create that image array we worked on in the last tutorial.</p>

<pre><code class="language-python">		counter += 1
		print(&quot;Epoch [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}&quot;.format(
		        epoch, idx, batch_idxs, time.time() - start_time, errD_fake + errD_real, errG))
		
		if np.mod(counter, 100) == 1:
		    samples, d_loss, g_loss = self.sess.run([self.G, self.d_loss, self.g_loss], 
		                                            feed_dict={self.z: sample_z, self.images: sample_images, self.is_training: False})
		    save_images(samples, [8,8], './samples/train_{:02d}-{:04d}.png'.format(epoch, idx))
		    print(&quot;[Sample] d_loss: {:.8f}, g_loss: {:.8f}&quot;.format(d_loss, g_loss))
</code></pre>

<p>Finally, we need to save the current weights from our networks.</p>

<pre><code class="language-python">		if np.mod(counter, 500) == 2:
		    self.save(config.checkpoint_dir, counter)
</code></pre>

<h2 id="conclusion"> Conclusion </h2>

<p>That&rsquo;s it! We&rsquo;ve completed the <code>gantut_gan.py</code> and <code>gantut_datafuncs.py</code> files. Checkout the completed files below:</p>

<p>Completed versions of:</p>

<ul>
<li><a href="/docs/GAN/gantut_trainer.py" title="gantut_trainer.py">gantut_trainer.py</a></li>
<li><a href="/docs/GAN/gantut_imgfuncs_complete.py" title="gantut_imgfuncs_complete.py">gantut_imgfuncs_complete.py</a></li>
<li><a href="/docs/GAN/gantut_datafuncs_complete.py" title="gantut_datafuncs_complete.py">gantut_datafuncs_complete.py</a></li>
<li><a href="/docs/GAN/gantut_gan_complete.py" title="gantut_gan_complete.py">gantut_gan_complete.py</a></li>
</ul>

<p>By following this tutorial series we should now have:</p>

<ol>
<li>A background in how GANs work</li>
<li>Necessary data, fullly pre-processed and ready to use</li>
<li>The <code>gantut_imgfuncs.py</code> for loading data into the neworks</li>
<li>A GAN <code>class</code> with the necessary methods in <code>gantut_gan.py</code> and the <code>gantut_datafuncs.py</code> we need to do the computations.</li>
</ol>

<p>In the final part of the series, we will run this network and take a look at the outputs in TensorBoard.</p>

  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="/post/GAN3/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="/post/GAN3/">Generative Adversarial Network (GAN) in TensorFlow - Part 3</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'mlnotebook-1';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>

</div>
</div>
<script src="/js/ui.js"></script>



<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { 	equationNumbers: { autoNumber: "AMS" },
         	extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-5586925126063559",
    enable_page_level_ads: true
  });
</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92930264-1', 'auto');
  ga('send', 'pageview');

</script>



</body>
</html>

