<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Machine Learning Notebook</title>
    <link>/post/index.xml</link>
    <description>Recent content in Posts on Machine Learning Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Mar 2017 09:55:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Simple Neural Network - With Numpy in Python</title>
      <link>/post/nn-in-python/</link>
      <pubDate>Wed, 15 Mar 2017 09:55:00 +0000</pubDate>
      
      <guid>/post/nn-in-python/</guid>
      <description>&lt;p&gt;Part 4 of our tutorial series on Simple Neural Networks. We&amp;rsquo;re ready to write our Python script! Having gone through the maths, vectorisation and activation functions, we&amp;rsquo;re now ready to put it all together and write it up. By the end of this tutorial, you will have a working NN in Python, using only numpy, which can be used to learn the output of logic gates (e.g. XOR)
&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferfunction&#34;&gt;Transfer Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backpropclass&#34;&gt;Back Propagation Class&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#initialisation&#34;&gt;Initialisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forwardpass&#34;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backprop&#34;&gt;Back Propagation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iterating&#34;&gt;Iterating&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;intro&#34;&gt; Introduction &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve &lt;a href=&#34;/post/neuralnetwork&#34;&gt;ploughed through the maths&lt;/a&gt;, then &lt;a href=&#34;/post/nn-more-maths&#34;&gt;some more&lt;/a&gt;, now we&amp;rsquo;re finally here! This tutorial will run through the coding up of a simple neural network (NN) in Python. We&amp;rsquo;re not going to use any fancy packages (though they obviously have their advantages in tools, speed, efficiency&amp;hellip;) we&amp;rsquo;re only going to use numpy!&lt;/p&gt;

&lt;p&gt;By the end of this tutorial, we will have built an algorithm which will create a neural network with as many layers (and nodes) as we want. It will be trained by taking in multiple training examples and running the back propagation algorithm many times.&lt;/p&gt;

&lt;p&gt;Here are the things we&amp;rsquo;re going to need to code:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The transfer functions&lt;/li&gt;
&lt;li&gt;The forward pass&lt;/li&gt;
&lt;li&gt;The back propagation algorithm&lt;/li&gt;
&lt;li&gt;The update function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To keep things nice and contained, the forward pass and back propagation algorithms should be coded into a class. We&amp;rsquo;re going to expect that we can build a NN by creating an instance of this class which has some internal functions (forward pass, delta calculation, back propagation, weight updates).&lt;/p&gt;

&lt;p&gt;First things first&amp;hellip; lets import numpy:&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span style=&#34;color: #f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;numpy&lt;/span&gt; &lt;span style=&#34;color: #f92672&#34;&gt;as&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now let&amp;rsquo;s go ahead and get the first bit done:&lt;/p&gt;

&lt;h2 id=&#34;transferfunction&#34;&gt; Transfer Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To begin with, we&amp;rsquo;ll focus on getting the network working with just one transfer function: the sigmoid function. As we discussed in a &lt;a href=&#34;/post/transfer-functions&#34;&gt;previous post&lt;/a&gt; this is very easy to code up because of its simple derivative:&lt;/p&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \frac{1}{1 + e^{  - x_{i}  }} \ \ \ \
f^{\prime}\left( x_{i} \right) = \sigma(x_{i}) \left( 1 -  \sigma(x_{i}) \right)
$$&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(x, Derivative=False):
	if not Derivative:
		return 1 / (1 + np.exp (-x))
	else:
		out = sigmoid(x)
		return out * (1 - out)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a succinct expression which actually calls itself in order to get a value to use in its derivative. We&amp;rsquo;ve used numpy&amp;rsquo;s exponential function to create the sigmoid function and created an &lt;code&gt;out&lt;/code&gt; variable to hold this in the derivative. Whenever we want to use this function, we can supply the parameter &lt;code&gt;True&lt;/code&gt; to get the derivative, We can omit this, or enter &lt;code&gt;False&lt;/code&gt; to just get the output of the sigmoid. This is the same function I used to get the graphs in the &lt;a href=&#34;/post/transfer-functions&#34;&gt;post on transfer functions&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;backpropclass&#34;&gt; Back Propagation Class &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fairly new to building my own classes in Python, but for this tutorial, I really relied on the videos of &lt;a href=&#34;https://www.youtube.com/playlist?list=PLRyu4ecIE9tibdzuhJr94uQeKnOFkkbq6&#34;&gt;Ryan on YouTube&lt;/a&gt;. Some of his hacks were very useful so I&amp;rsquo;ve taken some of those on board, but i&amp;rsquo;ve made a lot of the variables more self-explanatory.&lt;/p&gt;

&lt;p&gt;First we&amp;rsquo;re going to get the skeleton of the class setup. This means that whenever we create a new variable with the class of &lt;code&gt;backPropNN&lt;/code&gt;, it will be able to access all of the functions and variables within itself.&lt;/p&gt;

&lt;p&gt;It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class backPropNN:
    &amp;quot;&amp;quot;&amp;quot;Class defining a NN using Back Propagation&amp;quot;&amp;quot;&amp;quot;
    
    # Class Members (internal variables that are accessed with backPropNN.member) 
    numLayers = 0
    shape = None
    weights = []
    
    # Class Methods (internal functions that can be called)
    
    def __init__(self):
        &amp;quot;&amp;quot;&amp;quot;Initialise the NN - setup the layers and initial weights&amp;quot;&amp;quot;&amp;quot;
        
    # Forward Pass method
    def FP(self):
    	&amp;quot;&amp;quot;&amp;quot;Get the input data and run it through the NN&amp;quot;&amp;quot;&amp;quot;
    	 
    # TrainEpoch method
    def backProp(self):
        &amp;quot;&amp;quot;&amp;quot;Get the error, deltas and back propagate to update the weights&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve not added any detail to the functions (or methods) yet, but we know there needs to be an &lt;code&gt;__init__&lt;/code&gt; method for any class, plus we&amp;rsquo;re going to want to be able to do a forward pass and then back propagate the error.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve also added a few class members, variables which can be called from an instance of the &lt;code&gt;backPropNN&lt;/code&gt; class. &lt;code&gt;numLayers&lt;/code&gt; is just that, a count of the number of layers in the network, initialised to &lt;code&gt;0&lt;/code&gt;.  The &lt;code&gt;shape&lt;/code&gt; of the network will return the size of each layer of the network in an array and the &lt;code&gt;weights&lt;/code&gt; will return an array of the weights across the network.&lt;/p&gt;

&lt;h3 id=&#34;initialisation&#34;&gt; Initialisation &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to make the user supply an input variablewhich is the size of the layers in the network i.e. the number of nodes in each layer: &lt;code&gt;numNodes&lt;/code&gt;. This will be an array which is the length of the number of layers (including the input and output layers) where each element is the number of nodes in that layer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, numNodes):
	&amp;quot;&amp;quot;&amp;quot;Initialise the NN - setup the layers and initial weights&amp;quot;&amp;quot;&amp;quot;

	# Layer information
	self.numLayers = len(numNodes) - 1
	self.shape = numNodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve told our network to ignore the input layer when counting the number of layers (common practice) and that the shape of the network should be returned as the input array &lt;code&gt;numNodes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Lets also initialise the weights. We will take the approach of initialising all of the weights to small, random numbers. To keep the code succinct, we&amp;rsquo;ll use a neat function&lt;code&gt;zip&lt;/code&gt;. &lt;code&gt;zip&lt;/code&gt; is a function which takes two vectors and pairs up the elements in corresponding locations (like a zip). For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A = [1, 2, 3]
B = [4, 5, 6]

zip(A,B)
[(1,4), (2,5), (3,6)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why might this be useful? Well, when we talk about weights we&amp;rsquo;re talking about the connections between layers. Lets say we have &lt;code&gt;numNodes=(2, 2, 1)&lt;/code&gt; i.e. a 2 layer network with 2 inputs, 1 output and 2 nodes in the hidden layer. Then we need to let the algorithm know that we expect two input nodes to send weights to 2 hidden nodes. Then 2 hidden nodes to send weights to 1 output node, or &lt;code&gt;[(2,2), (2,1)]&lt;/code&gt;. Note that overall we will have 4 weights from the input to the hidden layer, and 2 weights from the hidden to the output layer.&lt;/p&gt;

&lt;p&gt;What is our &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; in the code above that will give us &lt;code&gt;[(2,2), (2,1)]&lt;/code&gt;? It&amp;rsquo;s this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numNodes = (2,2,1)
A = numNodes[:-1]
B = numNodes[1:]

A
(2,2)
B
(2,1)
zip(A,B)
[(2,2), (2,1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! So each pair represents the nodes between which we need initialise some weights. In fact, the shape of each pair &lt;code&gt;(2,2)&lt;/code&gt; is the clue to how many weights we are going to need between each layer e.g. between the input and hidden layers we are going to need &lt;code&gt;(2 x 2) =4&lt;/code&gt; weights.&lt;/p&gt;

&lt;p&gt;so &lt;code&gt;for&lt;/code&gt; each pair &lt;code&gt;in zip(A,B)&lt;/code&gt; (hint hint) we need to &lt;code&gt;append&lt;/code&gt; some weights into that empty weight matrix we initialised earlier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialise the weight arrays
for (l1,l2) in zip(numNodes[:-1],numNodes[1:]):
    self.weights.append(np.random.normal(scale=0.1,size=(l2,l1+1)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;self.weights&lt;/code&gt; as we&amp;rsquo;re appending to the class member initialised earlier. We&amp;rsquo;re using the numpy random number generator from a &lt;code&gt;normal&lt;/code&gt; distribution. The &lt;code&gt;scale&lt;/code&gt; just tells numpy to choose numbers around the 0.1 kind of mark and that we want a matrix of results which is the size of the tuple &lt;code&gt;(l2,l1+1)&lt;/code&gt;. Huh, &lt;code&gt;+1&lt;/code&gt;? Don&amp;rsquo;t think we&amp;rsquo;re getting away without including the &lt;em&gt;bias&lt;/em&gt; term! We want a random starting point even for the weight connecting the bias node (&lt;code&gt;=1&lt;/code&gt;) to the next layer. Ok, but why this way and not &lt;code&gt;(l1+1,l2)&lt;/code&gt;? Well, we&amp;rsquo;re looking for &lt;code&gt;l2&lt;/code&gt; connections from each of the &lt;code&gt;l1+1&lt;/code&gt; nodes in the previous layer - think of it as (number of observations x number of features). We&amp;rsquo;re creating a matrix of weights which goes across the nodes and down the weights from each node, or as we&amp;rsquo;ve seen in our maths tutorial:&lt;/p&gt;

&lt;div&gt;$$
W_{ij} = \begin{pmatrix} w_{11} &amp; w_{21} &amp; w_{31} \\ w_{12} &amp;w_{22} &amp; w_{32} \end{pmatrix}, \ \ \ \

W_{jk} = \begin{pmatrix} w_{11} &amp; w_{21} &amp; w_{31} \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;Between the first two layers, and second 2 layers respectively with node 3 being the bias node.&lt;/p&gt;

&lt;p&gt;Before we move on, lets also put in some placeholders in &lt;code&gt;__init__&lt;/code&gt; for the input and output values to each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self._layerInput = []
self._layerOutput = []
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;forwardpass&#34;&gt; Forward Pass &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve now initialised out network enough to be able to focus on the forward pass (FP).&lt;/p&gt;

&lt;p&gt;Our &lt;code&gt;FP&lt;/code&gt; function needs to have the input data. It needs to know how many training examples it&amp;rsquo;s going to have to go through, and it will need to reassign the inputs and outputs at each layer, so lets clean those at the beginning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def FP(self,input):

	numExamples = input.shape[0]

	# Clean away the values from the previous layer
	self._layerInput = []
	self._layerOutput = []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So lets propagate. We already have a matrix of (randomly initialised) weights. We just need to know what the input is to each of the layers. We&amp;rsquo;ll separate this into the first hidden layer, and subsequent hidden layers.&lt;/p&gt;

&lt;p&gt;For the first hidden layer we will write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, numExamples])]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s break this down:&lt;/p&gt;

&lt;p&gt;Our training example inputs need to match the weights that we&amp;rsquo;ve already created. We expect that our examples will come in rows of an array with columns acting as features, something like &lt;code&gt;[(0,0), (0,1),(1,1),(1,0)]&lt;/code&gt;. We can use numpy&amp;rsquo;s &lt;code&gt;vstack&lt;/code&gt; to put each of these examples one on top of the other.&lt;/p&gt;

&lt;p&gt;Each of the input examples is a matrix which will be multiplied by the weight matrix to get the input to the current layer:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{x_{J}} = \mathbf{W_{IJ} \vec{\mathcal{O}}_{I}}
$$&lt;/div&gt;

&lt;p&gt;where $\mathbf{x_{J}}$ are the inputs to the layer $J$ and  $\mathbf{\vec{\mathcal{O}}_{I}}$ is the output from the precious layer (the input examples in this case).&lt;/p&gt;

&lt;p&gt;So given a set of $n$ input examples we &lt;code&gt;vstack&lt;/code&gt; them so we just have &lt;code&gt;(n x numInputNodes)&lt;/code&gt;. We want to transpose this, &lt;code&gt;(numInputNodes x n)&lt;/code&gt; such that we can multiply by the weight matrix which is &lt;code&gt;(numOutputNodes x numInputNodes)&lt;/code&gt;. This gives an input to the layer which is &lt;code&gt;(numOutputNodes x n)&lt;/code&gt; as we expect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; we&amp;rsquo;re actually going to do the transposition first before doing the &lt;code&gt;vstack&lt;/code&gt; - this does exactly the same thing, but it also allows us to more easily add the bias nodes in to each input.&lt;/p&gt;

&lt;p&gt;Bias! Lets not forget this: we add a bias node which always has the value &lt;code&gt;1&lt;/code&gt; to each input (including the input layer). So our actual method is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Transpose the inputs &lt;code&gt;input.T&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add a row of ones to the bottom (one bias node for each input) &lt;code&gt;[input.T, np.ones([1,numExamples])]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vstack&lt;/code&gt; this to compact the array &lt;code&gt;np.vstack(...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Multipy with the weights connecting from the previous to the current layer &lt;code&gt;self.weights[0].dot(...)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But what about the subsequent hidden layers? We&amp;rsquo;re not using the input examples in these layers, we are using the output from the previous layer &lt;code&gt;[self._layerOutput[-1]]&lt;/code&gt; (multiplied by the weights).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for index in range(self.numLayers):
#Get input to the layer
if index ==0:
        layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, numExamples])]))
else:
        layerInput = self.weights[index].dot(np.vstack([self._layerOutput[-1],np.ones([1,numExamples])]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure to save this output, but also to now calculate the output of the current layer i.e.:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{ \vec{ \mathcal{O}}_{J}} = \sigma(\mathbf{x_{J}})
$$&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self._layerInput.append(layerInput)
self._layerOutput.append(sigmoid(layerInput))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, make sure that we&amp;rsquo;re returning the data from our output layer the same way that we got it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;return self._layerOutput[-1].T
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;backprop&#34;&gt;Back Propagation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve successfully sent the data from the input layer to the output layer using some initially randomised weights &lt;strong&gt;and&lt;/strong&gt; we&amp;rsquo;ve included the bias term (a kind of threshold on the activation functions). Our vectorised equations from the previous post will now come into play:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}

\mathbf{\vec{\delta}_{K}} &amp;= \sigma^{\prime}\left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}} \right) * \left( \mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}\right) \\[0.5em]

\mathbf{ \vec{ \delta }_{J}} &amp;= \sigma^{\prime} \left( \mathbf{ W_{IJ} \mathcal{O}_{I} } \right) * \mathbf{ W^{\intercal}_{JK}} \mathbf{ \vec{\delta}_{K}}

\end{align}
$$&lt;/div&gt;

&lt;div&gt;$$
\begin{align}

\mathbf{W_{JK}} + \Delta \mathbf{W_{JK}} &amp;\rightarrow \mathbf{W_{JK}}, \ \ \ \Delta \mathbf{W_{JK}} = -\eta \mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}} \\[0.5em]

\vec{\theta}  + \Delta \vec{\theta}  &amp;\rightarrow \vec{\theta}, \ \ \ \Delta \vec{\theta} = -\eta \mathbf{ \vec{ \delta }_{K}} 

\end{align}
$$&lt;/div&gt;

&lt;p&gt;With $*$ representing an elementwise multiplication between the matrices.&lt;/p&gt;

&lt;p&gt;First, lets initialise some variables and get the error on the output of the output layer. We assume that the target values have been formatted in the same way as the input values i.e. they are a row-vector per input example. In our forward propagation method, the outputs are stored as column-vectors, thus the targets have to be transposed. We will need to supply the input data, the target data and  $\eta$, the learning rate, which we will set at some small number for default. So we start back propagation by first initialising a placeholder for the deltas and getting the number of training examples before running them through the &lt;code&gt;FP&lt;/code&gt; method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backProp(self, input, target, trainingRate = 0.2):
&amp;quot;&amp;quot;&amp;quot;Get the error, deltas and back propagate to update the weights&amp;quot;&amp;quot;&amp;quot;

delta = []
numExamples = input.shape[0]

# Do the forward pass
self.FP(input)

output_delta = self._layerOutput[index] - target.T
error = np.sum(output_delta**2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We know from previous posts that the error is squared to get rid of the negatives. From this we compute the deltas for the output layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;delta.append(output_delta * sigmoid(self._layerInput[index], True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have the error but need to know what direction to alter the weights in, thus the gradient of the inputs to the layer need to be known. So, we get the gradient of the activation function at the input to the layer and get the product with the error. Notice we&amp;rsquo;ve supplied &lt;code&gt;True&lt;/code&gt; to the sigmoid function to get its derivative.&lt;/p&gt;

&lt;p&gt;This is the delta for the output layer. So this calculation is only done when we&amp;rsquo;re considering the index at the end of the network. We should be careful that when telling the algorithm that this is the &amp;ldquo;last layer&amp;rdquo; we take account of the zero-indexing in Python i.e. the last layer is &lt;code&gt;self.numLayers - 1&lt;/code&gt; i.e. in a network with 2 layers, &lt;code&gt;layer[2]&lt;/code&gt; does not exist.&lt;/p&gt;

&lt;p&gt;We also need to get the deltas of the intermediate hidden layers. To do this, (according to our equations above) we have to &amp;lsquo;pull back&amp;rsquo; the delta from the output layer first. More accurately, for any hidden layer, we pull back the delta from the &lt;em&gt;next&lt;/em&gt; layer, which may well be another hidden layer. These deltas from the &lt;em&gt;next&lt;/em&gt; layer are multiplied by the weights from the &lt;em&gt;next&lt;/em&gt; layer &lt;code&gt;[index + 1]&lt;/code&gt;, before getting the product with the sigmoid derivative evaluated at the &lt;em&gt;current&lt;/em&gt; layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: this is &lt;em&gt;back&lt;/em&gt; propagation. We have to start at the end and work back to the beginning. We use the &lt;code&gt;reversed&lt;/code&gt; keyword in our loop to ensure that the algorithm considers the layers in reverse order.&lt;/p&gt;

&lt;p&gt;Combining this into one method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Calculate the deltas
for index in reversed(range(self.numLayers)):
    if index == self.numLayers - 1:
        # If the output layer, then compare to the target values
        output_delta = self._layerOutput[index] - target.T
        error = np.sum(output_delta**2)
        delta.append(output_delta * sigmoid(self._layerInput[index], True))
    else:
        # If a hidden layer. compare to the following layer&#39;s delta
        delta_pullback = self.weights[index + 1].T.dot(delta[-1])
        delta.append(delta_pullback[:-1,:] * sigmoid(self._layerInput[index], True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pick this piece of code apart. This is an important snippet as it calculates all of the deltas for all of the nodes in the network. Be sure that we understand:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;This is a &lt;code&gt;reversed&lt;/code&gt; loop because we want to deal with the last layer first&lt;/li&gt;
&lt;li&gt;The delta of the output layer is the residual between the output and target multiplied with the gradient (derivative) of the activation function &lt;em&gt;at the current layer&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The delta of a hidden layer first needs the product of the &lt;em&gt;subsequent&lt;/em&gt; layer&amp;rsquo;s delta with the &lt;em&gt;subsequent&lt;/em&gt; layer&amp;rsquo;s weights. This is then multiplied with the gradient of the activation function evaluated at the &lt;em&gt;current&lt;/em&gt; layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Double check that this matches up with the equations above too! We can double check the matrix multiplication. For the output layer:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output_delta&lt;/code&gt; = (numOutputNodes x 1) - (1 x numOutputNodes).T = (numOutputNodes x 1)
&lt;code&gt;error&lt;/code&gt; = (numOutputNodes x 1) **2 = (numOutputNodes x 1)
&lt;code&gt;delta&lt;/code&gt; = (numOutputNodes x 1) * sigmoid( (numOutputNodes x 1) ) = (numOutputNodes  x 1)&lt;/p&gt;

&lt;p&gt;For the hidden layers (take the one previous to the output as example):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;delta_pullback&lt;/code&gt; = (numOutputNodes x numHiddenNodes).T.dot(numOutputNodes x 1) = (numHiddenNodes x 1)
&lt;code&gt;delta&lt;/code&gt; = (numHiddenNodes x 1) * sigmoid ( (numHuddenNodes x 1) ) = (numHiddenNodes x 1)&lt;/p&gt;

&lt;p&gt;Hurray! We have the delta at each node in our network. We can use them to update the weights for each layer in the network. Remember, to update the weights between layer $J$ and $K$ we need to use the output of layer $J$ and the deltas of layer $K$. This means we need to keep a track of the index of the layer we&amp;rsquo;re currently working on ($J$) and the index of the delta layer ($K$) - not forgetting about the zero-indexing in Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for index in range(self.numLayers):
    delta_index = self.numLayers - 1 - index
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s first get the outputs from each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    if index == 0:
        layerOutput = np.vstack([input.T, np.ones([1, numExamples])])
    else:
        layerOutput = np.vstack([self._layerOutput[index - 1], np.ones([1,self._layerOutput[index -1].shape[1]])])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the input layer is just the input examples (which we&amp;rsquo;ve &lt;code&gt;vstack&lt;/code&gt;-ed again and the output from the other layers we take from calculation in the forward pass (making sure to add the bias term on the end).&lt;/p&gt;

&lt;p&gt;For the current &lt;code&gt;index&lt;/code&gt; (layer) lets use this &lt;code&gt;layerOutput&lt;/code&gt; to get the change in weight. We will use a few neat tricks to make this succinct:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	thisWeightDelta = np.sum(\
	    layerOutput[None,:,:].transpose(2,0,1) * delta[delta_index][None,:,:].transpose(2,1,0) \
	    , axis = 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Break it down. We&amp;rsquo;re looking for $\mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}} $ so it&amp;rsquo;s the delta at &lt;code&gt;delta_index&lt;/code&gt;, the next layer along.&lt;/p&gt;

&lt;p&gt;We want to be able to deal with all of the input training examples simultaneously. This requires a bit of fancy slicing and transposing of the matrices. Take a look: by calling &lt;code&gt;vstack&lt;/code&gt; we made all of the input data and bias terms live in the same matrix of a numpy array. When we slice this arraywith the &lt;code&gt;[None,:,:]&lt;/code&gt; argument, it tells Python to take all (&lt;code&gt;:&lt;/code&gt;) the data in the rows and columns and shift it to the 1st and 2nd dimensions and leave the first dimension empty (&lt;code&gt;None&lt;/code&gt;). We do this to create the three dimensions which we can now transpose into. Calling &lt;code&gt;transpose(2,0,1)&lt;/code&gt; instructs Python to move around the dimensions of the data (e.g. its rows&amp;hellip; or examples). This creates an array where each example now lives in its own plane. The same is done for the deltas of the subsequent layer, but being careful to transpost them in the opposite direction so that the matrix multiplication can occur. The &lt;code&gt;axis= 0&lt;/code&gt; is supplied to make sure that the inputs are multiplied by the correct dimension of the delta matrix.&lt;/p&gt;

&lt;p&gt;This looks incredibly complicated. It an be broken down into a for-loop over the input examples, but this reduces the efficiency of the network. Taking advantage of the numpy array like this keeps our calculations fast. In reality, if you&amp;rsquo;re struggling with this particular part, just copy and paste it, forget about it and be happy with yourself for understanding the maths behind back propagation, even if this random bit of Python is perplexing.&lt;/p&gt;

&lt;p&gt;Anyway. Lets take this set of weight deltas and put back the $\eta$. We&amp;rsquo;ll call this the &lt;code&gt;learningRate&lt;/code&gt;. It&amp;rsquo;s called a lot of things, but this seems to be the most common. We&amp;rsquo;ll update the weights by making sure to include the &lt;code&gt;-&lt;/code&gt; from the $-\eta$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	weightDelta = trainingRate * thisWeightDelta
	self.weights[index] -= weightDelta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the &lt;code&gt;-=&lt;/code&gt; is Python slang for: take the current value and subtract the value of &lt;code&gt;weightDelta&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To finish up, we want our back propagation to return the current error in the network, so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;return error
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing&#34;&gt; A Toy Example&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Believe it or not, that&amp;rsquo;s it! The fundamentals of forward and back propagation have now been implemented in Python. If you want to double check your code, have a look at my completed .py &lt;a href=&#34;/docs/simpleNN.py&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test it!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Input = np.array([[0,0],[1,1],[0,1],[1,0]])
Target = np.array([[0.0],[0.0],[1.0],[1.0]])

NN = backPropNN((2,2,1))

Error = NN.backProp(Input, Target)
Output = NN.FP(Input)

print &#39;Input \tOutput \t\tTarget&#39;
for i in range(Input.shape[0]):
    print &#39;{0}\t {1} \t{2}&#39;.format(Input[i], Output[i], Target[i])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will provide 4 input examples and the expected targets. We create an instance of the network called &lt;code&gt;NN&lt;/code&gt; with 2 layers (2 nodes in the hidden and 1 node in the output layer). We make &lt;code&gt;NN&lt;/code&gt; do &lt;code&gt;backProp&lt;/code&gt; with the input and target data and then get the output from the final layer by running out input through the network with a &lt;code&gt;FP&lt;/code&gt;. The printout is self explantory. Give it a try!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input 	Output 		Target
[0 0]	 [ 0.51624448] 	[ 0.]
[1 1]	 [ 0.51688469] 	[ 0.]
[0 1]	 [ 0.51727559] 	[ 1.]
[1 0]	 [ 0.51585529] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that the network has taken our inputs, and we have some outputs too. They&amp;rsquo;re not great, and all seem to live around the same value. This is because we initialised the weights across the network to a similarly small random value. We need to repeat the &lt;code&gt;FP&lt;/code&gt; and &lt;code&gt;backProp&lt;/code&gt; process many times in order to keep updating the weights.&lt;/p&gt;

&lt;h2 id=&#34;iterating&#34;&gt; Iterating &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Iteration is very straight forward. We just tell our algorithm to repeat a maximum of &lt;code&gt;maxIterations&lt;/code&gt; times or until the &lt;code&gt;Error&lt;/code&gt; is below &lt;code&gt;minError&lt;/code&gt; (whichever comes first). As the weights are stored internally within &lt;code&gt;NN&lt;/code&gt; every time we call the &lt;code&gt;backProp&lt;/code&gt; method, it uses the latest, internally stored weights and doesn&amp;rsquo;t start again - the weights are only initialised once upon creation of &lt;code&gt;NN&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;maxIterations = 100000
minError = 1e-5

for i in range(maxIterations + 1):
    Error = NN.backProp(Input, Target)
    if i % 2500 == 0:
        print(&amp;quot;Iteration {0}\tError: {1:0.6f}&amp;quot;.format(i,Error))
    if Error &amp;lt;= minError:
        print(&amp;quot;Minimum error reached at iteration {0}&amp;quot;.format(i))
        break
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s the end of my output from the first run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iteration 100000	Error: 0.000291
Input 	Output 		Target
[0 0]	 [ 0.00780385] 	[ 0.]
[1 1]	 [ 0.00992829] 	[ 0.]
[0 1]	 [ 0.99189799] 	[ 1.]
[1 0]	 [ 0.99189943] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better! The error is very small and the outputs are very close to the correct value. However, they&amp;rsquo;re note completely right. We can do better, by implementing different activation functions which we will do in the next tutorial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please&lt;/strong&gt; let me know if anything is unclear, or there are mistakes. Let me know how you get on!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Simple Neural Network - Vectorisation</title>
      <link>/post/nn-more-maths/</link>
      <pubDate>Mon, 13 Mar 2017 10:33:08 +0000</pubDate>
      
      <guid>/post/nn-more-maths/</guid>
      <description>&lt;p&gt;The third in our series of tutorials on Simple Neural Networks. This time, we&amp;rsquo;re looking a bit deeper into the maths, specifically focusing on vectorisation. This is an important step before we can translate our maths in a functioning script in Python.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ve &lt;a href=&#34;/post/neuralnetwork&#34;&gt;been through the maths&lt;/a&gt; of a neural network (NN) using back propagation and taken a look at the &lt;a href=&#34;/post/transfer-functions&#34;&gt;different activation functions&lt;/a&gt; that we could implement. This post will translate the mathematics into Python which we can piece together at the end into a functioning NN!&lt;/p&gt;

&lt;h2 id=&#34;forwardprop&#34;&gt; Forward Propagation &lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s remimnd ourselves of our notation from our 2 layer network in the &lt;a href=&#34;/post/neuralnetwork&#34;&gt;maths tutorial&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I is our input layer&lt;/li&gt;
&lt;li&gt;J is our hidden layer&lt;/li&gt;
&lt;li&gt;$w_{ij}$ is the weight connecting the $i^{\text{th}}$ node in in $I$ to the $j^{\text{th}}$ node in $J$&lt;/li&gt;
&lt;li&gt;$x_{j}$ is the total input to the $j^{\text{th}}$ node in $J$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, assuming that we have three features (nodes) in the input layer, the input to the first node in the hidden layer is given by:&lt;/p&gt;

&lt;div&gt;$$
x_{1} = \mathcal{O}_{1}^{I} w_{11} + \mathcal{O}_{2}^{I} w_{21} + \mathcal{O}_{3}^{I} w_{31}
$$&lt;/div&gt;

&lt;p&gt;Lets generalise this for any connected nodes in any layer: the input to node $j$ in layer $l$ is:&lt;/p&gt;

&lt;div&gt;$$
x_{j} = \mathcal{O}_{1}^{l-1} w_{1j} + \mathcal{O}_{2}^{l-1} w_{2j} + \mathcal{O}_{3}^{l-1} w_{3j}
$$&lt;/div&gt;

&lt;p&gt;But we need to be careful and remember to put in our &lt;em&gt;bias&lt;/em&gt; term $\theta$. In our maths tutorial, we said that the bias term was always equal to 1; now we can try to understand why.&lt;/p&gt;

&lt;p&gt;We could just add the bias term onto the end of the previous equation to get:&lt;/p&gt;

&lt;div&gt;$$
x_{j} = \mathcal{O}_{1}^{l-1} w_{1j} + \mathcal{O}_{2}^{l-1} w_{2j} + \mathcal{O}_{3}^{l-1} w_{3j} + \theta_{i}
$$&lt;/div&gt;

&lt;p&gt;If we think more carefully about this, what we are really saying is that &amp;ldquo;an extra node in the previous layer, which always outputs the value 1, is connected to the node $j$ in the current layer by some weight $w_{4j}$&amp;ldquo;. i.e. $1 \cdot w_{4j}$:&lt;/p&gt;

&lt;div&gt;$$
x_{j} = \mathcal{O}_{1}^{l-1} w_{1j} + \mathcal{O}_{2}^{l-1} w_{2j} + \mathcal{O}_{3}^{l-1} w_{3j} + 1 \cdot w_{4j}
$$&lt;/div&gt;

&lt;p&gt;By the magic of matrix multiplication, we should be able to convince ourselves that:&lt;/p&gt;

&lt;div&gt;$$
x_{j} = \begin{pmatrix} w_{1j} &amp;w_{2j} &amp;w_{3j} &amp;w_{4j} \end{pmatrix}
     \begin{pmatrix}    \mathcal{O}_{1}^{l-1} \\
                    \mathcal{O}_{2}^{l-1} \\
                    \mathcal{O}_{3}^{l-1} \\
                    1
        \end{pmatrix}

$$&lt;/div&gt;

&lt;p&gt;Now, lets be a little more explicit, consider the input $x$ to the first two nodes of the layer $J$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{1} &amp;= \begin{pmatrix} w_{11} &amp;w_{21} &amp;w_{31} &amp;w_{41} \end{pmatrix}
     \begin{pmatrix}    \mathcal{O}_{1}^{l-1} \\
                    \mathcal{O}_{2}^{l-1} \\
                    \mathcal{O}_{3}^{l-1} \\
                    1
        \end{pmatrix}
\\[0.5em]
x_{2} &amp;= \begin{pmatrix} w_{12} &amp;w_{22} &amp;w_{32} &amp;w_{42} \end{pmatrix}
     \begin{pmatrix}    \mathcal{O}_{1}^{l-1} \\
                    \mathcal{O}_{2}^{l-1} \\
                    \mathcal{O}_{3}^{l-1} \\
                    1
        \end{pmatrix}
\end{align}
$$&lt;/div&gt;

&lt;p&gt;Note that the second matrix is constant between the input calculations as it is only the output values of the previous layer (including the bias term). This means (again by the magic of matrix multiplication) that we can construct a single vector containing the input values $x$ to the current layer:&lt;/p&gt;

&lt;div&gt; $$
\begin{pmatrix} x_{1} \\ x_{2} \end{pmatrix}
= \begin{pmatrix}   w_{11} &amp; w_{21} &amp; w_{31} &amp; w_{41} \\
                    w_{12} &amp; w_{22} &amp; w_{32} &amp; w_{42} 
                    \end{pmatrix}
     \begin{pmatrix}    \mathcal{O}_{1}^{l-1} \\
                    \mathcal{O}_{2}^{l-1} \\
                    \mathcal{O}_{3}^{l-1} \\
                    1
        \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;This is an $\left(n \times m+1 \right)$ matrix multiplied with an $\left(m +1 \times  1 \right)$ where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$n$ is the number of nodes in the current layer $l$&lt;/li&gt;
&lt;li&gt;$m$ is the number of nodes in the previous layer $l-1$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lets generalise - the vector of inputs to the $n$ nodes in the current layer from the nodes $m$ in the previous layer is:&lt;/p&gt;

&lt;div&gt; $$
\begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{pmatrix}
= \begin{pmatrix}   w_{11} &amp; w_{21} &amp; \cdots &amp; w_{(m+1)1} \\
                    w_{12} &amp; w_{22} &amp; \cdots &amp; w_{(m+1)2} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    w_{1n} &amp; w_{2n} &amp; \cdots &amp; w_{(m+1)n} \\
                    \end{pmatrix}
     \begin{pmatrix}    \mathcal{O}_{1}^{l-1} \\
                    \mathcal{O}_{2}^{l-1} \\
                    \mathcal{O}_{3}^{l-1} \\
                    1
        \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;or:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{x_{J}} = \mathbf{W_{IJ}} \mathbf{\vec{\mathcal{O}}_{I}}
$$&lt;/div&gt;

&lt;p&gt;In this notation, the output from the current layer $J$ is easily written as:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{\vec{\mathcal{O}}_{J}} = \sigma \left( \mathbf{W_{IJ}} \mathbf{\vec{\mathcal{O}}_{I}} \right)
$$&lt;/div&gt;

&lt;p&gt;Where $\sigma$ is the activation or transfer function chosen for this layer which is applied elementwise to the product of the matrices.&lt;/p&gt;

&lt;p&gt;This notation allows us to very efficiently calculate the output of a layer which reduces computation time. Additionally, we are now able to extend this efficiency by making out network consider &lt;strong&gt;all&lt;/strong&gt; of our input examples at once.&lt;/p&gt;

&lt;p&gt;Remember that our network requires training (many epochs of forward propagation followed by back propagation) and as such needs training data (preferably a lot of it!). Rather than consider each training example individually, we vectorise each example into a large matrix of inputs.&lt;/p&gt;

&lt;p&gt;Our weights $\mathbf{W_{IJ}}$ connecting the layer $l$ to layer $J$ are the same no matter which input example we put into the network: this is fundamental as we expect that the network would act the same way for similar inputs i.e. we expect the same neurons (nodes) to fire based on the similar features in the input.&lt;/p&gt;

&lt;p&gt;If 2 input examples gave the outputs $ \mathbf{\vec{\mathcal{O}}_{I_{1}}} $ and $ \mathbf{\vec{\mathcal{O}}_{I_{2}}} $  from the nodes in layer $I$ to a layer $J$ then the outputs from layer $J$ , $\mathbf{\vec{\mathcal{O}}_{J_{1}}}$ and $\mathbf{\vec{\mathcal{O}}_{J_{1}}}$ can be written:&lt;/p&gt;

&lt;div&gt;$$
\begin{pmatrix}
    \mathbf{\vec{\mathcal{O}}_{J_{1}}} \\
    \mathbf{\vec{\mathcal{O}}_{J_{2}}}
\end{pmatrix}
=
\sigma \left(\mathbf{W_{IJ}}\begin{pmatrix}
        \mathbf{\vec{\mathcal{O}}_{I_{1}}} &amp;
        \mathbf{\vec{\mathcal{O}}_{I_{2}}}  
    \end{pmatrix}
    \right)
=
\sigma \left(\mathbf{W_{IJ}}\begin{pmatrix}
        \begin{bmatrix}\mathcal{O}_{I_{1}}^{1} \\ \vdots \\ \mathcal{O}_{I_{1}}^{m}
        \end{bmatrix}
        \begin{bmatrix}\mathcal{O}_{I_{2}}^{1} \\ \vdots \\ \mathcal{O}_{I_{2}}^{m}
        \end{bmatrix}   
    \end{pmatrix}
        \right)
=   \sigma \left(\begin{pmatrix} \mathbf{W_{IJ}}\begin{bmatrix}\mathcal{O}_{I_{1}}^{1} \\ \vdots \\ \mathcal{O}_{I_{1}}^{m}
        \end{bmatrix} &amp; 
    \mathbf{W_{IJ}}     \begin{bmatrix}\mathcal{O}_{I_{2}}^{1} \\ \vdots \\ \mathcal{O}_{I_{2}}^{m}
        \end{bmatrix}
    \end{pmatrix}
        \right)

$$&lt;/div&gt;

&lt;p&gt;For the $m$ nodes in the input layer. Which may look hideous, but the point is that all of the training examples that are input to the network can be dealt with simultaneously because each example becomes another column in the input vector and a corresponding column in the output vector.&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

In summary, for forward propagation:

&lt;uo&gt;
&lt;li&gt; All $n$ training examples with $m$ features (input nodes) are put into column vectors to build the input matrix $I$, taking care to add the bias term to the end of each.&lt;/li&gt;

&lt;li&gt; All weight vectors that connect $m +1$ nodes in the layer $I$ to the $n$ nodes in layer $J$ are put together in a weight-matrix&lt;/li&gt;

&lt;div&gt;$$
\mathbf{I} =    \left(
    \begin{bmatrix}
        \mathcal{O}_{I_{1}}^{1} \\ \vdots \\ \mathcal{O}_{I_{1}}^{m} \\ 1 \end{bmatrix}
    \begin{bmatrix}
        \mathcal{O}_{I_{2}}^{1} \\ \vdots \\ \mathcal{O}_{I_{2}}^{m} \\ 1
    \end{bmatrix}
        \begin{bmatrix}
    \cdots \\ \cdots \\ \ddots \\ \cdots
        \end{bmatrix}
    \begin{bmatrix}
        \mathcal{O}_{I_{n}}^{1} \\ \vdots \\ \mathcal{O}_{I_{n}}^{m} \\ 1

    \end{bmatrix}
    \right)

\ \ \ \ 


\mathbf{W_{IJ}} = 
\begin{pmatrix}     w_{11} &amp; w_{21} &amp; \cdots &amp; w_{(m+1)1} \\
                    w_{12} &amp; w_{22} &amp; \cdots &amp; w_{(m+1)2} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    w_{1n} &amp; w_{2n} &amp; \cdots &amp; w_{(m+1)n} \\
                    \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;&lt;li&gt; We perform $ \mathbf{W_{IJ}} \mathbf{I}$ to get the vector $\mathbf{\vec{\mathcal{O}}_{J}}$ which is the output from each of the $m$ nodes in layer $J$ &lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;backprop&#34;&gt; Back Propagation &lt;/h2&gt;

&lt;p&gt;To perform back propagation there are a couple of things that we need to vectorise. The first is the error on the weights when we compare the output of the network $\mathbf{\vec{\mathcal{O}}_{K}}$ with the known target values:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{T_{K}} = \begin{bmatrix} t_{1} \\ \vdots \\ t_{k} \end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;A reminder of the formulae:&lt;/p&gt;

&lt;div&gt;$$

    \delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right), 
    \ \ \ \
    \delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}

$$&lt;/div&gt;
    

&lt;p&gt;Where $\delta_{k}$ is the error on the weights to the output layer and $\delta_{j}$ is the error on the weights to the hidden layers. We also need to vectorise the update formulae for the weights and bias:&lt;/p&gt;

&lt;div&gt;$$
    W + \Delta W \rightarrow W, \ \ \ \
    \theta + \Delta\theta \rightarrow \theta
$$&lt;/div&gt;

&lt;h3 id=&#34;outputdeltas&#34;&gt;  Vectorising the Output Layer Deltas &lt;/h3&gt;

&lt;p&gt;Lets look at the output layer delta: we need a subtraction between the outputs and the target which is multiplied by the derivative of the transfer function (sigmoid). Well, the subtraction between two matrices is straight forward:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}
$$&lt;/div&gt;

&lt;p&gt;but we need to consider the derivative. Remember that the output of the final layer is:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{\vec{\mathcal{O}}_{K}}  = \sigma \left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}}  \right)
$$&lt;/div&gt;

&lt;p&gt;and the derivative can be written:&lt;/p&gt;

&lt;div&gt;$$
 \sigma ^{\prime} \left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}}  \right) =   \mathbf{\vec{\mathcal{O}}_{K}}\left( 1 - \mathbf{\vec{\mathcal{O}}_{K}}  \right) 
$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is the derivative of the sigmoid as evaluated at each of the nodes in the layer $K$. It is acting &lt;em&gt;elementwise&lt;/em&gt; on the inputs to layer $K$. Thus it is a column vector with the same length as the number of nodes in layer $K$.&lt;/p&gt;

&lt;p&gt;Put the derivative and subtraction terms together and we get:&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;$$
\mathbf{\vec{\delta}_{K}} = \sigma^{\prime}\left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}} \right) * \left( \mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}\right)
$$&lt;/div&gt;

&lt;p&gt;Again, the derivatives are being multiplied elementwise with the results of the subtration. Now we have a vector of deltas for the output layer $K$! Things aren&amp;rsquo;t so straight forward for the detlas in the hidden layers.&lt;/p&gt;

&lt;p&gt;Lets visualise what we&amp;rsquo;ve seen:&lt;/p&gt;

&lt;div  id=&#34;fig1&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img img title=&#34;NN Vectorisation&#34; src=&#34;/img/simpleNN/nn_vectors1.png&#34; width=&#34;30%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: NN showing the weights and outputs in vector form along with the target values for layer $K$
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;hiddendeltas&#34;&gt; Vectorising the Hidden Layer Deltas &lt;/h3&gt;

&lt;p&gt;We need to vectorise:&lt;/p&gt;

&lt;div&gt;$$
    \delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s deal with the summation. We&amp;rsquo;re multipying each of the deltas $\delta_{k}$ in the output layer (or more generally, the subsequent layer could be another hidden layer) by the weight $w_{jk}$ that pulls them back to the node $j$ in the current layer before adding the results. For the first node in the hidden layer:&lt;/p&gt;

&lt;div&gt;$$
\sum_{k \in K} \delta_{k} W_{jk} = \delta_{k}^{1}w_{11} + \delta_{k}^{2}w_{12} + \delta_{k}^{3}w_{13}

= \begin{pmatrix} w_{11} &amp; w_{12} &amp; w_{13} \end{pmatrix}  \begin{pmatrix} \delta_{k}^{1} \\ \delta_{k}^{2} \\ \delta_{k}^{3}\end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;Notice the weights? They pull the delta from each output layer node back to the first node of the hidden layer. In forward propagation, these we consider multiple nodes going out to a single node, rather than this way of receiving multiple nodes at a single node.&lt;/p&gt;

&lt;p&gt;Combine this summation with the multiplication by the activation function derivative:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j}^{1} = \sigma^{\prime} \left(  x_{j}^{1} \right)
\begin{pmatrix} w_{11} &amp; w_{12} &amp; w_{13} \end{pmatrix}  \begin{pmatrix} \delta_{k}^{1} \\ \delta_{k}^{2} \\ \delta_{k}^{3} \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;remembering that the input to the $\text{1}^\text{st}$ node in the layer $J$&lt;/p&gt;

&lt;div&gt;$$
x_{j}^{1} = \mathbf{W_{I1}}\mathbf{\vec{\mathcal{O}}_{I}}
$$&lt;/div&gt;

&lt;p&gt;What about the $\text{2}^\text{nd}$ node in the hidden layer?&lt;/p&gt;

&lt;div&gt;$$
\delta_{j}^{2} = \sigma^{\prime} \left(  x_{j}^{2} \right)
\begin{pmatrix} w_{21} &amp; w_{22} &amp; w_{23} \end{pmatrix}  \begin{pmatrix}  \delta_{k}^{1} \\ \delta_{k}^{2} \\ \delta_{k}^{3} \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;This is looking familiar, hopefully we can be confident based upon what we&amp;rsquo;ve done before to say that:&lt;/p&gt;

&lt;div&gt;$$
\begin{pmatrix}
    \delta_{j}^{1} \\ \delta_{j}^{2}
\end{pmatrix}
 = 
 \begin{pmatrix}
     \sigma^{\prime} \left(  x_{j}^{1} \right) \\ \sigma^{\prime} \left(  x_{j}^{2} \right)
 \end{pmatrix}
 *
  \begin{pmatrix}
    w_{11} &amp; w_{12} &amp; w_{13} \\
    w_{21} &amp; w_{22} &amp; w_{23} 
 \end{pmatrix}
 
 \begin{pmatrix}\delta_{k}^{1} \\ \delta_{k}^{2} \\ \delta_{k}^{3}  \end{pmatrix}

$$&lt;/div&gt;

&lt;p&gt;We&amp;rsquo;ve seen a version of this weights matrix before when we did the forward propagation vectorisation. In this case though, look carefully - as we mentioned, the weights are not in the same places, in fact, the weight matrix has been &lt;em&gt;transposed&lt;/em&gt; from the one we used in forward propagation. This makes sense because we&amp;rsquo;re going backwards through the network now! This is useful because it means there is very little extra calculation needed here - the matrix we need is already available from the forward pass, but just needs transposing. We can call the weights in back propagation here $ \mathbf{ W_{KJ}} $ as we&amp;rsquo;re pulling the deltas from $K$ to $J$.&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
    \mathbf{W_{KJ}} &amp;=
    \begin{pmatrix}
    w_{11} &amp; w_{12} &amp; \cdots &amp; w_{1n} \\
    w_{21} &amp; w_{22} &amp; \cdots &amp; w_{23}  \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    w_{(m+1)1} &amp; w_{(m+1)2} &amp; \cdots &amp; w_{(m+1)n}
    \end{pmatrix} , \ \ \
    
    \mathbf{W_{JK}} = 
    \begin{pmatrix}     w_{11} &amp; w_{21} &amp; \cdots &amp; w_{(m+1)1} \\
                    w_{12} &amp; w_{22} &amp; \cdots &amp; w_{(m+1)2} \\
                    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    w_{1n} &amp; w_{2n} &amp; \cdots &amp; w_{(m+1)n} \\
                    \end{pmatrix} \\[0.5em]
                        
\mathbf{W_{KJ}} &amp;= \mathbf{W^{\intercal}_{JK}}
\end{align}
$$&lt;/div&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

And so, the vectorised equations for the output layer and hidden layer deltas are:

&lt;div&gt;$$
\begin{align}

\mathbf{\vec{\delta}_{K}} &amp;= \sigma^{\prime}\left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}} \right) * \left( \mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}\right) \\[0.5em]

\mathbf{ \vec{ \delta }_{J}} &amp;= \sigma^{\prime} \left( \mathbf{ W_{IJ} \mathcal{O}_{I} } \right) * \mathbf{ W^{\intercal}_{JK}} \mathbf{ \vec{\delta}_{K}} 
\end{align}

$$&lt;/div&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Lets visualise what we&amp;rsquo;ve seen:&lt;/p&gt;

&lt;div  id=&#34;fig2&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img img title=&#34;NN Vectorisation 2&#34; src=&#34;/img/simpleNN/nn_vectors2.png&#34; width=&#34;20%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The NN showing the delta vectors
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;updates&#34;&gt; Vectorising the Update Equations &lt;/h3&gt;

&lt;p&gt;Finally, now that we have the vectorised equations for the deltas (which required us to get the vectorised equations for the forward pass) we&amp;rsquo;re ready to get the update equations in vector form. Let&amp;rsquo;s recall the update equations&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
    \Delta W &amp;= -\eta \ \delta_{l} \ \mathcal{O}_{l-1} \\
    \Delta\theta &amp;= -\eta \ \delta_{l}
\end{align}
$$&lt;/div&gt;

&lt;p&gt;Ignoring the $-\eta$ for now, we need to get a vector form for $\delta_{l} \ \mathcal{O}_{l-1}$ in order to get the update to the weights. We have the matrix of weights:&lt;/p&gt;

&lt;div&gt;$$
    
\mathbf{W_{JK}} = 
\begin{pmatrix}     w_{11} &amp; w_{21}  &amp; w_{31} \\
                w_{12} &amp; w_{22}  &amp; w_{32} \\

                \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;Suppose we are updating the weight $w_{21}$ in the matrix. We&amp;rsquo;re looking to find the product of the output from the second node in $J$ with the delta from the first node in $K$.&lt;/p&gt;

&lt;div&gt;$$
    \Delta w_{21} = \delta_{K}^{1} \mathcal{O}_{J}^{2} 
$$&lt;/div&gt;

&lt;p&gt;Considering this example, we can write the matrix for the weight updates as:&lt;/p&gt;

&lt;div&gt;$$
    
\Delta \mathbf{W_{JK}} = 
\begin{pmatrix}     \delta_{K}^{1} \mathcal{O}_{J}^{1} &amp; \delta_{K}^{1}  \mathcal{O}_{J}^{2}  &amp; \delta_{K}^{1} \mathcal{O}_{J}^{3}  \\
                \delta_{K}^{2} \mathcal{O}_{J}^{1} &amp; \delta_{K}^{2} \mathcal{O}_{J}^{2}  &amp; \delta_{K}^{2} \mathcal{O}_{J}^{3} 

                \end{pmatrix}
 = 

\begin{pmatrix}  \delta_{K}^{1} \\ \delta_{K}^{2}\end{pmatrix}

\begin{pmatrix}     \mathcal{O}_{J}^{1} &amp; \mathcal{O}_{J}^{2}&amp; \mathcal{O}_{J}^{3}

\end{pmatrix}

$$&lt;/div&gt;

&lt;p&gt;Generalising this into vector notation and including the &lt;em&gt;learning rate&lt;/em&gt; $\eta$, the update for the weights in layer $J$ is:&lt;/p&gt;

&lt;div&gt;$$
    
\Delta \mathbf{W_{JK}} = -\eta \mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}}

$$&lt;/div&gt;

&lt;p&gt;Similarly, we have the update to the bias term. If:&lt;/p&gt;

&lt;div&gt;$$
\Delta \vec{\theta} = -\eta \mathbf{ \vec{ \delta }_{K}} 
$$&lt;/div&gt;

&lt;p&gt;So the bias term is updated just by taking the deltas straight from the nodes in the subsequent layer (with the negative factor of learning rate).&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

In summary, for back propagation, the equations we need in vector form are:

&lt;div&gt;$$
\begin{align}

\mathbf{\vec{\delta}_{K}} &amp;= \sigma^{\prime}\left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}} \right) * \left( \mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}\right) \\[0.5em]

\mathbf{ \vec{ \delta }_{J}} &amp;= \sigma^{\prime} \left( \mathbf{ W_{IJ} \mathcal{O}_{I} } \right) * \mathbf{ W^{\intercal}_{JK}} \mathbf{ \vec{\delta}_{K}}

\end{align}
$$&lt;/div&gt;

&lt;div&gt;$$
\begin{align}

\mathbf{W_{JK}} + \Delta \mathbf{W_{JK}} &amp;\rightarrow \mathbf{W_{JK}}, \ \ \ \Delta \mathbf{W_{JK}} = -\eta \mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}} \\[0.5em]

\vec{\theta}  + \Delta \vec{\theta}  &amp;\rightarrow \vec{\theta}, \ \ \ \Delta \vec{\theta} = -\eta \mathbf{ \vec{ \delta }_{K}} 

\end{align}
$$&lt;/div&gt;

&lt;p&gt;With $*$ representing an elementwise multiplication between the matrices.&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;h2 id=&#34;nextsteps&#34;&gt; What&#39;s next? &lt;/h2&gt;

&lt;p&gt;Although this kinds of mathematics can be tedious and sometimes hard to follow (and probably with numerous notation mistakes&amp;hellip; please let me know if you find them!), it is necessary in order to write a quick, efficient NN. Our next step is to implement this setup in Python.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Simple Neural Network - Transfer Functions</title>
      <link>/post/transfer-functions/</link>
      <pubDate>Wed, 08 Mar 2017 10:43:07 +0000</pubDate>
      
      <guid>/post/transfer-functions/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re going to write a little bit of Python in this tutorial on Simple Neural Networks (Part 2). It will focus on the different types of activation (or transfer) functions, their properties and how to write each of them (and their derivatives) in Python.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;As promised in the previous post, we&amp;rsquo;ll take a look at some of the different activation functions that could be used in our nodes. Again &lt;strong&gt;please&lt;/strong&gt; let me know if there&amp;rsquo;s anything I&amp;rsquo;ve gotten totally wrong - I&amp;rsquo;m very much learning too.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#linear&#34;&gt;Linear Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sigmoid&#34;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tanh&#34;&gt;Hyperbolic Tangent Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gaussian&#34;&gt;Gaussian Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#step&#34;&gt;Heaviside (step) Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ramp&#34;&gt;Ramp Function&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#relu&#34;&gt;Rectified Linear Unit (ReLU)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;linear&#34;&gt; Linear (Identity) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig1&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/linear.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dlinear.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: The linear function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
f \left( x_{i} \right) = x_{i}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def linear(x, Derivative=False):
    if not Derivative:
        return x
    else:
        return 1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;If there&amp;rsquo;s a situation where we want a node to give its output without applying any thresholds, then the identity (or linear) function is the way to go.&lt;/p&gt;

&lt;p&gt;Hopefully you can see why it is used in the final output layer nodes as we only want these nodes to do the $ \text{input} \times \text{weight}$ operations before giving us its answer without any further modifications.&lt;/p&gt;

&lt;p&gt;&lt;font color=&#34;blue&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The linear function is not used in the hidden layers. We must use non-linear transfer functions in the hidden layer nodes or else the output will only ever end up being a linearly separable solution.&lt;/p&gt;

&lt;p&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;sigmoid&#34;&gt; The Sigmoid (or Fermi) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-1&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig2&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/sigmoid.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dsigmoid.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The sigmoid function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-1&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \frac{1}{1 + e^{  - x_{i}  }}, \ \
f^{\prime}\left( x_{i} \right) = \sigma(x_{i}) \left( 1 -  \sigma(x_{i}) \right)
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-1&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(x,Derivative=False):
    if not Derivative:
        return 1 / (1 + np.exp (-x))
    else:
        out = sigmoid(x)
        return out * (1 - out)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-1&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;This function maps the input to a value between 0 and 1 (but not equal to 0 or 1). This means the output from the node will be a high signal (if the input is positive) or a low one (if the input is negative). This function is often chosen as it is one of the easiest to hard-code in terms of its derivative. The simplicity of its derivative allows us to efficiently perform back propagation without using any fancy packages or approximations. The fact that this function is smooth, continuous (differentiable), monotonic and bounded means that back propagation will work well.&lt;/p&gt;

&lt;p&gt;The sigmoid&amp;rsquo;s natural threshold is 0.5, meaning that any input that maps to a value above 0.5 will be considered high (or 1) in binary terms.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;tanh&#34;&gt; Hyperbolic Tangent Function ( $\tanh(x)$ ) &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-2&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig3&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/tanh.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dtanh.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The hyperbolic tangent function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-2&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \tanh\left(x_{i}\right),
f^{\prime}\left(x_{i} \right) = 1 - \tanh\left(x_{i}\right)^{2}
$$&lt;/div&gt;

&lt;h3 id=&#34;why-is-it-used-2&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;This is a very similar function to the previous sigmoid function and has much of the same properties: even its derivative is straight forward to compute. However, this function allows us to map the input to any value between -1 and 1 (but not inclusive of those). In effect, this allows us to apply a plenalty to the node (negative) rather than just have the node not fire at all. It also gives us a larger range of output to play with in the positive end of the scale meaning finer adjustments can be made.&lt;/p&gt;

&lt;p&gt;This function has a natural threshold of 0, meaning that any input which maps to a value greater than 0 is considered high (or 1) in binary terms.&lt;/p&gt;

&lt;p&gt;Again, the fact that this function is smooth, continuous (differentiable), monotonic and bounded means that back propagation will work well. The subsequent functions don&amp;rsquo;t all have these properties which makes them more difficult to use in back propagation (though it is done).
&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;what-s-the-difference-between-the-sigmoid-and-hyperbolic-tangent&#34;&gt;What&amp;rsquo;s the difference between the sigmoid and hyperbolic tangent?&lt;/h2&gt;

&lt;p&gt;They both achieve a similar mapping, are both continuous, smooth, monotonic and differentiable, but give out different values. For a sigmoid function, a large negative input generates an almost zero output. This lack of output will affect all subsequent weights in the network which may not be desirable - effectively stopping the next nodes from learning. In contrast, the $\tanh$ function supplies -1 for negative values, maintaining the output of the node and allowing subsequent nodes to learn from it.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;gaussian&#34;&gt; Gaussian Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-3&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig4&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/gaussian.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dgaussian.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 4&lt;/font&gt;: The gaussian function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-3&#34;&gt;Formulae&lt;/h3&gt;

&lt;div &gt;$$
f\left( x_{i}\right ) = e^{ -x_{i}^{2}}, \ \
f^{\prime}\left( x_{i}\right ) = - 2x e^{ - x_{i}^{2}}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-2&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gaussian(x, Derivative=False):
    if not Derivative:
        return np.exp(-x**2)
    else:
        return -2 * x * np.exp(-x**2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-3&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;The gaussian function is an even function, thus is gives the same output for equally positive and negative values of input. It gives its maximal output when there is no input and has decreasing output with increasing distance from zero. We can perhaps imagine this function is used in a node where the input feature is less likely to contribute to the final result.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;step&#34;&gt; Step (or Heaviside) Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-4&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig5&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/step.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 5&lt;/font&gt;: The Heaviside function (left) and its derivative (right)
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-4&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
    f(x)= 
\begin{cases}
\begin{align}
    0  \ &amp;: \ x_{i} \leq T\\
    1 \ &amp;: \ x_{i} &gt; T\\
    \end{align}
\end{cases}
$$&lt;/div&gt;

&lt;h3 id=&#34;why-is-it-used-4&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;Some cases call for a function which applies a hard thresold: either the output is precisely a single value, or not. The other functions we&amp;rsquo;ve looked at have an intrinsic probablistic output to them i.e. a higher output in decimal format implying a greater probability of being 1 (or a high output). The step function does away with this opting for a definite high or low output depending on some threshold on the input $T$.&lt;/p&gt;

&lt;p&gt;However, the step-function is discontinuous and therefore non-differentiable (its derivative is the Dirac-delta function). Therefore use of this function in practice is not done with back-propagation.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;ramp&#34;&gt; Ramp Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-does-it-look-like-5&#34;&gt;What does it look like?&lt;/h3&gt;

&lt;div  id=&#34;fig6&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/ramp.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/dramp.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 6&lt;/font&gt;: The ramp function (left) and its derivative (right) with $T1=-2$ and $T2=3$.
        &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;formulae-5&#34;&gt;Formulae&lt;/h3&gt;

&lt;div&gt;$$
    f(x)= 
\begin{cases}
\begin{align}
    0 \ &amp;: \ x_{i} \leq T_{1}\\[0.5em]
    \frac{\left( x_{i} - T_{1} \right)}{\left( T_{2} - T_{1} \right)} \ &amp;: \ T_{1} \leq x_{i} \leq T_{2}\\[0.5em]
    1 \ &amp;: \ x_{i} &gt; T_{2}\\
    \end{align}
\end{cases}
$$&lt;/div&gt;

&lt;h3 id=&#34;python-code-3&#34;&gt;Python Code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ramp(x, Derivative=False, T1=0, T2=np.max(x)):
    out = np.ones(x.shape)
    ids = ((x &amp;lt; T1) | (x &amp;gt; T2))
    if not Derivative:
        out = ((x - T1)/(T2-T1))
        out[(x &amp;lt; T1)] = 0
        out[(x &amp;gt; T2)] = 1
        return out
    else:
        out[ids]=0
        return out
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-is-it-used-5&#34;&gt;Why is it used?&lt;/h3&gt;

&lt;p&gt;The ramp function is a truncated version of the linear function. From its shape, the ramp function looks like a more definitive version of the sigmoid function in that its maps a range of inputs to outputs over the range (0 1) but this time with definitive cut off points $T1$ and $T2$. This gives the function the ability to fire the node very definitively above a threshold, but still have some uncertainty in the lower regions. It may not be common to see $T1$ in the negative region unless the ramp is equally distributed about $0$.&lt;/p&gt;

&lt;h3 id=&#34;relu&#34;&gt; 6.1 Rectified Linear Unit (ReLU) &lt;/h3&gt;

&lt;p&gt;There is a popular, special case of the ramp function in use in the powerful &lt;em&gt;convolutional neural network&lt;/em&gt; (CNN) architecture called a &lt;em&gt;&lt;strong&gt;Re&lt;/strong&gt;ctifying &lt;strong&gt;L&lt;/strong&gt;inear &lt;strong&gt;U&lt;/strong&gt;nit&lt;/em&gt; (ReLU). In a ReLU, $T1=0$ and $T2$ is the maximum of the input giving a linear function with no negative values as below:&lt;/p&gt;

&lt;div  id=&#34;fig7&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/relu.png&#34; width=&#34;40%&#34;&gt;&lt;img title=&#34;Simple NN&#34; src=&#34;/img/transferFunctions/drelu.png&#34; width=&#34;40%&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 7&lt;/font&gt;: The Rectified Linear Unit (ReLU) (left) with its derivative (right).
        &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;and in Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def relu(x, Derivative=False):
    if not Derivative:
        return np.maximum(0,x)
    else:
        out = np.ones(x.shape)
        out[(x &amp;lt; 0)]=0
        return out
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>A Simple Neural Network - Mathematics</title>
      <link>/post/neuralnetwork/</link>
      <pubDate>Mon, 06 Mar 2017 17:04:53 +0000</pubDate>
      
      <guid>/post/neuralnetwork/</guid>
      <description>&lt;p&gt;This is the first part of a series of tutorials on Simple Neural Networks (NN). Tutorials on neural networks (NN) can be found all over the internet. Though many of them are the same, each is written (or recorded) slightly differently. This means that I always feel like I learn something new or get a better understanding of things with every tutorial I see. I&amp;rsquo;d like to make this tutorial as clear as I can, so sometimes the maths may be simplistic, but hopefully it&amp;rsquo;ll give you a good unserstanding of what&amp;rsquo;s going on.  &lt;strong&gt;Please&lt;/strong&gt; let me know if any of the notation is incorrect or there are any mistakes - either comment or use the contact page on the left.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#nnarchitecture&#34;&gt;Neural Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferFunction&#34;&gt;Transfer Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedforward&#34;&gt;Feed-forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#error&#34;&gt;Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationGrads&#34;&gt;Back Propagation - the Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationAlgorithm&#34;&gt;Back Propagaton - the Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;nnarchitecture&#34;&gt;1. Neural Network Architecture &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By now, you may well have come across diagrams which look very similar to the one below. It shows some input node, connected to some output node via an intermediate node in what is called a &amp;lsquo;hidden layer&amp;rsquo; - &amp;lsquo;hidden&amp;rsquo; because in the use of NN only the input and output is of concern to the user, the &amp;lsquo;under-the-hood&amp;rsquo; stuff may not be interesting to them. In real, high-performing NN there are usually more hidden layers.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; width=40% src=&#34;/img/simpleNN/simpleNN.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: A simple 2-layer NN with 2 features in the input layer, 3 nodes in the hidden layer and two nodes in the output layer.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;When we train our network, the nodes in the hidden layer each perform a calculation using the values from the input nodes. The output of this is passed on to the nodes of the next layer. When the output hits the final layer, the &amp;lsquo;output layer&amp;rsquo;, the results are compared to the real, known outputs and some tweaking of the network is done to make the output more similar to the real results. This is done with an algorithm called &lt;em&gt;back propagation&lt;/em&gt;. Before we get there, lets take a closer look at these calculations being done by the nodes.&lt;/p&gt;

&lt;h2 id=&#34;transferFunction&#34;&gt;2. Transfer Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At each node in the hidden and output layers of the NN, an &lt;em&gt;activation&lt;/em&gt; or &lt;em&gt;transfer&lt;/em&gt; function is executed. This function takes in the output of the previous node, and multiplies it by some &lt;em&gt;weight&lt;/em&gt;. These weights are the lines which connect the nodes. The weights that come out of one node can all be different, that is they will &lt;em&gt;activate&lt;/em&gt; different neurons. There can be many forms of the transfer function, we will first look at the &lt;em&gt;sigmoid&lt;/em&gt; transfer function as it seems traditional.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;The sigmoid function&#34; width=50% src=&#34;/img/simpleNN/sigmoid.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The sigmoid function.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As you can see from the figure, the sigmoid function takes any real-valued input and maps it to a real number in the range $(0 \ 1)$ - i.e. between, but not equal to, 0 and 1. We can think of this almost like saying &amp;lsquo;if the value we have maps to an output near 1, this node fires, if it maps to an output near 0, the node does not fire&amp;rsquo;. The equation for this sigmoid function is:&lt;/p&gt;

&lt;div id=&#34;eqsigmoidFunction&#34;&gt;$$
\sigma ( x ) = \frac{1}{1 + e^{-x}}
$$&lt;/div&gt;

&lt;p&gt;We need to have the derivative of this transfer function so that we can perform back propagation later on. This is the process where by the connections in the network are updated to tune the performance of the NN. We&amp;rsquo;ll talk about this in more detail later, but let&amp;rsquo;s find the derivative now.&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\frac{d}{dx}\sigma ( x ) &amp;= \frac{d}{dx} \left( 1 + e^{ -x }\right)^{-1}\\
&amp;=  -1 \times -e^{-x} \times \left(1 + e^{-x}\right)^{-2}= \frac{ e^{-x} }{ \left(1 + e^{-x}\right)^{2} } \\
&amp;= \frac{\left(1 + e^{-x}\right) - 1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{\left(1 + e^{-x}\right) }{\left(1 + e^{-x}\right)^{2}} - \frac{1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{1}{\left(1 + e^{-x}\right)} - \left( \frac{1}{\left(1 + e^{-x}\right)} \right)^{2} \\[0.5em]
&amp;= \sigma ( x ) - \sigma ( x ) ^ {2}
\end{align*}
$$&lt;/div&gt;

&lt;p&gt;Therefore, we can write the derivative of the sigmoid function as:&lt;/p&gt;

&lt;div id=&#34;eqdsigmoid&#34;&gt;$$
\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)
$$&lt;/div&gt;

&lt;p&gt;The sigmoid function has the nice property that its derivative is very simple: a bonus when we want to hard-code this into our NN later on. Now that we have our activation or transfer function selected, what do we do with it?&lt;/p&gt;

&lt;h2 id=&#34;feedforward&#34;&gt;3. Feed-forward &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;During a feed-forward pass, the network takes in the input values and gives us some output values. To see how this is done, let&amp;rsquo;s first consider a 2-layer neural network like the one in Figure 1. Here we are going to refer to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$i$ - the $i^{\text{th}}$ node of the input layer $I$&lt;/li&gt;
&lt;li&gt;$j$ - the $j^{\text{th}}$ node of the hidden layer $J$&lt;/li&gt;
&lt;li&gt;$k$ - the $k^{\text{th}}$ node of the input layer $K$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The activation function at a node $j$ in the hidden layer takes the value:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{j} &amp;= \xi_{1} w_{1j} + \xi_{2} w_{2j} \\[0.5em]
&amp;= \sum_{i \in I} \xi_{i} w_{i j}

\end{align}
$$&lt;/div&gt;

&lt;p&gt;where $\xi_{i}$ is the value of the $i^{\text{th}}$ input node and $w_{i j}$ is the weight of the connection between $i^{\text{th}}$ input node and the $j^{\text{th}}$ hidden node. &lt;strong&gt;In short:&lt;/strong&gt; at each hidden layer node, multiply each input value by the connection received by that node and add them together.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the weights are initisliased when the network is setup. Sometimes they are all set to 1, or often they&amp;rsquo;re set to some small random value.&lt;/p&gt;

&lt;p&gt;We apply the activation function on $x_{j}$ at the $j^{\text{th}}$ hidden node and get:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{j} &amp;= \sigma(x_{j}) \\
&amp;= \sigma(  \xi_{1} w_{1j} + \xi_{2} w_{2j})
\end{align}
$$&lt;/div&gt;

&lt;p&gt;$\mathcal{O}_{j}$ is the output of the $j^{\text{th}}$ hidden node. This is calculated for each of the $j$ nodes in the hidden layer. The resulting outputs now become the input for the next layer in the network. In our case, this is the final output later. So for each of the $k$ nodes in $K$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{k} &amp;= \sigma(x_{k}) \\
&amp;= \sigma \left( \sum_{j \in J}  \mathcal{O}_{j} w_{jk}  \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;As we&amp;rsquo;ve reached the end of the network, this is also the end of the feed-foward pass. So how well did our network do at getting the correct result $\mathcal{O}_{k}$? As this is the training phase of our network, the true results will be known an we cal calculate the error.&lt;/p&gt;

&lt;h2 id=&#34;error&#34;&gt;4. Error &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We measure error at the end of each foward pass. This allows us to quantify how well our network has performed in getting the correct output. Let&amp;rsquo;s define $t_{k}$ as the expected or &lt;em&gt;target&lt;/em&gt; value of the $k^{\text{th}}$ node of the output layer $K$. Then the error $E$ on the entire output is:&lt;/p&gt;

&lt;div id=&#34;eqerror&#34;&gt;$$
\text{E} = \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;Dont&amp;rsquo; be put off by the random &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; in front there, it&amp;rsquo;s been manufactured that way to make the upcoming maths easier. The rest of this should be easy enough: get the residual (difference between the target and output values), square this to get rid of any negatives and sum this over all of the nodes in the output layer.&lt;/p&gt;

&lt;p&gt;Good! Now how does this help us? Our aim here is to find a way to tune our network such that when we do a forward pass of the input data, the output is exactly what we know it should be. But we can&amp;rsquo;t change the input data, so there are only two other things we can change:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the weights going into the activation function&lt;/li&gt;
&lt;li&gt;the activation function itself&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will indeed consider the second case in another post, but the magic of NN is all about the &lt;em&gt;weights&lt;/em&gt;. Getting each weight i.e. each connection between nodes, to be just the perfect value, is what back propagation is all about. The back propagation algorithm we will look at in the next section, but lets go ahead and set it up by considering the following: how much of this error $E$ has come from each of the weights in the network?&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re asking, what is the proportion of the error coming from each of the $W_{jk}$ connections between the nodes in layer $J$ and the output layer $K$. Or in mathematical terms:&lt;/p&gt;

&lt;div&gt;$$
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{\partial{}}{\partial{W_{jk}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;If you&amp;rsquo;re not concerned with working out the derivative, skip this highlighted section.&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

To tackle this we can use the following bits of knowledge: the derivative of the sum is equal to the sum of the derivatives i.e. we can move the derivative term inside of the summation:

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \sum_{k \in K} \frac{\partial{}}{\partial{W_{jk}}} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the weight $w_{1k}$ does not affect connection $w_{2k}$ therefore the change in $W_{jk}$ with respect to any node other than the current $k$ is zero. Thus the summation goes away:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;apply the power rule knowing that $t_{k}$ is a constant:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}} &amp;=  \frac{1}{2} \times 2 \times \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right) \\
 &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right)
\end{align}
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the leftover derivative is the chage in the output values with respect to the weights. Substituting $ \mathcal{O}_{k} = \sigma(x_{k}) $ and the sigmoid derivative $\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)$:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( x_{k}\right)
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the final derivative, the input value $x_{k}$ is just $\mathcal{O}_{j} W_{jk}$ i.e. output of the previous layer times the weight to this layer. So the change in  $\mathcal{O}_{j} w_{jk}$ with respect to $w_{jk}$ just gives us the output value of the previous layer $ \mathcal{O}_{j} $ and so the full derivative becomes:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{j} W_{jk} \right) \\[0.5em]
&amp;=\left( \mathcal{O}_{k} - t_{k} \right) \sigma (x )  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j} 
\end{align}
$$&lt;/div&gt;

&lt;p&gt;We can replace the sigmoid function with the output of the layer
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;The derivative of the error function with respect to the weights is then:&lt;/p&gt;

&lt;div id=&#34;derror&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  =\left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j}
$$&lt;/div&gt;

&lt;p&gt;We group the terms involving $k$ and define:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;

&lt;p&gt;And therefore:&lt;/p&gt;

&lt;div id=&#34;derrorjk&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  = \mathcal{O}_{j} \delta_{k} 
$$&lt;/div&gt;

&lt;p&gt;So we have an expression for the amount of error, called &amp;lsquo;deta&amp;rsquo; ($\delta_{k}$), on the weights from the nodes in $J$ to each node $k$ in $K$. But how does this help us to improve out network? We need to back propagate the error.&lt;/p&gt;

&lt;h2 id=&#34;backPropagationGrads&#34;&gt;5. Back Propagation - the gradients&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Back propagation takes the error function we found in the previous section, uses it to calculate the error on the current layer and updates the weights to that layer by some amount.&lt;/p&gt;

&lt;p&gt;So far we&amp;rsquo;ve only looked at the error on the output layer, what about the hidden layer? This also has an error, but the error here depends on the output layer&amp;rsquo;s error too (because this is where the difference between the target $t_{k}$ and output $\mathcal{O}_{k}$ can be calculated). Lets have a look at the error on the weights of the hidden layer $W_{ij}$:&lt;/p&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{ij}}} =  \frac{\partial{}}{\partial{W_{ij}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;p&gt;Now, unlike before, we cannot just drop the summation as the derivative is not directly acting on a subscript $k$ in the summation. We should be careful to note that the output from every node in $J$ is actually connected to each of the nodes in $K$ so the summation should stay. But we can still use the same tricks as before: lets use the power rule again and move the derivative inside (because the summation is finite):&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;=  \frac{1}{2} \times 2 \times  \frac{\partial{}}{\partial{W_{ij}}}   \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)  \mathcal{O}_{k} \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} \mathcal{O}_{k}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Again, we substitute $\mathcal{O}_{k} = \sigma( x_{k})$ and its derivative and revert back to our output notation:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (\sigma(x_{k}) )\\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \sigma(x_{k}) \left( 1 - \sigma(x_{k}) \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k}) \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k})
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;This still looks familar from the output layer derivative, but now we&amp;rsquo;re struggling with the derivative of the input to $k$ i.e. $x_{k}$ with respect to the weights from $I$ to $J$. Let&amp;rsquo;s use the chain rule to break apart this derivative in terms of the output from $J$:&lt;/p&gt;

&lt;div&gt; $$
\frac{\partial{ x_{k}}}{\partial{W_{ij}}} = \frac{\partial{ x_{k}}}{\partial{\mathcal{O}_{j}}}\frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}
$$&lt;/div&gt;

&lt;p&gt;The change of the input to the $k^{\text{th}}$ node with respect to the output from the $j^{\text{th}}$ node is down to a product with the weights, therefore this derivative just becomes the weights $W_{jk}$. The final derivative has nothing to do with the subscript $k$ anymore, so we&amp;rsquo;re free to move this around - lets put it at the beginning:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Lets finish the derivatives, remembering that the output of the node $j$ is just $\mathcal{O}_{j} = \sigma(x_{j}) $ and we know the derivative of this function too:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{}}{\partial{W_{ij}}}\sigma(x_{j})  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \sigma(x_{j}) \left( 1 - \sigma(x_{j}) \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;The final derivative is straightforward too, the derivative of the input to $j$ with repect to the weights is just the previous input, which in our case is $\mathcal{O}_{i}$,&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Almost there! Recall that we defined $\delta_{k}$ earlier, lets sub that in:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \delta_{k} W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;To clean this up, we now define the &amp;lsquo;delta&amp;rsquo; for our hidden layer:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;

&lt;p&gt;Thus, the amount of error on each of the weights going into our hidden layer:&lt;/p&gt;

&lt;div id=&#34;derrorij&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{ij}}}  = \mathcal{O}_{i} \delta_{j} 
$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the reason for the name &lt;em&gt;back&lt;/em&gt; propagation is that we must calculate the errors at the far end of the network and work backwards to be able to calculate the weights at the front.&lt;/p&gt;

&lt;h2 id=&#34;bias&#34;&gt;6.  Bias &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets remind ourselves what happens inside our hidden layer nodes:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInsideNoBias.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Each feature $\xi_{i}$ from the input layer $I$ is multiplied by some weight $w_{ij}$&lt;/li&gt;
&lt;li&gt;These are added together to get $x_{i}$ the total, weighted input from the nodes in $I$&lt;/li&gt;
&lt;li&gt;$x_{i}$ is passed through the activation, or transfer, function $\sigma(x_{i})$&lt;/li&gt;
&lt;li&gt;This gives the output $\mathcal{O}_{j}$ for each of the $j$ nodes in hidden layer $J$&lt;/li&gt;
&lt;li&gt;$\mathcal{O}_{j}$ from each of the $J$ nodes becomes $\xi_{j}$ for the next layer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we talk about the &lt;em&gt;bias&lt;/em&gt; term in NN, we are talking about an additional parameter that is inluded in the summation of step 2 above. The bias term is usually denoted with the symbol $\theta$ (theta). It&amp;rsquo;s function is to act as a threshold for the activation (transfer) function. It is given the value of 1 and is not connected to anything else. As such, this means that any derivative of the node&amp;rsquo;s output with respect to the bias term would just give a constant, 1. This allows us to just think of the bias term as an output from the node with the value of 1. This will be updated later during backpropagation to change the threshold at which the node fires.&lt;/p&gt;

&lt;p&gt;Lets update the equation for $x_{i}$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{i} &amp;= \xi_{1j} w_{1j} + \xi_{2j} w_{2j} + \theta_{j} \\[0.5em]
\sigma( x_{i} ) &amp;= \sigma \left( \sum_{i \in I} \left( \xi_{ij} w_{ij} \right) + \theta_{j} \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;and put it on the diagram:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInside.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;backPropagationAlgorithm&#34;&gt;7. Back Propagation - the algorithm&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now we have all of the pieces! We&amp;rsquo;ve got the initial outputs after our feed-forward, we have the equations for the delta terms (the amount by which the error is based on the different weights) and we know we need to update our bias term too. So what does it look like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Input the data into the network and feed-forward&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;output&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;hidden layer&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calculate the changes that need to be made to the weights and bias terms:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\Delta W &amp;= -\eta \ \delta_{l} \ \mathcal{O}_{l-1} \\
\Delta\theta &amp;= -\eta \ \delta_{l}
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update the weights and biases across the network:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
W + \Delta W &amp;\rightarrow W \\
\theta + \Delta\theta &amp;\rightarrow \theta
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, $\eta$ is just a small number that limit the size of the deltas that we compute: we don&amp;rsquo;t want the network jumping around everywhere. The $l$ subscript denotes the deltas and output for that layer $l$. That is, we compute the delta for each of the nodes in a layer and vectorise them. Thus we can compute the element-wise product with the output values of the previous layer and get our update $\Delta W$ for the weights of the current later. Similarly with the bias term.&lt;/p&gt;

&lt;p&gt;This algorithm is looped over and over until the error between the output and the target values is below some set threshold. Depending on the size of the network i.e. the number of layers and number of nodes per layer, it can take a long time to complete one &amp;lsquo;epoch&amp;rsquo; or run through of this algorithm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some of the ideas notation in this tutorial comes from the good videos by &lt;a href=&#34;https://www.youtube.com/playlist?list=PL29C61214F2146796&#34; title=&#34; NN Videos&#34;&gt;Ryan Harris&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Web Design Wisdom</title>
      <link>/post/webdesign/</link>
      <pubDate>Sat, 04 Mar 2017 17:21:15 +0000</pubDate>
      
      <guid>/post/webdesign/</guid>
      <description>&lt;p&gt;So I&amp;rsquo;m quite a bit into getting MLNotebook setup and I&amp;rsquo;ve been learning a hell of a lot about web design using Hugo (a static site generator). There are a few things around the internet that could be explained more clearly or where more examples could be given, so hopefully that&amp;rsquo;s what I can do for you here!
&lt;/p&gt;

&lt;p&gt;I thought I&amp;rsquo;d give an overview of some of the wisdom I&amp;rsquo;ve gained from creating MLNotebook - my adventures in markdown&amp;hellip; and the rest!&lt;/p&gt;

&lt;h2 id=&#34;hugo&#34;&gt; Hugo &lt;/h2&gt;

&lt;h3 id=&#34;hugoSetup&#34;&gt; Setup &lt;/h3&gt;

&lt;p&gt;Hugo was relatively easy to setup, but I think some of the guides around could be a lot clearer particularly when it comes to hosting on Githib Pages. Firstly, make sure that you download Hugo &lt;a href=&#34;https://github.com/spf13/hugo/releases&#34; title=&#34;Hugo Github&#34;&gt;here&lt;/a&gt; and extract it to &lt;code&gt;/usr/local/bin&lt;/code&gt;. I renamed mine to &amp;ldquo;hugo&amp;rdquo;. Check whether its properly installed with the command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will provide the version number. If not, add &lt;code&gt;/usr/local/bin&lt;/code&gt; to your system path:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ PATH=$PATH:/usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating a new site called &amp;ldquo;newsite&amp;rdquo; from scratch is the easy bit:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo new site ./newsite
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;themeAndOverrides&#34;&gt;Theme and overrides &lt;/h3&gt;

&lt;p&gt;To get my theme to work, I simply cloned the repository (as shown &lt;a href=&#34;https://themes.gohugo.io/blackburn/&#34; title=&#34;Blackburn theme&#34;&gt;here&lt;/a&gt;) directly into ./newsite/themes/blackburn. Be sure to copy the &lt;code&gt;config.toml&lt;/code&gt; file to &lt;code&gt;./newsite&lt;/code&gt;. That&amp;rsquo;s all there is to it!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir themes
$ cd themes
$ git clone https://github.com/yoshiharuyamashita/blackburn.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Customising this theme was really easy as it is mostly done in config.toml. What I wish I knew about Hugo straight off the bat is that the tree structure is important. So anything in the &amp;ldquo;themes&amp;rdquo; folder is a fall-back for anything that &lt;strong&gt;isn&amp;rsquo;t&lt;/strong&gt; present in the root folder of the site. That means if you have your own template for a post in &lt;code&gt;./newsite/layouts/single.html&lt;/code&gt; it will be used instead of the themes one in &lt;code&gt;./newsite/themes/layouts/single.html&lt;/code&gt;. Thus if you want to edit the layout, copy the theme&amp;rsquo;s one into your sites layout folder and edit it from there.&lt;/p&gt;

&lt;p&gt;The index page is the same deal, just copy it to your sites root and it will take precident over the default theme&amp;rsquo;s one.&lt;/p&gt;

&lt;h3 id=&#34;partials&#34;&gt;Partials&lt;/h3&gt;

&lt;p&gt;The partials bit can be a little confusing if you&amp;rsquo;re not too familiar with how the site is put together. Effectively, the page you&amp;rsquo;re loooking at right now is made up of lots of different parts (partials) that have been edited separately, put through a parser, turned into HTML and pasted together into a single HTML page. The head and footer don&amp;rsquo;t have much in them but are important for adding calls to Javascripts as they are stitched into each and every page on the website. Don&amp;rsquo;t confuse the head.html and header.html files, the latter is the actual title/banner at the top of the homepage (it is another partial that is stitched into index.html.&lt;/p&gt;

&lt;h3 id=&#34;socialMediaButtons&#34;&gt;Social Media Buttons&lt;/h3&gt;

&lt;p&gt;I spend a while trying to figure out how to get my social media buttons to actually take the url of the page they were on and share that exact post. I tried a hosted service which gave me a script that pulled down the buttons from them and allowed me to edit them via their interface, but it wasn&amp;rsquo;t content-specific. To dynamically get the url and get some nice-looking icons, I actually used the site &lt;a href=&#34;https://simplesharingbuttons.com/&#34; title=&#34;Simple Sharing Buttons&#34;&gt;Simple Sharing Buttons&lt;/a&gt;, chose the sites I wanted and theyprovided the icons along with the HTML. In comparisson to other sites and methods, this seems to work the best (except for the reddit one really).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;ul class=&amp;quot;share-buttons&amp;quot;&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmlnotebook.github.io&amp;amp;t=&amp;quot; title=&amp;quot;Share on Facebook&amp;quot; target=&amp;quot;_blank&amp;quot; onclick=&amp;quot;window.open(&#39;https://www.facebook.com/sharer/sharer.php?u=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;t=&#39; + encodeURIComponent(document.URL),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Share on facebook&amp;quot; src=&amp;quot;/img/facebook.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;https://twitter.com/intent/tweet?source=https%3A%2F%2Fmlnotebook.github.io&amp;amp;text=:%20https%3A%2F%2Fmlnotebook.github.io&amp;amp;via=mlnotebook&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Tweet&amp;quot; onclick=&amp;quot;window.open(&#39;https://twitter.com/intent/tweet?text=&#39; + encodeURIComponent(document.title) + &#39;:%20&#39;  + encodeURIComponent(document.URL),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Tweet&amp;quot; src=&amp;quot;/img/twitter.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;http://www.reddit.com/submit?url=https%3A%2F%2Fmlnotebook.github.io&amp;amp;title=&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Submit to Reddit&amp;quot; onclick=&amp;quot;window.open(&#39;http://www.reddit.com/submit?url=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;title=&#39; +  encodeURIComponent(document.title),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Submit to Reddit&amp;quot; src=&amp;quot;/img/reddit.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https%3A%2F%2Fmlnotebook.github.io&amp;amp;title=&amp;amp;summary=&amp;amp;source=https%3A%2F%2Fmlnotebook.github.io&amp;quot; target=&amp;quot;_blank&amp;quot; title=&amp;quot;Share on LinkedIn&amp;quot; onclick=&amp;quot;window.open(&#39;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=&#39; + encodeURIComponent(document.URL) + &#39;&amp;amp;title=&#39; +  encodeURIComponent(document.title),&#39;&#39;,&#39;width=500,height=300&#39;); return false;&amp;quot;&amp;gt;&amp;lt;img alt=&amp;quot;Share on LinkedIn&amp;quot; src=&amp;quot;/img/linkedin.png&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;githubPages&#34;&gt; Hosting on Peronal Github Pages &lt;/h3&gt;

&lt;p&gt;Again, some of the tutorials out there aren&amp;rsquo;t great at properly explaining how to get your pages hosted on your &lt;strong&gt;personal&lt;/strong&gt; Github pages, rather than project ones (i.e. &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;) I&amp;rsquo;ll try to give you another version here.&lt;/p&gt;

&lt;p&gt;Firstly, login to Github and create the repository &lt;code&gt;&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;. This is important as the master branch will be used to locate your website at exactly &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt;. Initialise it with the &lt;code&gt;README.md&lt;/code&gt;. Create a new branch called &lt;code&gt;hugo&lt;/code&gt; and initialise this with the &lt;code&gt;README.md&lt;/code&gt; too.&lt;/p&gt;

&lt;p&gt;In your &lt;code&gt;./newsite&lt;/code&gt; directory you&amp;rsquo;ll need to build the site, initialise the git respository and add the remote:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ hugo
$
$ git init
$ git remote add origin git@github.com:&amp;lt;username&amp;gt;/&amp;lt;username&amp;gt;.github.io.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;re having trouble adding the remote because of &lt;em&gt;permissions&lt;/em&gt; it could be that you&amp;rsquo;re using a different Git account for your website than normal. Have a look at the &lt;code&gt;git config&lt;/code&gt; options to change the username/password. If that fails, it could be that you need to sort an &lt;code&gt;ssh&lt;/code&gt; key - instructions for that are on your account settings page.&lt;/p&gt;

&lt;p&gt;From here, I managed to find and adapt two scripts from &lt;a href=&#34;https://hjdskes.github.io/blog/deploying-hugo-on-personal-gh-pages/&#34; title=&#34;hjdskes&#34;&gt;here&lt;/a&gt;. The first is &lt;code&gt;setup.sh&lt;/code&gt; (&lt;a href=&#34;/docs/setup.sh&#34; title=&#34;setup.sh&#34;&gt;download&lt;/a&gt;) and only needs to be executed once. It does the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deletes the master branch (perfectly safe)&lt;/li&gt;
&lt;li&gt;Creates a new orphaned master branch&lt;/li&gt;
&lt;li&gt;Takes the &lt;code&gt;README.md&lt;/code&gt; from &lt;code&gt;hugo&lt;/code&gt; and makes an initial commit to &lt;code&gt;master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Changes back to &lt;code&gt;hugo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Removes the existing &lt;code&gt;./public&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;Sets the &lt;code&gt;master&lt;/code&gt; branch as a subtree for the &lt;code&gt;./public&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;Pulls the commited &lt;code&gt;master&lt;/code&gt; back into &lt;code&gt;./public&lt;/code&gt; to stop merge conflicts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;warn&#34;&gt;Make sure that you edit the `USERNAME` field in `setup.sh` before executing.&lt;/div&gt;

&lt;p&gt;After that, whenever you want to upload your site, just run the second script &lt;code&gt;deploy.sh&lt;/code&gt; which I&amp;rsquo;ve altered slightly (&lt;a href=&#34;/docs/deploy.sh&#34; title=&#34;deploy.sh&#34;&gt;download&lt;/a&gt;) with an optional argument which will be your commit message: missing out the argument submits a default message.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;deploy.sh&lt;/code&gt; commits and pushes all of your changes to the &lt;code&gt;hugo&lt;/code&gt; source branch before putting the &lt;code&gt;./public&lt;/code&gt; folder on &lt;code&gt;master&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&#34;warn&#34;&gt;Make sure that you edit the `USERNAME` field in `deploy.sh` before executing&lt;/div&gt;

&lt;p&gt;And that&amp;rsquo;s it! If the website doesn&amp;rsquo;t load when you go to &lt;code&gt;https://&amp;lt;your username&amp;gt;.github.io&lt;/code&gt; you may need to hit &lt;code&gt;settings&lt;/code&gt; in your repo (top right of the menu bar), scroll down to &amp;ldquo;Github Pages&amp;rdquo; and select &lt;code&gt;master&lt;/code&gt; as your source.&lt;/p&gt;

&lt;h2 id=&#34;htmlCss&#34;&gt;HTML / CSS&lt;/h2&gt;

&lt;h3 id=&#34;contactForm&#34;&gt;Contact Form&lt;/h3&gt;

&lt;p&gt;The first part of the site I altered was the contant page. I added a contact form which largely involves &lt;code&gt;html&lt;/code&gt; formatted with &lt;code&gt;css&lt;/code&gt;. The magic that makes it work comes from the free service called &lt;a href=&#34;https://formspree.io/&#34; title=&#34;Formspree&#34;&gt;Formspree&lt;/a&gt;. Essentially, the submit button sends the information to formspree and they forward it on to me directly. It uses a hidden field to give the forwarded emails the same subject, this makes for easy filtering. It also provides a free &amp;ldquo;I&amp;rsquo;m not a robot&amp;rdquo; page after clicking submit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div id=&amp;quot;contactform&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
&amp;lt;form action=&amp;quot;https://formspree.io/your@email.com method=&amp;quot;POST&amp;quot; name=&amp;quot;sentMessage&amp;quot; id=&amp;quot;contactForm&amp;quot; novalidate&amp;gt;
	&amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;name&amp;quot; placeholder=&amp;quot;Name&amp;quot; id=&amp;quot;name&amp;quot; required data-validation-required-message=&amp;quot;Please enter your name.&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
	&amp;lt;input type=&amp;quot;email&amp;quot; name=&amp;quot;_replyto&amp;quot; placeholder=&amp;quot;Email Address&amp;quot; id=&amp;quot;email&amp;quot; required data-validation-required-message=&amp;quot;Please enter your email address.&amp;quot; &amp;gt;&amp;lt;br&amp;gt;

	&amp;lt;input type=&amp;quot;hidden&amp;quot;  name=&amp;quot;_subject&amp;quot; value=&amp;quot;Message from MLNotebook&amp;quot;&amp;gt;
	&amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;_gotcha&amp;quot; style=&amp;quot;display:none&amp;quot; /&amp;gt;
	&amp;lt;textarea rows=&amp;quot;10&amp;quot; name=&amp;quot;message&amp;quot; class=&amp;quot;form-control&amp;quot; placeholder=&amp;quot;Message&amp;quot; id=&amp;quot;message&amp;quot; required data-validation-required-message=&amp;quot;Please enter a message.&amp;quot;&amp;gt;&amp;lt;/textarea&amp;gt;&amp;lt;br&amp;gt;
	&amp;lt;input type=&amp;quot;submit&amp;quot; value=&amp;quot;Send&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The formatting was a pain as I&amp;rsquo;d never used the box-size argument before - this is what I found made the boxes all the same size and have the same alignment. I added for all browsers too.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;
input[type=text], input[type=email], textarea {
	display: inline-block;
  	border: 1px solid transparent;
  	border-top: none;
  	border-bottom: 1px solid #DDD;
  	box-shadow: inset 0 1px 2px rgba(0,0,0,.39), 0 -1px 1px #FFF, 0 1px 0 #FFF;
	border-radius: 4px;
	margin: 2px 2px 2px 2px;
	resize:none;
	float: left;
	width: 100%;
}

textarea, input {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    box-sizing: border-box;
}

input[type=submit] {
	width: 100%;
}

.center {
	margin: auto;
}

input {
	height:50px;
}

textarea {
	height: 200px;
	padding-left: 0px;
}

input, textarea::-webkit-input-placeholder {
   padding-left: 10px;
}
input, textarea::-moz-placeholder {
   padding-left: 10px;
}
input, textarea:-ms-input-placeholder {
   padding-left: 10px;
}
input, textarea:-moz-placeholder {
   padding-left: 10px;
}
  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;resizing&#34;&gt;Resizing for Small Screens&lt;/h3&gt;

&lt;p&gt;One of my final hurdles in getting the site setup was making the homepage a little more friendly that just showing the recent posts. So I decided to add my &lt;a href=&#34;https://twitter.com/mlnotebook&#34; title=&#34;@MLNotebook&#34;&gt;twitter&lt;/a&gt; feed to the side. Twitter has an easy code to embed this, and I just put it into its own partial in &lt;code&gt;layouts/partials/twitterfeed.html&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;My problem here though was that when I viewed my site on my phone, or resized the web-browser on the computer, the content would shrink and be almost unreadable - I wanted the feed to move below the text if the screen was below a certain size. So I created the usual &lt;code&gt;div&lt;/code&gt; containers within my &lt;code&gt;index.html&lt;/code&gt; file and added the shortcode to include my &lt;code&gt;twitterfeed.html&lt;/code&gt; in the right-hand side.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div id=&amp;quot;container&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
	&amp;lt;div id=&amp;quot;left_content&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
		&amp;lt;div class=&amp;quot;content&amp;quot;&amp;gt;
		  {{ range ( .Paginate (where .Data.Pages &amp;quot;Type&amp;quot; &amp;quot;post&amp;quot;)).Pages }}
		    {{ .Render &amp;quot;summary&amp;quot;}}
		  {{ end }}

		  {{ partial &amp;quot;pagination.html&amp;quot; . }}
		&amp;lt;/div&amp;gt;
	&amp;lt;/div&amp;gt;
	&amp;lt;div id=&amp;quot;right_content&amp;quot; class=&amp;quot;center&amp;quot;&amp;gt;
		&amp;lt;center&amp;gt;{{ partial &amp;quot;twitterfeed.html&amp;quot; . }}&amp;lt;/center&amp;gt;
	&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then used &lt;code&gt;css&lt;/code&gt; to give the &lt;code&gt;div&lt;/code&gt; containers their own properties for different screen sizes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;#container {
	position: relative;
	width:auto;

}

#right_content {
	float:left;
	overflow:hidden;
	display:block;
	padding-right:1%;

}

#left_content {
	float:left;
	width:80%;
	display:block;
	margin:auto;
	min-width=600px;

}

pre &amp;gt; code {
	font-size:11pt;
}

@media screen and (max-width: 1000px) {

#left_content {
	width: 100%;
	}
	
	.content {
	max-width:100%;
	}



#right_content {
	width:100%;
}

pre &amp;gt; code {
	font-size:8pt;
}

}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this allows the size of the font in the code-snippets to shrink when the screensize is small - I find that it reads more easily.&lt;/p&gt;

&lt;h2 id=&#34;syntaxHighlighting&#34;&gt;Syntax highlighting&lt;/h2&gt;

&lt;p&gt;So actually getting code into the website was trickier than I thought. The in-built markdown codeblocks seem to work just fine by adding code between backticks: &lt;code&gt;`&amp;lt;code here&amp;gt;`&lt;/code&gt;. Markdown doesn&amp;rsquo;t do syntax highlightsing right out of the box though. So I&amp;rsquo;m using &lt;code&gt;highlight.js&lt;/code&gt;. My theme does come with a highlight shortcode option, but I found that I couldn&amp;rsquo;t customise it how I wanted - particuarly, the font size was just too big. I tried everything, even adding extra &lt;code&gt;&amp;lt;pre&amp;gt; &amp;lt;/pre&amp;gt;&lt;/code&gt; tags around it and using &lt;code&gt;css&lt;/code&gt; to format them. In the end, I found that using &lt;code&gt;highlight.js&lt;/code&gt; was much simpler - I just loaded the script straight off their server and voila! The link just needed editing to select the theme I wanted, but I opted for the standard &lt;code&gt;monokai&lt;/code&gt; anyway. I placed this in my site&amp;rsquo;s &lt;code&gt;head&lt;/code&gt; partial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai.min.css&amp;quot;&amp;gt;
&amp;lt;script src=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script&amp;gt;hljs.initHighlightingOnLoad();&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mathsRendering&#34;&gt;Maths Rendering&lt;/h2&gt;

&lt;p&gt;Being a site on machine learning, I&amp;rsquo;m going to need to be able to include some mathematics sometimes. I&amp;rsquo;m very familiar with $\rm\LaTeX$ and I&amp;rsquo;ve written-up a lot of formulae already, so I looked into getting $\rm\LaTeX$ formatting into markdown/Hugo. A few math rendering engines are around, but not all are simple to implement. The best option I found was &lt;a href=&#34;https://www.mathjax.org/&#34; title=&#34;MathJax&#34;&gt;MathJax&lt;/a&gt; which literally required me to add these few lines to my &lt;code&gt;head&lt;/code&gt; partial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;
  src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot;&amp;gt;
&amp;lt;/script&amp;gt;

&amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\\[&#39;,&#39;\\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &amp;quot;AMS&amp;quot; },
         extensions: [&amp;quot;AMSmath.js&amp;quot;, &amp;quot;AMSsymbols.js&amp;quot;] }
  }
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From there, it allows me to put inline math into my websites such as $ c = \sqrt{a^{2} + b^{2}} $ by enclosing them in the normal \$ symbols like so: &lt;code&gt;\$ some math \$&lt;/code&gt;. MathJax also provides display-style input with enclosing &lt;code&gt;&amp;lt;div&amp;gt;\$\$ code \$\$&amp;lt;/div&amp;gt;&lt;/code&gt; e.g.:&lt;/p&gt;

&lt;div&gt;$$ c = \sqrt{a^{2} + b^{2}}  $$&lt;/div&gt;

&lt;p&gt;The formatting is done by some css&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;code.has-jax {
	font: inherit;
	font-size: 100%;
	background: inherit;
	border: inherit;
	color: #515151;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/td&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Surface Distance Function</title>
      <link>/post/surface-distance-function/</link>
      <pubDate>Wed, 01 Mar 2017 19:27:27 +0000</pubDate>
      
      <guid>/post/surface-distance-function/</guid>
      <description>&lt;p&gt;Surface Distance measures are a good way of evaluating the accuracy of an image-segmentation if we already know the ground truth (GT). The problem is that there is no nicely packaged function in Python to do this directly. In this post, we&amp;rsquo;ll write a surface distance function in Python which uses numpy and scipy. It&amp;rsquo;ll help us to calculate Mean Surface Distance (MSD), Residual Mean-Square Error (RMS) and the Hausdorff Distance (HD).
&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Recently, I have been doing a &lt;strong&gt;lot&lt;/strong&gt; of segmentation evaluation - seeing how good a segmentation done by a machine compares with one that&amp;rsquo;s done manual, a &amp;lsquo;ground truth&amp;rsquo; (GT). Traditionally, such verification is done by comparing the overlap between the two e.g. Dice Simlarity Coefficient (DSC) [1]. There are a few different calculations that can be done (there&amp;rsquo;ll be a longer post on just that) and &amp;lsquo;surface distance&amp;rsquo; calculations are one of them.&lt;/p&gt;

&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;

&lt;p&gt;For this calculation, we need to be able to find the outline of the segmentation and compare it to the outline of the GT. We can then take measurements of how far each segmentation pixel is from its corresponding pixel in the GT outline.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at the maths. Surface distance metrics estimate the error between the outer surfaces $S$ and $S^{\prime}$ of the segmentations $X$ and $X^{\prime}$. The distance between a point $p$ on surface $S$ and the surface $S^{\prime}$ is given by the minimum of the Euclidean norm:&lt;/p&gt;

&lt;div&gt;$$ d(p, S^{\prime}) = \min_{p^{\prime} \in S^{\prime}} \left|\left| p - p^{\prime} \right|\right|_{2} $$&lt;/div&gt;

&lt;p&gt;Doing this for all pixels in the surface gives the total surface distance between $S$ and $S^{\prime}$: $d(S, S^{\prime})$:&lt;/p&gt;

&lt;p&gt;Now I&amp;rsquo;ve seen MATLAB code that can do this, though often its not entirely accurate. Plus I wanted to do this calculation on-the-fly as part of my program which was written in Python. So I came up with this function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology

def surfd(input1, input2, sampling=1, connectivity=1):
    
    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
    

    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)

    
    dta = morphology.distance_transform_edt(~S,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
       
    
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets go through it bit-by-bit. The function &lt;em&gt;surfd&lt;/em&gt; is defined to take in four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input1&lt;/em&gt; - the segmentation that has been created. It can be a multi-class segmentation, but this function will make the image binary. We&amp;rsquo;ll talk about how to use this function on individual classes later.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;input2&lt;/em&gt; - the GT segmentation against which we wish to compare &lt;em&gt;input1&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sampling&lt;/em&gt; - the pixel resolution or pixel size. This is entered as an &lt;em&gt;n&lt;/em&gt;-vector where &lt;em&gt;n&lt;/em&gt; is equal to the number of dimensions in the segmentation i.e. 2D or 3D. The default value is 1 which means pixels (or rather voxels) are 1 x 1 x 1 mm in size.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;connectivity&lt;/em&gt; - creates either a 2D (3 x 3) or 3D (3 x 3 x 3) matrix defining the neighbourhood around which the function looks for neighbouring pixels. Typically, this is defined as a six-neighbour kernel which is the default behaviour of this function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we&amp;rsquo;ll be making use of simple numpy operations, but we&amp;rsquo;ll also need the &lt;em&gt;morphology&lt;/em&gt; module from &lt;em&gt;scipy&lt;/em&gt;&amp;rsquo;s &lt;em&gt;dnimage&lt;/em&gt; package. These are imported first. More information on this module can be found &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.18.1/reference/ndimage.html&#34; title=&#34;Scipy _ndimage_ package&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two inputs are checked for their size and made binary. Any value greater than zero is made 1 (true).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the the &lt;em&gt;morphology.generate_binary_structure&lt;/em&gt; function, along with the number of dimensions of the segmentation, to create the kernel that will be used to detect the edges of the segmentations. This could be done just by hard-coding the kernel itself: &lt;code&gt;[[0 0 0],[0 1 0],[0 0 0]; [0 1 0], [1 1 1], [0 1 0]; [0 0 0], [0 1 0], [0 0 0]]&lt;/code&gt;. This kernel &amp;lsquo;&lt;em&gt;conn&lt;/em&gt;&amp;rsquo; is supplied to the &lt;em&gt;morphology.binary_erosion&lt;/em&gt; function which strips the outermost pixel from the edge of the segmentation. Subtracting this result from the segmentation itself leaves only the single-pixel-wide surface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we again use the &lt;em&gt;morphology&lt;/em&gt; module. This time we give the &lt;em&gt;distance_transform_edt&lt;/em&gt; function our pixel-size (&lt;em&gt;samping&lt;/em&gt;) and also the inverted surface-image. The inversion is used such that the surface itself is given the value of zero i.e. any pixel at this location, will have zero surface-distance. The transform increases the value/error/penalty of the remaining pixels with increasing distance away from the surface.&lt;/p&gt;

&lt;p&gt;Each pixel of the opposite segmentation-surface is then laid upon this &amp;lsquo;map&amp;rsquo; of penalties and both results are concatenated into a vector which is as long as the number of pixels in the surface of each segmentation. This vector of &lt;em&gt;surface distances&lt;/em&gt; is returned. Note that this is technically the &lt;em&gt;symmetric&lt;/em&gt; surface distance as we are not assuming that just doing this for &lt;em&gt;one&lt;/em&gt; of the surfaces is enough. It may be that the distance between a pixel in A and in B is not the same as between the pixel in B and in A. i.e. $d(S, S^{\prime}) \neq d(S^{\prime}, S)$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    dta = morphology.distance_transform_edt(~input1_border,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
        
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-is-it-used&#34;&gt;How is it used?&lt;/h2&gt;

&lt;p&gt;The function example below takes two segmentations (which both have multiple classes). The sampling vector is a typical pixel-size from an MRI scan and the 1 indicated I&amp;rsquo;d like a 6 neighbour (cross-shaped) kernel for finding the edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg, GT_seg, [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By specifcing the value of the voxel-label I&amp;rsquo;m interested in (assuming we&amp;rsquo;re talking about classes which are contiguous and not spread out), we can find the surface accuracy of that class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg(test_seg==1), \
                   GT_seg(GT_seg==1), [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-do-the-results-mean&#34;&gt;What do the results mean?&lt;/h2&gt;

&lt;p&gt;The returned surface distances can be used to calculate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Mean Surface Distance (MSD)&lt;/em&gt; - the mean of the vector is taken. This tell us how much, on average, the surface varies between the segmentation and the GT (in mm).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$     \text{MSD} = \frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime}) + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S) \right) $$ &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Residual Mean Square Distance (RMS)&lt;/em&gt; - as it says, the mean is taken from each of the points in the vector, these residuals are squared (to remove negative signs), summated, weighted by the mean and then the square-root is taken. Measured in mm.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \text{RMS} = \sqrt{\frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime})^{2} + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S)^{2} \right) }\ $$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hausdorff Distance (HD)&lt;/em&gt; - the maximum of the vector. The largest difference between the surface distances. Also measured in mm. We calculate the &lt;em&gt;symmetric Hausdorff distance&lt;/em&gt; as:&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$\text{HD} = \max \left[ d(S, S^{\prime}) , d(S^{\prime}, S) \right]$$&lt;/div&gt;

&lt;p&gt;Or in Python:
&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    msd = surface_distance.mean()
    rms = np.sqrt((surface_distance**2).mean())
    hd  = surface_distance.max()
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1]     Dice, L. R. (1945). Measures of the Amount of Ecologic Association Between Species. Ecology, 26(3), 297–302. &lt;a href=&#34;https://doi.org/10.2307/1932409&#34;&gt;https://doi.org/10.2307/1932409&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MLNotebook First Post</title>
      <link>/post/my-first-post/</link>
      <pubDate>Wed, 01 Mar 2017 10:38:47 +0000</pubDate>
      
      <guid>/post/my-first-post/</guid>
      <description>&lt;p&gt;This is the first post on the Machine Learning Notebook (MLN)! I&amp;rsquo;ll give a brief overview of what the site is for, how I&amp;rsquo;m constructing it and the kinds of things I&amp;rsquo;m learning along the way.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m a n00b when it comes to Github Pages and relatively new to working with Github at all! It took me a while to decide what platform to use for creating this humble resource. I was initially looking at writing everything in iPython notebooks, but it was confusing to figure out how to keep the scripts live rather than coverting them to static sites.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve read several blogs, but &lt;a href=&#34;http://bruceeckel.github.io/2014/11/19/using-github-pages/&#34; title=&#34;Using Github Pages&#34;&gt;this one&lt;/a&gt; seemed to be written to my liking and was pretty much a document of trail and error, but perhaps more imporantly, &lt;strong&gt;solutions&lt;/strong&gt;. I&amp;rsquo;m actually writing this first post after reading &lt;a href=&#34;https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/&#34; title=&#34;Build a Blog with Jekyll and Github Pages&#34;&gt;this&lt;/a&gt; where I&amp;rsquo;ve settled on using Jekyll. Though I must say that I was initially pushed away from this idea after seeing that Jekyll was written for Ruby (in which I have no experience). But here it goes - hopefully it&amp;rsquo;ll be straight-forward.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/em&gt;: Scratch that - after playing around with &lt;a href=&#34;https://gohugo.io/&#34; title=&#34;Hugo Homepage&#34;&gt;Hugo&lt;/a&gt; and reading the &amp;lsquo;start from scratch&amp;rsquo; post &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-and-use-hugo-a-static-site-generator-on-ubuntu-14-04#adjusting-the-initial-configuration-for-your-site&#34; title=&#34;Quickstart Hugo&#34;&gt;here&lt;/a&gt; I think I&amp;rsquo;m a hugo-convert!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m (still) going to have to play about with writing in markdown just to get used to the command structure. Luckily, I found a good cheatsheet over &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34; title=&#34;Markdown Cheatsheet&#34;&gt;here&lt;/a&gt; and another &lt;a href=&#34;https://sourceforge.net/p/hugo-generator/wiki/markdown_syntax/&#34; title=&#34;Sourceforge Markdown&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Eventually, I want this site to build up into small sampling of what I&amp;rsquo;ve been learning to do whilst learning Python and applying it to Machine Learning problems. It may at times become quite specialised with my research in medical imaging, but the general processes (I hope) should benefit at least one other person in the world.&lt;/p&gt;

&lt;p&gt;Here goes!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/em&gt;: Just to test out the code-block highlighting&amp;hellip;
&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;def hello_world():
    print &amp;ldquo;Hello there!&amp;rdquo;
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>