<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Machine Learning Notebook</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Machine Learning Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jan 2018 10:13:20 +0000</lastBuildDate>
    <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Data Augmentations for n-Dimensional Image Input to CNNs</title>
      <link>/post/dataaug/</link>
      <pubDate>Thu, 04 Jan 2018 10:13:20 +0000</pubDate>
      
      <guid>/post/dataaug/</guid>
      <description>&lt;p&gt;One of the greatest limiting factors for training effective deep learning frameworks is the availability, quality and organisation of the &lt;em&gt;training data&lt;/em&gt;. To be good at classification tasks, we need to show our CNNs &lt;em&gt;etc.&lt;/em&gt; as many examples as we possibly can. However, this is not always possible especially in situations where the training data is hard to collect e.g. medical image data. In this post, we will learn how to apply &lt;em&gt;data augmentation&lt;/em&gt; strategies to n-Dimensional images get the most of our limited number of examples.&lt;/p&gt;

&lt;h2 id=&#34;intro&#34;&gt; Introduction &lt;/h2&gt;

&lt;p&gt;If we take any image, like our little Android below, and we shift all of the data in the image to the right by a single pixel, you may struggle to see any difference visually. However, numerically, this may as well be a completely different image! Imagine taking a stack of 10 of these images, each shifted by a single pixel compared to the previous one. Now consider the pixels in the images at [20, 25] or some arbitrary location. Focusing on that point, each pixel has a different colour, different average surrounding intensity etc. A CNN take these values into account when performing convolutions and deciding upon weights. If we supplied this set of 10 images to a CNN, it would effectively be making it learn that it should be invariant to these kinds of translations.&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34;  style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/android.jpg&#34; &gt;&lt;br&gt;
&lt;b&gt;Android&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34;&#34; height=300 src=&#34;/img/augmentation/android1px.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted 1 pixel right&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34;&#34; height=300 src=&#34;/img/augmentation/android10px.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted 10 pixels right&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Of course, translations are not the only way in which an image can change, but still &lt;em&gt;visually&lt;/em&gt; be the same image. Consider rotating the image by even a single degree, or 5 degrees. It&amp;rsquo;s still an Android. Traning a CNN without including translated and rotated versions of the image may cause the CNN to &lt;strong&gt;overfit&lt;/strong&gt; and assume that all images of Androids have to be perfectly upright and centered.&lt;/p&gt;

&lt;p&gt;Providing deep learning frameworks with images that are translated, rotated, scaling, intensified and flipped is what we mean when we talk about &lt;em&gt;data augmentation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In this post we&amp;rsquo;ll look at how to apply these transformations to an image, even in 3D and see how it affects the performance of a deep learning framework. We will use an image from &lt;em&gt;flickr&lt;/em&gt; user  &lt;a href=&#34;https://www.flickr.com/photos/andy_emcee/6416366321&#34; title=&#34;Cat and Dog Image&#34;&gt;andy_emcee&lt;/a&gt; as an example of a 2D nautral image. As this is an RGB (color) image it has shape [512, 640, 3], one layer for each colour channel. We could take one layer to make this grayscale and truly 2D, but most images we deal with will be color so let&amp;rsquo;s leave it. For 3D we will use a 3D MRI scan&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:49%; margin:auto;min-width:350px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34; height=300 src=&#34;/img/augmentation/naturalimg.jpg&#34;&gt;&lt;br&gt;
&lt;b&gt;RGB Image shape=[512, 640, 3]&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;augs&#34;&gt; Augmentations &lt;/h2&gt;

&lt;p&gt;As usual, we are going to write our augmentation functions in python. We&amp;rsquo;ll just be using simple functions from &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;translate&#34;&gt; Translation &lt;/h3&gt;

&lt;p&gt;In our functions, &lt;code&gt;image&lt;/code&gt; is a 2 or 3D array - if it&amp;rsquo;s a 3D array, we need to be careful about specifying our translation directions in the argument called &lt;code&gt;offset&lt;/code&gt;. We don&amp;rsquo;t really want to move images in the &lt;code&gt;z&lt;/code&gt; direction for a couple of reasons: firstly, if it&amp;rsquo;s a 2D image, the third dimension will be the colour channel, if we move the image through this dimension the image will either become all red, all blue or all black if we move it &lt;code&gt;-2&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt; or greater than these respectively; second, in a full 3D image, the third dimension is often the smallest e.g. most medical scans. In our translation function below, the &lt;code&gt;offset&lt;/code&gt; is given as a length 2 array defining the shift in the &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; directions respectively (dont forget index 0 is which horizontal row we&amp;rsquo;re at in python). We hard-code z-direction to&lt;code&gt;0&lt;/code&gt; but you&amp;rsquo;re welcome to change this if your use-case demands it. To ensure we get integer-pixel shifts, we enforce type &lt;code&gt;int&lt;/code&gt; too.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def translateit(image, offset, isseg=False):
    order = 0 if isseg == True else 5

    return scipy.ndimage.interpolation.shift(image, (int(offset[0]), int(offset[1]), 0), order=order, mode=&#39;nearest&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we have also provided the option for what kind of interpolation we want to perform: &lt;code&gt;order = 0&lt;/code&gt; means to just use the nearest-neighbour pixel intensity and &lt;code&gt;order = 5&lt;/code&gt; means to perform bspline interpolation with order 5 (taking into account many pixels around the target). This is triggered with a Boolean argument to the &lt;code&gt;scaleit&lt;/code&gt; function called &lt;code&gt;isseg&lt;/code&gt; so named because when dealing with image-segmentations, we want to keep their integer class numbers and not get a result which is a float with a value between two classes. This is not a problem with the actual image as we want to retain as much visual smoothness as possible (though there is an arugment that we&amp;rsquo;re introducing data which didn&amp;rsquo;t exist in the original image). Similarly, when we move our image, we will leave a gap around the edges from which it&amp;rsquo;s moved. We need a way to fill in this gap: by default &lt;code&gt;shift&lt;/code&gt; will use a contant value set to &lt;code&gt;0&lt;/code&gt;. This may not be helpful in some case, so it&amp;rsquo;s best to set the &lt;code&gt;mode&lt;/code&gt; to &lt;code&gt;&#39;nearest&#39;&lt;/code&gt; which takes the cloest pixel-value and replicates it. It&amp;rsquo;s barely noticable with small shifts but looks wrong at larger offsets. We need to be careful and only apply small translations to our data.&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34;  style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimg.jpg&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgtrans5px.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted 5 pixels right&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgtrans25px.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted 25 pixels right&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimg.png&#34; &gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrseg.png&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image and Segmentation&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgtrans1.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegtrans1.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted [-3, 1] pixels&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgtrans2.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegtrans2.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Shifted [4, -5] pixels&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;scale&#34;&gt; Scaling &lt;/h3&gt;

&lt;p&gt;When scaling an image, i.e. zooming in and out, we want to increase or decrease the area our image takes up whilst keeping the image dimensions the same. We scale our image by a certain &lt;code&gt;factor&lt;/code&gt;. A &lt;code&gt;factor &amp;gt; 1.0&lt;/code&gt; means the image scales-up, and &lt;code&gt;factor &amp;lt; 1.0&lt;/code&gt; scales the image down. Note that we should provide a factor for each dimension: if we want to keep the same number of layers or slices in our image, we should set last value to &lt;code&gt;1.0&lt;/code&gt;. To determine the intensity of the resulting image at each pixel, we are taking the lattice (grid) on which each pixel sits and using this to perform &lt;em&gt;interpolation&lt;/em&gt; of the surrounding pixel intensities. &lt;code&gt;scipy&lt;/code&gt; provides a handy function for this called &lt;code&gt;zoom&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;The definition is probably more complex than one would think:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def scaleit(image, factor, isseg=False):
    order = 0 if isseg == True else 3

    height, width, depth= image.shape
    zheight             = int(np.round(factor * height))
    zwidth              = int(np.round(factor * width))
    zdepth              = depth

    if factor &amp;lt; 1.0:
        newimg  = np.zeros_like(image)
        row     = (height - zheight) // 2
        col     = (width - zwidth) // 2
        layer   = (depth - zdepth) // 2
        newimg[row:row+zheight, col:col+zwidth, layer:layer+zdepth] = interpolation.zoom(image, (float(factor), float(factor), 1.0), order=order, mode=&#39;nearest&#39;)[0:zheight, 0:zwidth, 0:zdepth]

        return newimg

    elif factor &amp;gt; 1.0:
        row     = (zheight - height) // 2
        col     = (zwidth - width) // 2
        layer   = (zdepth - depth) // 2

        newimg = interpolation.zoom(image[row:row+zheight, col:col+zwidth, layer:layer+zdepth], (float(factor), float(factor), 1.0), order=order, mode=&#39;nearest&#39;)  
        
        extrah = (newimg.shape[0] - height) // 2
        extraw = (newimg.shape[1] - width) // 2
        extrad = (newimg.shape[2] - depth) // 2
        newimg = newimg[extrah:extrah+height, extraw:extraw+width, extrad:extrad+depth]

        return newimg

    else:
        return image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are three possibilities that we need to consider - we are scaling up, down or no scaling. In each case, we want to return an array that is &lt;em&gt;equal in size&lt;/em&gt; to the input &lt;code&gt;image&lt;/code&gt;. For the scaling down case, this involves making a blank image the same shape as the input, and finding the corresponding box in the resulting scaled image. For scaling up, it&amp;rsquo;s unnecessary to perform the scaling on the whole image, just the portion that will be &amp;lsquo;zoomed&amp;rsquo; - so we pass only part of the array to the &lt;code&gt;zoom&lt;/code&gt; function. There may also be some error in the final shape due to rounding, so we do some trimming of the extra rows and colums before passing it back. When no scaling is done, we just return the original image.&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34;  style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimg.jpg&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgscale075.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Scale-factor 0.75&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgscale125.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Scale-factor 1.25&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimg.png&#34; &gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrseg.png&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image and Segmentation&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgscale1.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegtrans1.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Scale-factor 1.07&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgscale2.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegtrans2.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Scale-factor 0.95&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#39;resample&#39;&gt; Resampling &lt;/h3&gt;

&lt;p&gt;It may be the case that we want to change the dimensions of our image such that they fit nicely into the input of our CNN. For example, most images and photographs have one dimension larger than the other or may be of different resolutions. This may not be the case in our training set, but most CNNs prefer to have inputs that are square and of identical sizes. We can use the same &lt;code&gt;scipy&lt;/code&gt; function &lt;code&gt;interpolation.zoom&lt;/code&gt; to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def resampleit(image, dims, isseg=False):
    order = 0 if isseg == True else 5

    image = interpolation.zoom(image, np.array(dims)/np.array(image.shape, dtype=np.float32), order=order, mode=&#39;nearest&#39;)

    if image.shape[-1] == 3: #rgb image
        return image
    else:
        return image if isseg else (image-image.min())/(image.max()-image.min()) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The key part here is that we&amp;rsquo;ve replaced the &lt;code&gt;factor&lt;/code&gt; argument with &lt;code&gt;dims&lt;/code&gt; of type &lt;code&gt;list&lt;/code&gt;. &lt;code&gt;dims&lt;/code&gt; should have length equal to the number of dimensions of our image i.e. 2 or 3. We are calculating the factor that each dimension needs to change by in order to change the image to the target &lt;code&gt;dims&lt;/code&gt;. We&amp;rsquo;ve forced the denominator of the scaling factor to be of type &lt;code&gt;float&lt;/code&gt; so that the resulting factor is also &lt;code&gt;float&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In this step, we are also changing the intensities of the image to use the full range from &lt;code&gt;0.0&lt;/code&gt; to &lt;code&gt;1.0&lt;/code&gt;. This ensures that all of our image intensities fall over the same range - one fewer thing for the network to be biased against. Again, note that we don&amp;rsquo;t want to do this for our segmentations as the pixel &amp;lsquo;intensities&amp;rsquo; are actually labels. We could do this in a separate function, but I want this to happen to all of my images at this point. There&amp;rsquo;s no difference to the visual display of the images because they are automaticallys rescaled to use the full range of display colours.&lt;/p&gt;

&lt;h3 id=&#34;rotate&#34;&gt; Rotation &lt;/h3&gt;

&lt;p&gt;This function utilises another &lt;code&gt;scipy&lt;/code&gt; function called &lt;code&gt;rotate&lt;/code&gt;. It takes a &lt;code&gt;float&lt;/code&gt; for the &lt;code&gt;theta&lt;/code&gt; argument which specifies the number of degrees of the roation (negative numbers rotate anti-clockwise). We want the returned image to be of the same shape as the input &lt;code&gt;image&lt;/code&gt; so &lt;code&gt;reshape = False&lt;/code&gt; is used. Again we need to specify the &lt;code&gt;order&lt;/code&gt; of the interpolation on the new lattice. The rotate function handles 3D images by rotating each slice by the same &lt;code&gt;theta&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def rotateit(image, theta, isseg=False):
    order = 0 if isseg == True else 5
        
    return rotate(image, float(theta), reshape=False, order=order, mode=&#39;nearest&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34;  style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimg.jpg&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgrotate-10.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Theta = -10.0 &lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgrotate10.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Theta = 10.0&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimg.png&#34; &gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrseg.png&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image and Segmentation&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgscale1.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegrotate1.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Theta = 6.18&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgrotate2.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegtrans2.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Theta = -1.91&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;intensify&#34;&gt; Intensity Changes &lt;/h3&gt;

&lt;p&gt;The final augmentation we can perform is a scaling in the intensity of the pixels. This effectively brightens or dims the image by appling a blanket increase or decrease across all pixels. We specify the amount by a factor: &lt;code&gt;factor &amp;lt; 1.0&lt;/code&gt; will dim the image, and &lt;code&gt;factor &amp;gt; 1.0&lt;/code&gt; will brighten it. Note that we don&amp;rsquo;t want a &lt;code&gt;factor = 0.0&lt;/code&gt; as this will blank the image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def intensifyit(image, factor):

    return image*float(factor)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;flip&#34;&gt; Flipping &lt;/h3&gt;

&lt;p&gt;One of the most common image augmentation procedures for natural images (dogs, cats, landscapes etc.) is to do flipping. The premise being that a dog is a dog no matter which was it&amp;rsquo;s facing. Or it doesn&amp;rsquo;t matter if a tree is on the right or the left of an image, it&amp;rsquo;s still a tree.&lt;/p&gt;

&lt;p&gt;We can do horizontal flipping, left-to-right or vertical flipping, up and down. It may make sense to do only one of these (if we know that dogs don&amp;rsquo;t walk on their heads for example). In this case, we can specify a &lt;code&gt;list&lt;/code&gt; of 2 boolean values: if each is &lt;code&gt;1&lt;/code&gt; then both flips are performed. We use the &lt;code&gt;numpy&lt;/code&gt; functions &lt;code&gt;fliplr&lt;/code&gt; and &lt;code&gt;flipup&lt;/code&gt; for these.&lt;/p&gt;

&lt;p&gt;As with resampling, the intensity changes are modified to take the range of the display so there wont be a noticable difference in the images. The maximum value for display is 255 so increasing this will just scale it back down.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def flipit(image, axes):
    
    if axes[0]:
        image = np.fliplr(image)
    if axes[1]:
        image = np.flipud(image)
    
    return image
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;cropping&#34;&gt; Cropping &lt;/h3&gt;

&lt;p&gt;This may be a very niche function, but it&amp;rsquo;s important in my case. Often in natrual image processing, random crops are done on the image in order to give patches - these patches often contain most of the image data e.g. 224 x 224 patch rather than 299 x 299 image. This is just another way of showing the network a very similar but also entirely different image. Central crops are also done. What&amp;rsquo;s different in my case is that I always want my segmentation to be fully-visible in the image that I show to the network (I&amp;rsquo;m working with 3D cardiac MRI segmentations).&lt;/p&gt;

&lt;p&gt;So this function looks at the segmentation and creates a bounding box using the outermost pixels. We&amp;rsquo;re producing &amp;lsquo;square&amp;rsquo; crops with side-length equal to the width of the image (the shortest side not including the depth). In this case, the bounding box is created and, if necessary, the window is moved up and down the image to make sure the full segmentation is visible. It also makes sure that the output is always square in the case that the bounding box moves off the image array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cropit(image, seg=None, margin=5):

    fixedaxes = np.argmin(image.shape[:2])
    trimaxes  = 0 if fixedaxes == 1 else 1
    trim    = image.shape[fixedaxes]
    center  = image.shape[trimaxes] // 2

    print image.shape
    print fixedaxes
    print trimaxes
    print trim
    print center

    if seg is not None:

        hits = np.where(seg!=0)
        mins = np.argmin(hits, axis=1)
        maxs = np.argmax(hits, axis=1)

        if center - (trim // 2) &amp;gt; mins[0]:
            while center - (trim // 2) &amp;gt; mins[0]:
                center = center - 1
            center = center + margin

        if center + (trim // 2) &amp;lt; maxs[0]:
            while center + (trim // 2) &amp;lt; maxs[0]:
                center = center + 1
            center = center + margin
    
    top    = max(0, center - (trim //2))
    bottom = trim if top == 0 else center + (trim//2)

    if bottom &amp;gt; image.shape[trimaxes]:
        bottom = image.shape[trimaxes]
        top = image.shape[trimaxes] - trim
  
    if trimaxes == 0:
        image   = image[top: bottom, :, :]
    else:
        image   = image[:, top: bottom, :]

    if seg is not None:
        if trimaxes == 0:
            seg   = seg[top: bottom, :, :]
        else:
            seg   = seg[:, top: bottom, :]

        return image, seg
    else:
        return image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this function will work to square an image even when there is no segmentation given. We also have to be careful about which axes we take as the &amp;lsquo;fixed&amp;rsquo; length for the square and which one to trim.&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;Natural Image RGB&#34;  style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimg.jpg&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;Natural Image Grayscale&#34; style=&#34;border: 2px solid black;&#34; height=300 src=&#34;/img/augmentation/naturalimgcrop.png&#34;&gt;&lt;br&gt;
&lt;b&gt; Cropped &lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:29%; margin:auto;min-width:325px;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimg.png&#34; &gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrseg.png&#34; &gt;&lt;br&gt;
&lt;b&gt;Original Image and Segmentation&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:325px;display:inline-block; width:29%;margin:auto;&#34;&gt;
&lt;img title=&#34;CMR Image&#34; height=300 src=&#34;/img/augmentation/cmrimgcrop.png&#34;&gt;
&lt;img title=&#34;CMR Segmentation&#34; height=300 src=&#34;/img/augmentation/cmrsegcrop.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Cropped&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;application&#34;&gt; Application &lt;/h2&gt;

&lt;p&gt;We should be careful about how we apply our transformations. For example, if we apply multiple transformations to the same image we need to make sure that we don&amp;rsquo;t apply &amp;lsquo;resampling&amp;rsquo; after &amp;lsquo;intensity changes&amp;rsquo; because this will reset the range of the image, defeating the point of the intensification. However, as we will generally want our data to span the same range, wholesale intensity shifts are less often seen. We also want to make sure that we are not being over zealous with the augmentations either - we need to set limits for our factors and other arguments.&lt;/p&gt;

&lt;p&gt;When I implement data augmentation, I put all of these transforms into one script which can be downloaded here: &lt;a href=&#34;/docs/transforms.py&#34; title=&#34;transforms.py&#34;&gt;&lt;code&gt;transforms.py&lt;/code&gt;&lt;/a&gt;. I then call the transforms that I want from another script.&lt;/p&gt;

&lt;p&gt;We create a set of cases, one for each transformation, which draws random (but controlled) parameters for our augmentations, remember we don&amp;rsquo;t want anything too extreme. We don&amp;rsquo;t want to apply all of these transformations every time, so we also create an array of random length (number of transformations) and randomly assigned elements (the transformations to apply).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.random.seed()
numTrans     = np.random.randint(1, 6, size=1) 
allowedTrans = [0, 1, 2, 3, 4]
whichTrans   = np.random.choice(allowedTrans, numTrans, replace=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We assign a new &lt;code&gt;random.seed&lt;/code&gt; every time to ensure that each pass is different to the last. There are 5 possible transformations so &lt;code&gt;numTrans&lt;/code&gt; is a single random integer between 1 and 5. We then take a &lt;code&gt;random.choice&lt;/code&gt; of the &lt;code&gt;allowedTrans&lt;/code&gt; up to &lt;code&gt;numTrans&lt;/code&gt;. We don&amp;rsquo;t want to apply the same transformation more than once, so &lt;code&gt;replace=False&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After some trial and error, I&amp;rsquo;ve found that the following parameters are good:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rotations - &lt;code&gt;theta&lt;/code&gt; $ \in [-10.0, 10.0] $ degrees&lt;/li&gt;
&lt;li&gt;scaling - &lt;code&gt;factor&lt;/code&gt; $ \in [0.9, 1.1] $ i.e. 10% zoom-in or zoom-out&lt;/li&gt;
&lt;li&gt;intensity - &lt;code&gt;factor&lt;/code&gt; $ \in [0.8, 1.2] $ i.e. 20% increase or decrease&lt;/li&gt;
&lt;li&gt;translation - &lt;code&gt;offset&lt;/code&gt; $ \in [-5, 5] $ pixels&lt;/li&gt;
&lt;li&gt;margin - I tend to set at either 5 or 10 pixels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For an image called &lt;code&gt;thisim&lt;/code&gt; and segmentation called &lt;code&gt;thisseg&lt;/code&gt;, the cases I use are:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if 0 in whichTrans:
    theta   = float(np.around(np.random.uniform(-10.0,10.0, size=1), 2))
    thisim  = rotateit(thisim, theta)
    thisseg = rotateit(thisseg, theta, isseg=True) if withseg else np.zeros_like(thisim)

if 1 in whichTrans:
    scalefactor  = float(np.around(np.random.uniform(0.9, 1.1, size=1), 2))
    thisim  = scaleit(thisim, scalefactor)
    thisseg = scaleit(thisseg, scalefactor, isseg=True) if withseg else np.zeros_like(thisim)

if 2 in whichTrans:
    factor  = float(np.around(np.random.uniform(0.8, 1.2, size=1), 2))
    thisim  = intensifyit(thisim, factor)
    #no intensity change on segmentation

if 3 in whichTrans:
    axes    = list(np.random.choice(2, 1, replace=True))
    thisim  = flipit(thisim, axes+[0])
    thisseg = flipit(thisseg, axes+[0]) if withseg else np.zeros_like(thisim)

if 4 in whichTrans:
    offset  = list(np.random.randint(-5,5, size=2))
    currseg = thisseg
    thisim  = translateit(thisim, offset)
    thisseg = translateit(thisseg, offset, isseg=True) if withseg else np.zeros_like(thisim)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In each case, a random set of parameters is found and passed to the transform functions. The image and segmentation are passed separately to each one. In my case, I only choose to flip horizontally by randomly choosing 0 or 1 and appending &lt;code&gt;[0]&lt;/code&gt; such that the transform ignores the second axis. We&amp;rsquo;ve also added a boolean variable called &lt;code&gt;withseg&lt;/code&gt;. When &lt;code&gt;True&lt;/code&gt; the segmentation is augmented, otherwise a blank image is returned.&lt;/p&gt;

&lt;p&gt;Finally, we crop the image to make it square before resampling it to the desired &lt;code&gt;dims&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;thisim, thisseg = cropit(thisim, thisseg)
thisim          = resampleit(thisim, dims)
thisseg         = resampleit(thisseg, dims, isseg=True) if withseg else np.zeros_like(thisim)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this together in a script makes testing the augmenter easier: you can download the script &lt;a href=&#34;/docs/augmenter.py&#34; title=&#34;augmenter.py&#34;&gt;here&lt;/a&gt;. Some things in the code to note:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The script takes one mandatory argument (image filename) and an optional segmentation filename&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s a bit of error checking - are the files able to be loaded? Is it an rgb or full 3D image (3rd dimension greater than 3).&lt;/li&gt;
&lt;li&gt;We specify the final image dimensions, [224, 224, 8] in this case&lt;/li&gt;
&lt;li&gt;We also declare some default values for the parameters so that we can&amp;hellip;&lt;/li&gt;
&lt;li&gt;&amp;hellip;print out the applied transformations and their parameters at the end&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s a definition for a &lt;code&gt;plotit&lt;/code&gt; function that just creates a 2 x 2 matrix where the top 2 images are the originals and the bottom two are the augmented images.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s a commented out part which is what I used to save the images created in this post&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a live setting where we want to do data-augmentation on the fly, we would essentially call this script with the filenames or image arrays to augment and create as many augmentations of the images as we wish. We&amp;rsquo;ll take a look at this as an example in the next post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks - Basics</title>
      <link>/post/CNN1/</link>
      <pubDate>Fri, 07 Apr 2017 09:46:56 +0100</pubDate>
      
      <guid>/post/CNN1/</guid>
      <description>&lt;p&gt;This series will give some background to CNNs, their architecture, coding and tuning. In particular, this tutorial covers some of the background to CNNs and Deep Learning. We won&amp;rsquo;t go over any coding in this session, but that will come in the next one. What&amp;rsquo;s the big deal about CNNs? What do they look like? Why do they work? Find out in this tutorial.&lt;/p&gt;

&lt;h2 id=&#34;intro&#34;&gt;  Introduction &lt;/h2&gt;

&lt;p&gt;A convolutional neural network (CNN) is very much related to the standard NN we&amp;rsquo;ve previously encountered. I found that when I searched for the link between the two, there seemed to be no natural progression from one to the other in terms of tutorials. It would seem that CNNs were developed in the late 1980s and then forgotten about due to the lack of processing power. In fact, it wasn&amp;rsquo;t until the advent of cheap, but powerful GPUs (graphics cards) that the research on CNNs and Deep Learning in general was given new life. Thus you&amp;rsquo;ll find an explosion of papers on CNNs in the last 3 or 4 years.&lt;/p&gt;

&lt;p&gt;Nonetheless, the research that has been churned out is &lt;em&gt;powerful&lt;/em&gt;. CNNs are used in so many applications now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Object recognition in images and videos (think image-search in Google, tagging friends faces in Facebook, adding filters in Snapchat and tracking movement in Kinect)&lt;/li&gt;
&lt;li&gt;Natural language processing (speech recognition in Google Assistant or Amazon&amp;rsquo;s Alexa)&lt;/li&gt;
&lt;li&gt;Playing games (the recent &lt;a href=&#34;https://en.wikipedia.org/wiki/AlphaGo&#34; title=&#34;AlphaGo on Wiki&#34;&gt;defeat of the world &amp;lsquo;Go&amp;rsquo; champion&lt;/a&gt; by DeepMind at Google)&lt;/li&gt;
&lt;li&gt;Medical innovation (from drug discovery to prediction of disease)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dispite the differences between these applications and the ever-increasing sophistication of CNNs, they all start out in the same way. Let&amp;rsquo;s take a look.&lt;/p&gt;

&lt;h2 id=&#34;deep&#34;&gt;  CNN or Deep Learning? &lt;/h2&gt;

&lt;p&gt;
You would be forgiven for thinking that Deep Learning (DL) takes its name from the number of layers in a neural network, but the root is a little more complex than that. In fact, some powerful neural networks, even CNNs, only consist of a few layers. The &#39;deep&#39; in DL acknowledges that each layer of the network has &#39;layers&#39; of its own. More on this later.
&lt;/p&gt;&lt;p&gt;
Often you may see a conflation of CNNs with DL, but the concept of DL comes some time before CNNs were first introduced. Connecting multiple neural networks together, altering the directionality of their weights and stacking such machines all gave rise to the increasing power and popularity of DL.
&lt;/p&gt;&lt;p&gt;
We won&#39;t delve too deeply into history or mathematics in this tutorial, but if you want to know the timeline of DL in more detail, I&#39;d suggest the paper &#34;On the Origin of Deep Learning&#34; (Wang and Raj 2016) available &lt;a href=&#34;https://t.co/aAw4rEpZEt&#34; title=&#34;On the Origin of Deep Learning&#34;&gt;here&lt;/a&gt;. It&#39;s a lengthy read - 72 pages including references - but shows the logic between progressive steps in DL.
&lt;/p&gt;&lt;p&gt;
As with the study of neural networks, the inspiration for CNNs came from nature: specifically, the visual cortex. It drew upon the idea that the neurons in the visual cortex focus upon different sized patches of an image getting different levels of information in different layers. If a computer could be programmed to work in this way, it may be able to mimic the image-recognition power of the brain. So how can this be done?
&lt;/p&gt;

&lt;p&gt;A CNN takes as input an array, or image (2D or 3D, grayscale or colour) and tries to learn the relationship between this image and some target data e.g. a classification. By &amp;lsquo;learn&amp;rsquo; we are still talking about weights just like in a regular neural network. The difference in CNNs is that these weights connect small subsections of the input to each of the different neurons in the first layer. Fundamentally, there are multiple neurons in a single layer that each have their own weights to the same subsection of the input. These different sets of weights are called &amp;lsquo;kernels&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s important at this stage to make sure we understand this weight or kernel business, because it&amp;rsquo;s the whole point of the &amp;lsquo;convolution&amp;rsquo; bit of the CNN.&lt;/p&gt;

&lt;h2 id=&#34;kernels&#34;&gt; Convolution and Kernels &lt;/h2&gt;

&lt;p&gt;Convolution is something that should be taught in schools along with addition, and multiplication - it&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution&#34; title=&#34;Convolution on Wiki&#34;&gt;just another mathematical operation&lt;/a&gt;. Perhaps the reason it&amp;rsquo;s not, is because it&amp;rsquo;s a little more difficult to visualise.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we have a pattern or a stamp that we want to repeat at regular intervals on a sheet of paper, a very convenient way to do this is to perform a convolution of the pattern with a regular grid on the paper. Think about hovering the stamp (or kernel) above the paper and moving it along a grid before pushing it into the page at each interval.&lt;/p&gt;

&lt;p&gt;This idea of wanting to repeat a pattern (kernel) across some domain comes up a lot in the realm of signal processing and computer vision. In fact, if you&amp;rsquo;ve ever used a graphics package such as Photoshop, Inkscape or GIMP, you&amp;rsquo;ll have seen many kernels before. The list of &amp;lsquo;filters&amp;rsquo; such as &amp;lsquo;blur&amp;rsquo;, &amp;lsquo;sharpen&amp;rsquo; and &amp;lsquo;edge-detection&amp;rsquo; are all done with a convolution of a kernel or filter with the image that you&amp;rsquo;re looking at.&lt;/p&gt;

&lt;p&gt;For example, let&amp;rsquo;s find the outline (edges) of the image &amp;lsquo;A&amp;rsquo;.&lt;/p&gt;

&lt;div style=&#34;text-align:center; display:inline-block; width:100%; margin:auto;&#34;&gt;
&lt;img title=&#34;Android&#34; src=&#34;/img/CNN/android.png&#34;&gt;&lt;br&gt;
&lt;b&gt;A&lt;/b&gt;
&lt;/div&gt;

&lt;p&gt;We can use a kernel, or set of weights, like the ones below.&lt;/p&gt;

&lt;div style=&#34;width:100%; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:49%; margin:auto;min-width:155px;&#34;&gt;
&lt;img title=&#34;Horizontal Filter&#34; height=150 src=&#34;/img/CNN/horizFilter.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Finds horizontals&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; min-width:150px;display:inline-block; width:49%;margin:auto;&#34;&gt;
&lt;img title=&#34;Vertical Filter&#34; height=150 src=&#34;/img/CNN/vertFilter.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Finds verticals&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;A kernel is placed in the top-left corner of the image. The pixel values covered by the kernel are multiplied with the corresponing kernel values and the products are summated. The result is placed in the new image at the point corresponding the centre of the kernel. An example for this first step is shown in the diagram below. This takes the vertical Sobel filter (used for edge-detection) and applies it to the pixels of the image.&lt;/p&gt;

&lt;div style=&#34;text-align:center; display:inline-block; width:100%;margin:auto;&#34;&gt;
&lt;img title=&#34;Conv Example&#34; height=&#34;350&#34; src=&#34;/img/CNN/convExample.png&#34;&gt;&lt;br&gt;
&lt;b&gt;A step in the Convolution Process.&lt;/b&gt;
&lt;/div&gt;

&lt;p&gt;The kernel is moved over by one pixel and this process is repated until all of the possible locations in the image are filtered as below, this time for the horizontal Sobel filter. Notice that there is a border of empty values around the convolved image. This is because the result of convolution is placed at the centre of the kernel. To deal with this, a process called &amp;lsquo;padding&amp;rsquo; or more commonly &amp;lsquo;zero-padding&amp;rsquo; is used. This simply means that a border of zeros is places around the original image to make it a pixel wider all around. The convolution is then done as normal, but the convolution result will now produce an image that is of equal size to the original.&lt;/p&gt;

&lt;div style=&#34;width:100%;margin:auto; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:45%;min-width:455px;margin:auto;&#34;&gt;
&lt;img title=&#34;Sobel Conv Gif&#34; height=&#34;450&#34; src=&#34;/img/CNN/convSobel.gif&#34;&gt;&lt;br&gt;
&lt;b&gt;The kernel is moved over the image performing the convolution as it goes.&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; display:inline-block; width:45%;min-width:450px;margin:auto;&#34;&gt;
&lt;img title=&#34;Zero Padding Conv&#34; height=&#34;450&#34; src=&#34;/img/CNN/convZeros.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Zero-padding is used so that the resulting image doesn&#39;t shrink.&lt;/b&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have our convolved image, we can use a colourmap to visualise the result. Here, I&amp;rsquo;ve just normalised the values between 0 and 255 so that I can apply a grayscale visualisation:&lt;/p&gt;

&lt;div style=&#34;text-align:center; display:inline-block; width:100%;margin:auto;&#34;&gt;
&lt;img title=&#34;Conv Result&#34; height=&#34;150&#34;src=&#34;/img/CNN/convResult.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Result of the convolution&lt;/b&gt;
&lt;/div&gt;

&lt;p&gt;This dummy example could represent the very bottom left edge of the Android&amp;rsquo;s head and doesn&amp;rsquo;t really look like it&amp;rsquo;s detected anything. To see the proper effect, we need to scale this up so that we&amp;rsquo;re not looking at individual pixels. Performing the horizontal and vertical sobel filtering on the full 264 x 264 image gives:&lt;/p&gt;

&lt;div style=&#34;width:100%;margin:auto; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block; min-width:100px;margin:auto;&#34;&gt;
&lt;img title=&#34;Horizontal Sobel&#34; src=&#34;/img/CNN/horiz.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Horizontal Sobel&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; display:inline-block; margin:auto;min-width:100px&#34;&gt;
&lt;img title=&#34;Vertical Sobel&#34; src=&#34;/img/CNN/vert.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Vertical Sobel&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; display:inline-block;margin:auto;min-width:100px&#34;&gt;
&lt;img title=&#34;Full Sobel&#34; src=&#34;/img/CNN/both.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Combined Sobel&lt;/b&gt;
&lt;/div&gt;  
&lt;/div&gt;

&lt;p&gt;Where we&amp;rsquo;ve also added together the result from both filters to get both the horizontal and vertical ones.&lt;/p&gt;

&lt;h3 id=&#34;relationship&#34;&gt; How does this feed into CNNs? &lt;/h3&gt;

&lt;p&gt;Clearly, convolution is powerful in finding the features of an image &lt;strong&gt;if&lt;/strong&gt; we already know the right kernel to use. Kernel design is an artform and has been refined over the last few decades to do some pretty amazing things with images (just look at the huge list in your graphics software!). But the important question is, what if we don&amp;rsquo;t know the features we&amp;rsquo;re looking for? Or what if we &lt;strong&gt;do&lt;/strong&gt; know, but we don&amp;rsquo;t know what the kernel should look like?&lt;/p&gt;

&lt;p&gt;Well, first we should recognise that every pixel in an image is a &lt;strong&gt;feature&lt;/strong&gt; and that means it represents an &lt;strong&gt;input node&lt;/strong&gt;. The result from each convolution is placed into the next layer in a &lt;strong&gt;hidden node&lt;/strong&gt;. Each feature or pixel of the convolved image is a node in the hidden layer.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve already said that each of these numbers in the kernel is a weight, and that weight is the connection between the feature of the input image and the node of the hidden layer. The kernel is swept across the image and so there must be as many hidden nodes as there are input nodes (well actually slightly fewer as we should add zero-padding to the input image). This means that the hidden layer is also 2D like the input image. Sometimes, instead of moving the kernel over one pixel at a time, the &lt;strong&gt;stride&lt;/strong&gt;, as it&amp;rsquo;s called, can be increased. This will result in fewer nodes or fewer pixels in the convolved image. Consider it like this:&lt;/p&gt;

&lt;div style=&#34;width:100%;margin:auto; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block;margin:auto;min-width:300px;&#34;&gt;
&lt;img title=&#34;Hidden Layer Nodes&#34; height=300 src=&#34;/img/CNN/hiddenLayer.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Hidden Layer Nodes in a CNN&lt;/b&gt;
&lt;/div&gt;  
&lt;div style=&#34;text-align:center; display:inline-block;margin:auto;min-width:300px&#34;&gt;
&lt;img title=&#34;Hidden Layer after Increased Stride&#34; height=225 src=&#34;/img/CNN/strideHidden.png&#34;&gt;&lt;br&gt;
&lt;b&gt;Increased stride means fewer hidden-layer nodes&lt;/b&gt;
&lt;/div&gt;  
&lt;/div&gt;

&lt;p&gt;These weights that connect to the nodes need to be learned in exactly the same way as in a regular neural network. The image is passed through these nodes (by being convolved with the weights a.k.a the kernel) and the result is compared to some output (the error of which is then backpropagated and optimised).&lt;/p&gt;

&lt;p&gt;In reality, it isn&amp;rsquo;t just the weights or the kernel for one 2D set of nodes that has to be learned, there is a whole array of nodes which all look at the same area of the image (called the &lt;strong&gt;receptive field&lt;/strong&gt;*). Each of the nodes in this row (or &lt;strong&gt;fibre&lt;/strong&gt;) tries to learn different kernels (different weights) that will show up some different features of the image, like edges. So the hidden-layer may look something more like this:&lt;/p&gt;

&lt;p&gt;* &lt;em&gt;Note: we&amp;rsquo;ll talk more about the receptive field after looking at the pooling layer below&lt;/em&gt;&lt;/p&gt;

&lt;div style=&#34;width:100%;margin:auto; text-align:center;&#34;&gt;
&lt;div style=&#34;text-align:center; display:inline-block;margin:auto;min-width:100px&#34;&gt;
&lt;img title=&#34;Multiple Kernel Hidden Layer&#34; height=350 src=&#34;/img/CNN/deepConv.png&#34;&gt;&lt;br&gt;
&lt;b&gt;For a 2D image learning a set of kernels.&lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align:center; display:inline-block;margin:auto;min-width:100px&#34;&gt;
&lt;img title=&#34;3 Channel Image&#34; height=350 src=&#34;/img/CNN/deepConv3.png&#34;&gt;&lt;br&gt;
&lt;b&gt;For a 3 channel RGB image the kernel becomes 3D.&lt;/b&gt; 
&lt;/div&gt;
&lt;/div&gt;  

&lt;p&gt;Now &lt;strong&gt;this&lt;/strong&gt; is why deep learning is called &lt;strong&gt;deep&lt;/strong&gt; learning. Each hidden layer of the convolutional neural network is capable of learning a large number of kernels. The output from this hidden-layer is passed to more layers which are able to learn their own kernels based on the &lt;em&gt;convolved&lt;/em&gt; image output from this layer. This is what gives the CNN the ability to see the edges of an image and build them up into larger features.&lt;/p&gt;

&lt;h2 id=&#34;CNN Architecture&#34;&gt;  CNN Archiecture &lt;/h2&gt;

&lt;p&gt;It is the &lt;em&gt;architecture&lt;/em&gt; of a CNN that gives it its power. In fact, most papers that are puplished these days on CNNs tend to be about a new achitecture i.e. the number and ordering of different layers and how many kernels are learnt. Let&amp;rsquo;s take a look at the other layers in a CNN.&lt;/p&gt;

&lt;h2 id=&#39;layers&#39;&gt; Layers &lt;/h2&gt;

&lt;h3 id=&#34;input&#34;&gt;  Input Layer &lt;/h3&gt;

&lt;p&gt;The input image is placed into this layer. It can be a single-layer 2D image (grayscale), 2D 3-channel image (RGB colour) or 3D. The main different between how the inputs are arranged comes in the formation of the expected kernel shapes. Kernels need to be learned that are the same depth as the input i.e. 5 x 5 x 3 for a 2D RGB image with a receptive field of 5 x 5.&lt;/p&gt;

&lt;p&gt;Input to a CNN seem to work best when they&amp;rsquo;re of certain dimensions. This is because of the behviour of the convolution. Depending on the &lt;em&gt;stride&lt;/em&gt; of the kernel and the subsequent &lt;em&gt;pooling layers&lt;/em&gt; the outputs may become an &amp;ldquo;illegal&amp;rdquo; size including half-pixels. We&amp;rsquo;ll look at this in the &lt;em&gt;pooling layer&lt;/em&gt; section.&lt;/p&gt;

&lt;h3 id=&#34;convolution&#34;&gt;  Convolutional Layer &lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve &lt;a href=&#34;#kernels&#34; title=&#34;Convolution and Kernels&#34;&gt;already looked at what the conv layer does&lt;/a&gt;. Just remember that it takes in an image e.g. [56 x 56 x 3] and assuming a stride of 1 and zero-padding, will produce an output of [56 x 56 x 32] if 32 kernels are being learnt. It&amp;rsquo;s important to note that the order of these dimensions can be important during the implementation of a CNN in Python. This is because there&amp;rsquo;s alot of matrix multiplication going on!&lt;/p&gt;

&lt;h3 id=&#34;nonlinear&#34;&gt; Non-linearity&lt;/h3&gt;

&lt;p&gt;The &amp;lsquo;non-linearity&amp;rsquo; here isn&amp;rsquo;t its own distinct layer of the CNN, but comes as part of the convolution layer as it is done in the neurons (just like a normal NN). By this, we mean &amp;ldquo;don&amp;rsquo;t take the data forwards as it is (linearity) let&amp;rsquo;s do something to it (non-linearlity) that will help us later on&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In our neural network tutorials we looked at different &lt;a href=&#34;/post/transfer-functions&#34; title=&#34;Transfer Functions&#34;&gt;activation functions&lt;/a&gt;. These each provide a different mapping of the input to an output, either to [-1 1], [0 1] or some other domain e.g the Rectified Linear Unit thresholds the data at 0: max(0,x). The &lt;em&gt;ReLU&lt;/em&gt; is very popular as it doesn&amp;rsquo;t require any expensive computation and it&amp;rsquo;s been &lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&#34; title=&#34;Krizhevsky et al 2012&#34;&gt;shown to speed up the convergence of stochastic gradient descent algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;pool&#34;&gt;  Pooling Layer &lt;/h3&gt;

&lt;p&gt;The pooling layer is key to making sure that the subsequent layers of the CNN are able to pick up larger-scale detail than just edges and curves. It does this by merging pixel regions in the convolved image together (shrinking the image) before attempting to learn kernels on it. Effectlively, this stage tages another kernel, say [2 x 2] and passes it over the entire image, just like in convolution. It is common to have the stride and kernal size equal i.e. a [2 x 2] kernel has a stride of 2. This example will &lt;em&gt;half&lt;/em&gt; the size of the convolved image. The number of feature-maps produced by the learned kernels will remain the same as &lt;strong&gt;pooling&lt;/strong&gt; is done on each one in turn. Thus the pooling layer returns an array with the same depth as the convolution layer. The figure below shows the principal.&lt;/p&gt;

&lt;div style=&#34;text-align:center; display:inline-block; width:100%;margin:auto;&#34;&gt;
&lt;img title=&#34;Pooling&#34; height=350 src=&#34;/img/CNN/poolfig.gif&#34;&gt;&lt;br&gt;
&lt;b&gt;Max-pooling: Pooling using a &#34;max&#34; filter with stride equal to the kernel size&lt;/b&gt;
&lt;/div&gt;  

&lt;h3 id=&#34;receptiveField&#34;&gt; A Note on the Receptive Field &lt;/h3&gt;

&lt;p&gt;This is quite an important, but sometimes neglected, concept. We said that the receptive field of a single neuron is the area of the image which is can &amp;lsquo;see&amp;rsquo;. Each neuron has a different receptive field. While this is true, the full impact of it can only be understood when we see what happens after pooling.&lt;/p&gt;

&lt;p&gt;For an image of size [12 x 12], say the receptive field (kernel size) in the first conv layer is [3 x 3]. The output of the conv layer (assuming padding and stride of 1) is going to be the [12 x 12 x 10] if we&amp;rsquo;re learning 10 kernels. After pooling with a [3 x 3] kernel, we get an output of [4 x 4 x 10]. This gets fed into the next conv layer. Suppose the kernel in the second conv layer is [2 x 2], would we say that the receptive field here is also [2 x 2]? Well, yes&amp;hellip; but actually, no. In fact, a neuron in this layer is not just seeing the [2 x 2] area of the &lt;em&gt;convolved&lt;/em&gt; image, it is actually seeing a [4 x 4] area of the &lt;em&gt;original&lt;/em&gt; image. That&amp;rsquo;s the [3 x 3] of the first layer for each of the pixels in the &amp;lsquo;receptive field&amp;rsquo; of the second layer (remembering we had a stride of 1 in the first layer). Continuing this through the rest of the network, it is possible to end up with a final layer with a recpetive field equal to the size of the original image. Understanding this gives us the real insight to how the CNN works, building up the image as it goes.&lt;/p&gt;

&lt;h3 id=&#34;dense&#34;&gt;  Fully-connected (Dense) Layer&lt;/h3&gt;

&lt;p&gt;So this layer took me a while to figure out. If I take all of the say [3 x 3 x 64] featuremaps of my final pooling layer I have 3 x 3 x 64 = 576 different weights to consider and update. What I need is to make sure that my training labels match with the outputs from my output layer. We may only have 10 possibilities in our output layer (say the digits 0 - 9 ). Thus we want the final numbers in our output layer to be [? x 10] where the ? represents the number of nodes in the layer before&amp;hellip; this one, the FC layer. If there was only 1 node in this layer, it would have 576 weights attached to it - one for each of the weights coming from the previous pooling layer. This is not very useful as it won&amp;rsquo;t allow us to learn any combinations of these low-dimensional outputs. Increasing the number of neurons to say 1,000+ will allow the FC layer to provide many different combinations of features and learn a potentially non-linear function that represents the feature space. The larger the FC layer, the more complex the function could be (though it may not need to be very big). Sometimes it&amp;rsquo;s seen that there are two FC layers together, this just increases the possibility of learning a complex function. FC layers are 1D vectors.&lt;/p&gt;

&lt;h4 id = &#34;fcConv&#34;&gt; Fully-connected as a Convolutional Layer &lt;/h4&gt;

&lt;p&gt;If the idea above doesn&amp;rsquo;t help you (as it doesn&amp;rsquo;t for me) lets remove the FC layer and replace it with another convolutional layer. This is very simple - take the output from the pooling layer as before and apply a convolution to it with a kernel that is the same size as a featuremap in the pooling layer. For this to be of use, the input to the conv should be down to around [5 x 5] or [3 x 3] by making sure there have been enough pooling layers in the network. What does this achieve? By convolving a [3 x 3] image with a [3 x 3] kernel we get a 1 pixel output. There is no striding, just one convolution per featuremap. So out output from this layer will be a [1 x k] vector where &lt;em&gt;k&lt;/em&gt; is the number of featuremaps. This is very similar to the FC layer, except that the output from the conv is only created from an individual featuremap rather than being connected to all of the featuremaps. This can be powerfulll though as we have represented a very large receptive field by a single pixel. We&amp;rsquo;re able to say, if the value of the output it high, that all of the featuremaps visible to this output have activated enough to represent a &amp;lsquo;cat&amp;rsquo; or whatever it is we are training our network to learn.&lt;/p&gt;

&lt;p&gt;Often, the kernel used in the final layer of a CNN incorporates some average pooling. Considering this as not just an average of the current featuremaps, but all previous featuremaps, then the output will be high if the previous layers were active within the receptive field of the final layer.&lt;/p&gt;

&lt;h3 id=&#34;dropout&#34;&gt; Dropout Layer &lt;/h3&gt;

&lt;p&gt;The previously mentioned fully-connected layer is connected to all weights in the previous layer - this can be a very large number. As such, an FC layer is prone to &lt;em&gt;overfitting&lt;/em&gt; meaning that the network won&amp;rsquo;t generalise well to new data. There are a number of techniques that can be used to reduce overfitting though the most commonly seen in CNNs is the dropout layer. As the name suggests, this causes the network to &amp;lsquo;drop&amp;rsquo; some nodes on each iteration with a particular probability. The &lt;em&gt;keep probability&lt;/em&gt; is between 0 and 1, most commonly around 0.2-0.5 it seems. This is the probability that a particular node is dropped during training. When back propagation occurs, the weights connected to these nodes are not updated. They are readded for the next iteration.&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt; Output Layer &lt;/h3&gt;

&lt;p&gt;Of course depending on the purpose of your CNN, the output layer will be slightly different. In general, the output layer consists of a number of nodes which have a high value if they are &amp;lsquo;true&amp;rsquo; or activated. Consider a classification problem where a CNN is given a set of images containing cats, dogs and elephants. If we&amp;rsquo;re asking the CNN to learn what a cat, dog and elephant looks like, output layer is going to be a set of three nodes, one for each &amp;lsquo;class&amp;rsquo; or animal. We&amp;rsquo;d expect that when the CNN finds an image of a cat, the value at the node representing &amp;lsquo;cat&amp;rsquo; is higher than the other two. This is the same idea as in a regular neural network. In fact, the FC layer and the output layer can be considered as a traditional NN. Some output layers are probabilities and as such will sum to 1, whilst others will just achieve a value which could be a pixel intensity in the range 0-255.&lt;/p&gt;

&lt;h3 id=&#34;backProp&#34;&gt; A Note on Back Propagation &lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve found it helpful to consider CNNs in reverse. It didn&amp;rsquo;t sit properly in my mind that the CNN first learns all different types of edges, curves etc. and then builds them up into large features e.g. a face. It came up in a discussion that we could consider the CNN working in reverse, and in fact this is effectively what happens - back propagation updates the weights from the final layer &lt;em&gt;back&lt;/em&gt; towards the first. In fact, the error (or loss) minimisation occurs firstly at the final layer and as such, this is where the network is &amp;lsquo;seeing&amp;rsquo; the bigger picture. The gradient (updates to the weights) vanishes towards the input layer and is greatest at the output layer. We can effectively think that the CNN is learning &amp;ldquo;face - has eyes, nose mouth&amp;rdquo; at the output layer, then &amp;ldquo;I don&amp;rsquo;t know what a face is, but here are some eyes, noses, mouths&amp;rdquo; in the previous one, then &amp;ldquo;What are eyes? I&amp;rsquo;m only seeing circles, some white bits and a black hole&amp;rdquo; followed by &amp;ldquo;woohoo! round things!&amp;rdquo; and initially by &amp;ldquo;I think that&amp;rsquo;s what a line looks like&amp;rdquo;. Possibly we could think of the CNN as being less sure about life at the first layers and being more advanced at the end.&lt;/p&gt;

&lt;p&gt;CNNs can be used for segmentation, classification, regression and a whole manner of other processes. On the whole, they only differ by four things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;architecture (number and order of conv, pool and fc layers plus the size and number of the kernels)&lt;/li&gt;
&lt;li&gt;output (probabilitstic etc.)&lt;/li&gt;
&lt;li&gt;training method (cost or loss function, regularisation and optimiser)&lt;/li&gt;
&lt;li&gt;hyperparameters (learning rate, regularisation weights, batch size, iterations&amp;hellip;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There may well be other posts which consider these kinds of things in more detail, but for now I hope you have some insight into how CNNs function. Now, lets code it up&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>