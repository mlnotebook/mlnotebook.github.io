<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Machine Learning Notebook</title>
    <link>/tags/machine-learning/index.xml</link>
    <description>Recent content in Machine Learning on Machine Learning Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Generative Adversarial Network (GAN) in TensorFlow - Part 5</title>
      <link>/post/GAN5/</link>
      <pubDate>Tue, 25 Jul 2017 11:07:22 +0100</pubDate>
      
      <guid>/post/GAN5/</guid>
      <description>&lt;p&gt;This is the final part in our series on Generative Adversarial Networks (GAN). We will write our training script and look at how to run the GAN. We will also take a look at the results we get out. Can you tell the difference between the real and generated faces?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt; Introduction &lt;/h2&gt;

&lt;p&gt;In this series we started out with a &lt;a href=&#34;/post/GAN1&#34; title=&#34;GAN - Part 1&#34;&gt;background to GAN&lt;/a&gt; including some of the mathematics behind them. We then downloaded and processed our &lt;a href=&#34;/post/GAN2&#34; title=&#34;GAN - Part 2&#34;&gt;dataset&lt;/a&gt;. In the subsequent posts, we wrote some &lt;a href=&#34;/post/GAN3&#34; title=&#34;GAN - Part 3&#34;&gt;image helper functions&lt;/a&gt; before completing some &lt;a href=&#34;/post/GAN4&#34; title=&#34;GAN - Part 4&#34;&gt;data processing functions&lt;/a&gt; and the &lt;a href=&#34;/post/GAN4&#34; title=&#34;GAN - Part 4&#34;&gt;GAN Class itself&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this final post, we will create the training script and visualise some of the results we get out.&lt;/p&gt;

&lt;h2 id=&#34;script&#34;&gt; Training Script &lt;/h2&gt;

&lt;p&gt;The training script is here: &lt;a href=&#34;/docs/GAN/gantut_trainer.py&#34; title=&#34;gantut_trainer.py&#34;&gt;`gantut_trainer.py&amp;rsquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s only short, so there isn&amp;rsquo;t anything to fill in, but let&amp;rsquo;s take a look. We need to make sure we import the GAN &lt;code&gt;class&lt;/code&gt; from our completed &lt;code&gt;gantut_gan.py&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you&amp;rsquo;re using the files called &lt;code&gt;gantut_*_complete.py&lt;/code&gt; you&amp;rsquo;ll need to modify this line (add the &lt;code&gt;_complete&lt;/code&gt;). Otherwise, just make sure it&amp;rsquo;s looking for the correctly named file where your GAN class is written.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/python

import os
import numpy as  np
import tensorflow as tf

from gantut_gan import DCGAN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;lsquo;shebang&amp;rsquo; on the first line allows us to call this script from the terminal without typing &lt;code&gt;python&lt;/code&gt; first. This is a useful line if you&amp;rsquo;re going to run this network on a cluster of computers where you will probably need to create your own python (or conda) virtual environment first. This line will be changed to point to the specific python installation that you want to use to run the script&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I&amp;rsquo;ll add this note here. The network &lt;em&gt;will&lt;/em&gt; take a long time to train. If you have access to a cluster, I recommend using it.&lt;/p&gt;

&lt;p&gt;Next, we define the possible &amp;lsquo;flags&amp;rsquo; or attributes that we need the network to take:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#DEFINE THE FLAGS FOR RUNNING SCRIPT FROM THE TERMINAL
# ARG1 = NAME OF THE FLAG
# ARG2 = DEFAULT VALUE
# ARG3 = DESCRIPTION
flags = tf.app.flags
flags.DEFINE_integer(&amp;quot;epoch&amp;quot;, 20, &amp;quot;Number of epochs to train [20]&amp;quot;)
flags.DEFINE_float(&amp;quot;learning_rate&amp;quot;, 0.0002, &amp;quot;Learning rate for adam optimiser [0.0002]&amp;quot;)
flags.DEFINE_float(&amp;quot;beta1&amp;quot;, 0.5, &amp;quot;Momentum term for adam optimiser [0.5]&amp;quot;)
flags.DEFINE_integer(&amp;quot;train_size&amp;quot;, np.inf, &amp;quot;The size of training images [np.inf]&amp;quot;)
flags.DEFINE_integer(&amp;quot;batch_size&amp;quot;, 64, &amp;quot;The batch-size (number of images to train at once) [64]&amp;quot;)
flags.DEFINE_integer(&amp;quot;image_size&amp;quot;, 64, &amp;quot;The size of the images [n x n] [64]&amp;quot;)
flags.DEFINE_string(&amp;quot;dataset&amp;quot;, &amp;quot;lfw-aligned-64&amp;quot;, &amp;quot;Dataset directory.&amp;quot;)
flags.DEFINE_string(&amp;quot;checkpoint_dir&amp;quot;, &amp;quot;checkpoint&amp;quot;, &amp;quot;Directory name to save the checkpoints [checkpoint]&amp;quot;)
flags.DEFINE_string(&amp;quot;sample_dir&amp;quot;, &amp;quot;samples&amp;quot;, &amp;quot;Directory name to save the image samples [samples]&amp;quot;)
FLAGS = flags.FLAGS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we&amp;rsquo;re using the &lt;code&gt;tf.flags&lt;/code&gt; module (which is a wrapper for &lt;code&gt;argparse&lt;/code&gt;) that takes arguments that trail the script name in the terminal and turn them into variables we can use in the network. The format for each argument is:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;flags.DEFINE_datatype(name, default_value, description)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;code&gt;datatype&lt;/code&gt; is what is expected (an integer, float, string etc.), &lt;code&gt;name&lt;/code&gt; is what the resulting variable will be called, &lt;code&gt;default_value&lt;/code&gt; is&amp;hellip; the default value in case it&amp;rsquo;s not explicitly defined at runtime, and &lt;code&gt;description&lt;/code&gt; is a useful descriptor of what this argument does. We package all these variables into one (called &lt;code&gt;FLAGS&lt;/code&gt;) that can be called later to assign values.&lt;/p&gt;

&lt;p&gt;Notice that the &lt;code&gt;name&lt;/code&gt; here is the same as those we wrote in the &lt;code&gt;__init__&lt;/code&gt; method of our GAN &lt;code&gt;class&lt;/code&gt; because these will be used to initialise the GAN.&lt;/p&gt;

&lt;p&gt;Our network will need folders to output to and also to check whether there&amp;rsquo;s an existing checkpoint that can be loaded (rather than doing it all over again).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#CREATE SOME FOLDERS FOR THE DATA
if not os.path.exists(FLAGS.checkpoint_dir):
    os.makedirs(FLAGS.checkpoint_dir)
if not os.path.exists(FLAGS.sample_dir):
    os.makedirs(FLAGS.sample_dir)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Even though we&amp;rsquo;ve just defined some variables for our network, there are plenty of others in the Graph that need some default value. TensorFlow has a handy function for that:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# GET ALL OF THE OPTIONS FOR TENSORFLOW RUNTIME 
config = tf.ConfigProto(intra_op_parallelism_threads=8)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: I&amp;rsquo;ve included the &lt;code&gt;intra_op_parallelism_threads&lt;/code&gt; argument to &lt;code&gt;tf.ConfigProto&lt;/code&gt; because TensorFlow has the power to take over as many cores as it can see when it&amp;rsquo;s running. This may not be a problem if you&amp;rsquo;re not using your machine too much, but if you&amp;rsquo;re running on a cluster, TF will ignore the &amp;lsquo;requested&amp;rsquo; number of cpus/gpus and leech into other cores. Setting &lt;code&gt;intra_op_parallelism_threads&lt;/code&gt; to the correct number of threads stops this from happening.&lt;/p&gt;

&lt;p&gt;Finally, we initialise the TensorFlow session (with out &lt;code&gt;config&lt;/code&gt; above), initialise the GAN and pass the flags to the &lt;code&gt;.train&lt;/code&gt; method of the GAN &lt;code&gt;class&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: It is good to initialise the session in this way with &lt;code&gt;with&lt;/code&gt; because it will be automatically closed when the GAN training is finished.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with tf.Session(config=config) as sess:
    #INITIALISE THE GAN BY CREATING A NEW INSTANCE OF THE DCGAN CLASS
    dcgan = DCGAN(sess, image_size=FLAGS.image_size, batch_size=FLAGS.batch_size,
                  is_crop=False, checkpoint_dir=FLAGS.checkpoint_dir)

    #TRAIN THE GAN
    dcgan.train(FLAGS)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training&#34;&gt; Training &lt;/h2&gt;

&lt;p&gt;This is it! 5 posts later and we can train our GAN. From our terminal, we are going to call the training script &lt;code&gt;gantut_trainer.py&lt;/code&gt; and pass it a couple of arguments:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;~/GAN/gantut_trainer.py --dataset ~/GAN/aligned --epoch 20
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, if you&amp;rsquo;ve put your aligned training set somewhere else, make sure that path goes into the &lt;code&gt;--dataset&lt;/code&gt; flag. The other flags can be set to default because that&amp;rsquo;s how we&amp;rsquo;ve written our GAN &lt;code&gt;class&lt;/code&gt;. Now 20 epochs will take a seriously long time (it look me nearly 4 days using 12 cores on a cluster).&lt;/p&gt;

&lt;p&gt;There will be 3 folders of output from the GAN:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;logs&lt;/code&gt; - where the logs from the training will be saved. These can be viewed with TensorBoard&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoints&lt;/code&gt; - where the model itself is saved&lt;/li&gt;
&lt;li&gt;&lt;code&gt;samples&lt;/code&gt; - this is where the image array we created in &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt; will be output to every so often.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;logs&#34;&gt; Logs &lt;/h3&gt;

&lt;p&gt;Whilst the network is training (if you&amp;rsquo;re doing it locally) you can pull up tensorboard and watch how the training is progressing. From the terminal:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tensorboard --logdir=&amp;quot;~/GAN/logs&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Follow the link it spits out and you&amp;rsquo;ll be presented with a lot of information about the network. You will find graphs of the loss-functions under &amp;lsquo;scalars&amp;rsquo;, some examples from the generator under &amp;lsquo;images&amp;rsquo; and the Graph itself is nicely represented under &amp;lsquo;graph&amp;rsquo;. &amp;lsquo;Histograms&amp;rsquo; show how the distributions are changing over time. We can see in these that our noise distribution $p_{z}$ is uniform (which is what we defined) and that the real and fake images take values around &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; at the discriminator, as we also described in &lt;a href=&#34;/post/GAN1&#34; title=&#34;GAN - Part 1&#34;&gt;part 1&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Noise (z) Distribution&#34; width=30% src=&#34;/img/CNN/hist_z_1.png&#34;&gt;
        &lt;img title=&#34;Real Image Discriminator Distribution&#34; width=30% src=&#34;/img/CNN/hist_d.png&#34;&gt;
        &lt;img title=&#34;Fake Image Discriminator Distribution&#34; width=30% src=&#34;/img/CNN/hist_d_.png&#34;&gt;
                        
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: The distributions of (Left to right) the noise vectors $z$ and the real and fake images at the discriminator.
    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;TensorFlow Graph&#34; width=100% src=&#34;/img/CNN/graph.png&#34;&gt;
                        
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The TensorFlow Graph that we build using our GAN `class`.
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&#34;results&#34;&gt; Results &lt;/h3&gt;

&lt;p&gt;Here it is, the output from our GAN (after 14 epochs in this case) showing how well the network has learned how to create faces. It may take longer than expect to load as I&amp;rsquo;ve tried to preserve quality.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;GAN Faces&#34; width=30% src=&#34;/img/CNN/faces_gif.gif&#34;&gt;
                        
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The output of our GAN at the end of each epoch ending at epoch 14. (created at gifmaker.me).
        
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;We can see that some of the faces are still not quite there yet, but there are a few that are unbelieveably realistic. In fact, we can perform a kind of &amp;lsquo;Turing Test&amp;rsquo; on this data. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Turing_test&#34; title=&#34;wiki:Turing Test&#34;&gt;Turing Test&lt;/a&gt;, put simply, is that if a user is unable to &lt;em&gt;reliably&lt;/em&gt; tell the difference between a computer and human performing the same task, then the computer has passed the Turing Test.&lt;/p&gt;

&lt;p&gt;Have a go at the test below: study each face, decide if it is a real or fake image; then click on the image to reveal the true result. If you only guess 50% or less, then the computer has passed this simplistic Turing Test.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;a href=&#34;/docs/GAN/turing_quiz.html&#34; target=&#34;_blank&#34;&gt;Click Here for the Turing Test&lt;/a&gt;&lt;br&gt;(opens in a new window)&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt; Conclusion &lt;/h2&gt;

&lt;p&gt;So it looks great, but what was the point? Well, remember back to &lt;a href=&#34;/post/GAN1&#34; title=&#34;GAN - Post 1&#34;&gt;part 1&lt;/a&gt; - GANs and other generative networks are used for &lt;em&gt;image completion&lt;/em&gt;. We can use the fact that our network has learned what a face should look like to &amp;lsquo;fill-in&amp;rsquo; any missing bits. Lets say someone has a large tattoo across their face, we can reconstruct what the skin would look like without it. Or maybe we have an amazing photo, with a beautiufl background, but we&amp;rsquo;re not smiling: the GAN can reconstruct a smile. More advanced work can include learning what glasses are and putting them onto other faces.&lt;/p&gt;

&lt;p&gt;Again, for credit, this series is based on the main code by &lt;a href=&#34;https://github.com/carpedm20/DCGAN-tensorflow&#34; title=&#34;carpedm20/DCGAN-tensorflow&#34;&gt;carpedm20&lt;/a&gt; and inspired from the blog of &lt;a href=&#34;http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks&#34; title=&#34;bamos.github.io&#34;&gt;B. Amos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;GANs are powerful networks, but work in a relatively simple way by trying to trick a discriminator by generating more and more realistic-looking images.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generative Adversarial Network (GAN) in TensorFlow - Part 4</title>
      <link>/post/GAN4/</link>
      <pubDate>Mon, 17 Jul 2017 09:37:58 +0100</pubDate>
      
      <guid>/post/GAN4/</guid>
      <description>&lt;p&gt;Now that we&amp;rsquo;re able to import images into our network, we really need to build the GAN iteself. This tuorial will build the GAN &lt;code&gt;class&lt;/code&gt; including the methods needed to create the generator and discriminator. We&amp;rsquo;ll also be looking at some of the data functions needed to make this work.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;*Note: This table of contents does not follow the order in the post. The contents is grouped by the methods in the GAN &lt;code&gt;class&lt;/code&gt; and the functions in &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt;.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gan&#34;&gt;The GAN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#datasetfiles&#34;&gt;dataset_files()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dcgan&#34;&gt;GAN Class&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#init&#34;&gt;__init__()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discriminator&#34;&gt;discriminator()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generator&#34;&gt;generator()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#buildmodel&#34;&gt;build_model()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#save&#34;&gt;save()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load&#34;&gt;load()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#train&#34;&gt;train()&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#batchnorm&#34;&gt;Data Functions&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#batchnorm&#34;&gt;batch_norm()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conv2d&#34;&gt;conv2d()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#relu&#34;&gt;relu()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear&#34;&gt;linear()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conv2dtrans&#34;&gt;conv2d_transpose()&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;intro&#34;&gt; Introduction &lt;/h2&gt;

&lt;p&gt;In the last tutorial, we build the functions in &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt;which allow us to import data into our networks. The completed file is &lt;a href=&#34;/docs/GAN/gantut_imgfuncs_complete.py&#34; title=&#34;gantut_imgfuncs_complete.py&#34;&gt;here&lt;/a&gt;. In this tutorial we will be working on the final two code skeletons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_gan.py&#34; title=&#34;gantut_gan.py&#34;&gt;&lt;code&gt;gantut_gan.py&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_datafuncs.py&#34; title=&#34;gantut_datafuncs.py&#34;&gt;&lt;code&gt;gantut_datafuncs.py&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First, let&amp;rsquo;s take a look at the various parts of our GAN in the &lt;code&gt;gantut_gan.py&lt;/code&gt; file and see what they&amp;rsquo;re going to do.&lt;/p&gt;

&lt;h2 id=&#34;gan&#34;&gt; The GAN &lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re going to import a number of modules for this file including those from our own &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; and &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import division
import os
import time
import math
import itertools
from glob import glob
import tensorflow as tf
import numpy as np
from six.moves import xrange

#IMPORT OUR IMAGE AND DATA FUNCTIONS
from gantut_datafuncs import *
from gantut_imgfuncs import *
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;datasetfiles&#34;&gt; dataset_files() &lt;/h3&gt;

&lt;p&gt;The initial part of this file is a little housekeeping - ensuring that we are only dealing with supported filetypes. This way of doing things I liked in &lt;a href=&#34;http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks&#34; title=&#34;B. Amos&#34;&gt;B. Amos blog&lt;/a&gt;. We define accepted file-extensions and then return a list of all of the possible files we can use for training purposes. the &lt;code&gt;itertools.chain.from_iterable&lt;/code&gt; function is useful for create a single &lt;code&gt;list&lt;/code&gt; of all of the files found in the folders and subfolders of a particular &lt;code&gt;root&lt;/code&gt; with an appropriate &lt;code&gt;ext&lt;/code&gt;. Notice that it doesn&amp;rsquo;t really matter what we call the images, so this will work for all datasets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;SUPPORTED_EXTENSIONS = [&amp;quot;png&amp;quot;, &amp;quot;jpg&amp;quot;, &amp;quot;jpeg&amp;quot;]

&amp;quot;&amp;quot;&amp;quot; Returns the list of all SUPPORTED image files in the directory
&amp;quot;&amp;quot;&amp;quot;
def dataset_files(root):
    return list(itertools.chain.from_iterable(
    glob(os.path.join(root, &amp;quot;*.{}&amp;quot;.format(ext))) for ext in SUPPORTED_EXTENSIONS))
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h3 id=&#34;dcgan&#34;&gt; DCGAN() &lt;/h3&gt;

&lt;p&gt;This is where the hard work begins. We&amp;rsquo;re going to build the DCGAN &lt;code&gt;class&lt;/code&gt; (i.e. Deep Convolutional Generative Adversarial Network). The skeleton code already has the necessary method names for our model, let&amp;rsquo;s have a look at what we&amp;rsquo;ve got to create:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__init__&lt;/code&gt;:  &amp;emsp;to initialise the model and set parameters&lt;/li&gt;
&lt;li&gt;&lt;code&gt;build_model&lt;/code&gt;: &amp;emsp;creates the model (or &amp;lsquo;graph&amp;rsquo; in TensorFlow-speak) by calling&amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;generator&lt;/code&gt;: &amp;emsp;defines the generator network&lt;/li&gt;
&lt;li&gt;&lt;code&gt;discriminator&lt;/code&gt;: &amp;emsp;defines the discriminator network&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train&lt;/code&gt;: &amp;emsp;is called to begin the training of the network with data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;save&lt;/code&gt;: &amp;emsp;saves the TensorFlow checkpoints of the GAN&lt;/li&gt;
&lt;li&gt;&lt;code&gt;load&lt;/code&gt;: &amp;emsp;loads the TensorFlow checkpoints of the GAN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We create an instance of our GAN class with &lt;code&gt;DCGAN(args)&lt;/code&gt; and be returned a DCGAN object with the above methods. Let&amp;rsquo;s code.&lt;/p&gt;

&lt;h4 id=&#34;init&#34;&gt; __init__() &lt;/h4&gt;

&lt;p&gt;To initialise our GAN object, we need some initial parameters. It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, sess, image_size=64, is_crop=False, batch_size=64, sample_size=64, z_dim=100,
             gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, checkpoint_dir=None, lam=0.1):
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The parameters are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sess&lt;/code&gt;: &amp;emsp; the TensorFlow session to run in&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image_size&lt;/code&gt;: &amp;emsp; the width of the images, which should be the same as the height as we like square inputs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;is_crop&lt;/code&gt;: &amp;emsp; whether to crop the images or leave them as they are&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: &amp;emsp; number of images to use in each run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sample_size&lt;/code&gt;: &amp;emsp; number of z samples to take on each run, should be equal to batch_size&lt;/li&gt;
&lt;li&gt;&lt;code&gt;z_dim&lt;/code&gt;: &amp;emsp; number of samples to take for each z&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gf_dim&lt;/code&gt;: &amp;emsp; dimension of generator filters in first conv layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df_dim&lt;/code&gt;: &amp;emsp; dimenstion of discriminator filters in first conv layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gfc_dim&lt;/code&gt;: &amp;emsp; dimension of generator units for fully-connected layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dfc_gim&lt;/code&gt;: &amp;emsp; dimension of discriminator units for fully-connected layer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c_dim&lt;/code&gt;: &amp;emsp; number of image cannels (gray=1, RGB=3)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpoint_dir&lt;/code&gt;: &amp;emsp; where to store the TensorFlow checkpoints&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lam&lt;/code&gt;: &amp;emsp;small constant weight for the sum of contextual and perceptual loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are the controllable parameters for the GAN. As this is the initialising function, we need to transfer these inputs to the &lt;code&gt;self&lt;/code&gt; of the class so they are accessible later on. We will also add two new lines:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Let&amp;rsquo;s add a check that the &lt;code&gt;image_size&lt;/code&gt; is a power of 2 (to make the convolution work well). This clever &amp;lsquo;bit-wise-and&amp;rsquo; operator &lt;code&gt;&amp;amp;&lt;/code&gt; will do the job for us. It uses the unique property of all power of 2 numbers have only one bit set to &lt;code&gt;1&lt;/code&gt; and all others to &lt;code&gt;0&lt;/code&gt;. Let&amp;rsquo;s also check that the image is bigger than $[8  \times 8]$ to we don&amp;rsquo;t convolve too far:&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Get the &lt;code&gt;image_shape&lt;/code&gt; which is the width and height of the image along with the number of channels (gray or RBG).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#image_size must be power of 2 and 8+
assert(image_size &amp;amp; (image_size - 1) == 0 and image_size &amp;gt;= 8)

self.sess = sess
self.is_crop = is_crop
self.batch_size = batch_size
self.image_size = image_size
self.sample_size = sample_size
self.image_shape = [image_size, image_size, c_dim]

self.z_dim = z_dim
self.gf_dim = gf_dim
self.df_dim = df_dim        
self.gfc_dim = gfc_dim
self.dfc_dim = dfc_dim

self.lam = lam
self.c_dim = c_dim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Later on, we will want to do &amp;lsquo;batch normalisation&amp;rsquo; on our data to make sure non of our images are extremely different to the others. We will need a batch-norm layer for each of the conv layers in our generator and discriminator. We will initialise the layers here, but define them in our &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; file shortly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#batchnorm (from funcs.py)
self.d_bns = [batch_norm(name=&#39;d_bn{}&#39;.format(i,)) for i in range(4)]

log_size = int(math.log(image_size) / math.log(2))
self.g_bns = [batch_norm(name=&#39;g_bn{}&#39;.format(i,)) for i in range(log_size)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This shows that we will be using 4 layers in our discriminator. But we will need more in our generator: our generator starts with a simple vector &lt;em&gt;z&lt;/em&gt; and needs to upscale to the size of &lt;code&gt;image_size&lt;/code&gt;. It does this by a factor of 2 in each layer, thus $\log(\mathrm{image \ size})/\log(2)$ is equal to the number of upsamplings to be done i.e. $2^{\mathrm{num \ of \ layers}} = 64$ in our case. Also note that we&amp;rsquo;ve created these objects (layers) with an iterator so that each has the name &lt;code&gt;g_bn1&lt;/code&gt;, &lt;code&gt;g_bn1&lt;/code&gt; etc.&lt;/p&gt;

&lt;p&gt;To finish &lt;code&gt;__init__()&lt;/code&gt; we set the checkpoint directory for TensorFlow saves, instruct the class to build the model and name it &amp;lsquo;DCGAN.model&amp;rsquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.checkpoint_dir = checkpoint_dir
self.build_model()

self.model_name=&amp;quot;DCGAN.model&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;batchnorm&#34;&gt; batch_norm() &lt;/h4&gt;

&lt;p&gt;This is the first of our &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; functions.&lt;/p&gt;

&lt;p&gt;If some of our images are very different to the others then the network will not learn the features correctly. To avoid this, we add batch normalisation (as described in &lt;a href=&#34;http://arxiv.org/abs/1502.03167&#34; title=&#34;Batch Normalization: Sergey Ioffe, Christian Szegedy&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Ioffe &amp;amp; Szegedy (2015)&lt;/a&gt;. We effectively redistribute the intensities of the images around a common mean with a set variance.&lt;/p&gt;

&lt;p&gt;This is a &lt;code&gt;class&lt;/code&gt; that will be instantiated with set parameters when called. Then, the method will perform batch normalisation whenever the object is called on the set of images &lt;code&gt;x&lt;/code&gt;. We are using Tensorflow&amp;rsquo;s built-in &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm&#34; title=&#34;tf.contrib.layers.batch_norm&#34;&gt;tf.contrib.layers.batch_norm()&lt;/a&gt; layer for this which implements the method from the paper above.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Parameters&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;epsilon&lt;/code&gt;:    &amp;lsquo;small float added to variance [of the input data] to avoid division by 0&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;momentum&lt;/code&gt;:   &amp;lsquo;decay value for the moving average, usually 0.999, 0.99, 0.9&amp;rsquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;:      the set of input images to be normalised&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train&lt;/code&gt;:  whether or not the network is in training mode [True or False]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A batch_norm &amp;lsquo;object&amp;rsquo; on instantiation&lt;/li&gt;
&lt;li&gt;A tensor representing the output of the batch_norm operation&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Batch normalisation function to standardise the input
Initialises an object with all of the batch norm properties
When called, performs batch norm on input &#39;x&#39;
&amp;quot;&amp;quot;&amp;quot;
class batch_norm(object):
    def __init__(self, epsilon=1e-5, momentum = 0.9, name=&amp;quot;batch_norm&amp;quot;):
        with tf.variable_scope(name):
            self.epsilon = epsilon
            self.momentum = momentum

            self.name = name

    def __call__(self, x, train):
        return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon,
                                            center=True, scale=True, is_training=train, scope=self.name)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;discriminator&#34;&gt; discriminator() &lt;/h4&gt;

&lt;p&gt;As the discriminator is a simple &lt;a href=&#34;/post/CNN1&#34; title=&#34;MLNotebook: Convolutional Neural Network&#34;&gt;convolutional neural network (CNN)&lt;/a&gt; this will not take many lines. We will have to create a couple of wrapper functions that will perform the actual convolutions, but let&amp;rsquo;s get the method written in &lt;code&gt;gantut_gan.py&lt;/code&gt; first.&lt;/p&gt;

&lt;p&gt;We want our discriminator to check a real &lt;code&gt;image&lt;/code&gt;, save varaibles and then use the same variables to check a fake &lt;code&gt;image&lt;/code&gt;. This way, if the images are fake, but fool the discriminator, we know we&amp;rsquo;re on the right track. Thus we use the variable &lt;code&gt;reuse&lt;/code&gt; when calling the &lt;code&gt;discriminator()&lt;/code&gt; method - we will set it to &lt;code&gt;True&lt;/code&gt; when we&amp;rsquo;re using the fake images.&lt;/p&gt;

&lt;p&gt;We add &lt;code&gt;tf.variable_scope()&lt;/code&gt; to our functions so that when we visualise our graph in TensorBoard we can recognise the various pieces of our GAN.&lt;/p&gt;

&lt;p&gt;Next are the definitions of the 4 layers of our discriminator. each one takes in the images, the kernel (filter) dimensions and has a name to identify it later on. Notice that we also call our &lt;code&gt;d_bns&lt;/code&gt; objects which are the batch-norm objects that were set-up during instantiation of the GAN. These act on the result of the convolution before being passed through the non-linear &lt;code&gt;lrelu&lt;/code&gt; function. The last layer is just a &lt;code&gt;linear&lt;/code&gt; layer that outputs the unbounded results from the network.&lt;/p&gt;

&lt;p&gt;As this is a classificaiton task (real or fake) we finish by returning the probabilities in the range $[0 \ 1]$ by applying the sigmoid function. The full output is also returned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def discriminator(self, image, reuse=False):
	with tf.variable_scope(&amp;quot;discriminator&amp;quot;) as scope:
	    if reuse:
		scope.reuse_variables()
	   	    
	    h0 = lrelu(conv2d(image, self.df_dim, name=&#39;d_h00_conv&#39;))
	    h1 = lrelu(self.d_bns[0](conv2d(h0, self.df_dim*2, name=&#39;d_h1_conv&#39;), self.is_training))
	    h2 = lrelu(self.d_bns[1](conv2d(h1, self.df_dim*4, name=&#39;d_h2_conv&#39;), self.is_training))
	    h3 = lrelu(self.d_bns[2](conv2d(h2, self.df_dim*8, name=&#39;d_h3_conv&#39;), self.is_training))
	    h4 = linear(tf.reshape(h3, [-1, 8192]), 1, &#39;d_h4_lin&#39;)
	    
	    return tf.nn.sigmoid(h4), h4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This method calls a couple of functions that we haven&amp;rsquo;t defined yet: &lt;code&gt;cov2d&lt;/code&gt;, &lt;code&gt;lrelu&lt;/code&gt; and &lt;code&gt;linear&lt;/code&gt; so lets do those now.&lt;/p&gt;

&lt;hr&gt;

&lt;h4 id=&#34;conv2d&#34;&gt; conv2d() &lt;/h4&gt;

&lt;p&gt;This function we&amp;rsquo;ve seen before in our &lt;a href=&#34;/post/CNN1&#34; title=&#34;MLNotebook: Convolutional Neural Networks&#34;&gt;CNN&lt;/a&gt; tutorial. We&amp;rsquo;ve defined the weights &lt;code&gt;w&lt;/code&gt; for each kernel which is &lt;code&gt;[k_h x k_w x number of images x number of kernels]&lt;/code&gt;not forgetting that different weights are learned for different images. We&amp;rsquo;ve initialised these weights using a standard, random sampling from a normal distribution with standard deviation &lt;code&gt;stddev&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The convolution is done by TensorFlow&amp;rsquo;s [tf.nn.conv2d]( &amp;ldquo;tf.nn.conv2d&amp;rdquo;) function using the weights &lt;code&gt;w&lt;/code&gt; we&amp;rsquo;ve already defined. The padding option &lt;code&gt;SAME&lt;/code&gt; makes sure that we end up with output that is the same size as the input. Biases are added (the same size as the number of kernels and initialised at a constant value) before the result is returned.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;input_&lt;/code&gt;:     the input images (full batch)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output_dim&lt;/code&gt;: the number of kernels/filters to be learned&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k_h&lt;/code&gt;, &lt;code&gt;k_w&lt;/code&gt;:   height and width of the kernels to be learned&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d_h&lt;/code&gt;, &lt;code&gt;d_w&lt;/code&gt;:   stride of the kernel horizontally and vertically&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stddev&lt;/code&gt;:     standard deviation for the normal func in weight-initialiser&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the convolved images for each kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Defines how to perform the convolution for the discriminator,
i.e. traditional conv rather than reverse conv for the generator
&amp;quot;&amp;quot;&amp;quot;
def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&amp;quot;conv2d&amp;quot;):
    with tf.variable_scope(name):
        w = tf.get_variable(&#39;w&#39;, [k_h, k_w, input_.get_shape()[-1], output_dim],
                            initializer=tf.truncated_normal_initializer(stddev=stddev))
        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=&#39;SAME&#39;)

        biases = tf.get_variable(&#39;biases&#39;, [output_dim], initializer=tf.constant_initializer(0.0))
        # conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())
        conv = tf.nn.bias_add(conv, biases)

        return conv 
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;relu&#34;&gt; relu() &lt;/h4&gt;

&lt;p&gt;The network need to be able to learn complex functions, so we add some non-linearity to the output of our convolution layers. We&amp;rsquo;ve seen this before in our tutorial on &lt;a href=&#34;/post/transfer_functions&#34; title=&#34;Transfer Functions&#34;&gt;transfer functions&lt;/a&gt;. Here we use the leaky rectified linear unit (lReLU).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Parameters&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;leak&lt;/code&gt;:   the &amp;lsquo;leakiness&amp;rsquo; of the lrelu&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: some data with a wide range&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the transformed input data&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Neural nets need this non-linearity to build complex functions
&amp;quot;&amp;quot;&amp;quot;
def lrelu(x, leak=0.2, name=&amp;quot;lrelu&amp;quot;):
    with tf.variable_scope(name):
        f1 = 0.5 * (1 + leak)
        f2 = 0.5 * (1 - leak)
        return f1 * x + f2 * abs(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;linear&#34;&gt; linear() &lt;/h4&gt;

&lt;p&gt;This linear layer takes the outputs from the convolution and does a linear transform using some randomly initialised weights. This does not have the same non-linear property as the &lt;code&gt;lrelu&lt;/code&gt; function because we will use this output to calcluate probabilities for classification. We return the result of &lt;code&gt;input_ x matrix&lt;/code&gt; by default, but if we also need the weights, we also output &lt;code&gt;matrix&lt;/code&gt; and &lt;code&gt;bias&lt;/code&gt; through the &lt;code&gt;if&lt;/code&gt; statement.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Parameters&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;stddev&lt;/code&gt;:     standard deviation for weight initialiser&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bias_start&lt;/code&gt;: for the bias initialiser (constant value)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;with_w&lt;/code&gt;:     return the weight matrix (and biases) as well as the output if True&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;input_&lt;/code&gt;:         input data (shape is used to define weight/bias matrices)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output_size&lt;/code&gt;:    desired output size of the linear layer&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;For the final layer of the discriminator network to get the
full detail (probabilities etc.) from the output
&amp;quot;&amp;quot;&amp;quot;
def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):
    shape = input_.get_shape().as_list()

    with tf.variable_scope(scope or &amp;quot;Linear&amp;quot;):
        matrix = tf.get_variable(&amp;quot;Matrix&amp;quot;, [shape[1], output_size], tf.float32,
                                 tf.random_normal_initializer(stddev=stddev))
        bias = tf.get_variable(&amp;quot;bias&amp;quot;, [output_size],
            initializer=tf.constant_initializer(bias_start))
        if with_w:
            return tf.matmul(input_, matrix) + bias, matrix, bias
        else:
            return tf.matmul(input_, matrix) + bias
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;generator&#34;&gt; generator() &lt;/h4&gt;

&lt;p&gt;Finally! We&amp;rsquo;re going to write the code for the generative part of the GAN. This method will take a single input - the randomly-sampled vector $z$ from the well known distribution $p_z$.&lt;/p&gt;

&lt;p&gt;Remember that the generator is effectively a reverse discriminator in that it is a CNN that works backwards. Thus we start with the &amp;lsquo;values&amp;rsquo; and must perform the linear transformation on them before feeding them through the other layers of the network. As we do not know the weights or biases yet in this network, we need to make sure we output these from the linear layer with &lt;code&gt;with_w=True&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This first hidden layer &lt;code&gt;hs[0]&lt;/code&gt; needs reshaping to be the small image-shaped array that we can send through the network to become the upscaled $[64 \times 64]$ image at the end. So we take the linearly-transformed z-values and reshape to $[4 x 4 x num_kernels]$. Don&amp;rsquo;t forget the &lt;code&gt;-1&lt;/code&gt; to do this for all images in the batch. As before, we must batch-norm the result and pass it through the non-linearity.&lt;/p&gt;

&lt;p&gt;The number of layers in this network has been calculated earlier (using the logarithm ratio of image size to downsampling factor. We can therefore do the next part of the generator in a loop.&lt;/p&gt;

&lt;p&gt;In each loop/layer we are going to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;give the layer a name&lt;/li&gt;
&lt;li&gt;perform the &lt;em&gt;inverse&lt;/em&gt; convolution&lt;/li&gt;
&lt;li&gt;apply non-linearity&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1 and 3 are self-explanatory, but the inverse convolution function still needs to be written. This is the function that will take in the small square image and upsample it to a larger image using some weights that are being learnt. We start at layer &lt;code&gt;i=1&lt;/code&gt; where we want the image to go to &lt;code&gt;size=8&lt;/code&gt; from &lt;code&gt;size=4&lt;/code&gt; at layer &lt;code&gt;i=0&lt;/code&gt;. This will increase by a factor of 2 at each layer. As with a regular CNN we want to learn fewer kernels on the larger images, so we need to decrease the &lt;code&gt;depth_mul&lt;/code&gt; by a factor of 2 at each layer. Note that the &lt;code&gt;while&lt;/code&gt; loop will terminate when the size gets to the size of the input images &lt;code&gt;image_size&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The final layer is added which takes the last output and does the inverse convolution to get the final fake image (that will be tested with the discriminator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generator(self, z):
	with tf.variable_scope(&amp;quot;generator&amp;quot;) as scope:
	    self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, &#39;g_h0_lin&#39;, with_w=True)

	    hs = [None]
	    hs[0] = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])
	    hs[0] = tf.nn.relu(self.g_bns[0](hs[0], self.is_training))
	    
	    i=1             #iteration number
	    depth_mul = 8   #depth decreases as spatial component increases
	    size=8          #size increases as depth decreases
	    
	    while size &amp;lt; self.image_size:
		hs.append(None)
		name=&#39;g_h{}&#39;.format(i)
		hs[i], _, _ = conv2d_transpose(hs[i-1], [self.batch_size, size, size, self.gf_dim*depth_mul],
		                                name=name, with_w=True)
		hs[i] = tf.nn.relu(self.g_bns[i](hs[i], self.is_training))
		
		i += 1
		depth_mul //= 2
		size *= 2
		
	    hs.append(None)
	    name = &#39;g_h{}&#39;.format(i)
	    hs[i], _, _ = conv2d_transpose(hs[i-1], [self.batch_size, size, size, 3], name=name, with_w=True)
	    
	    return tf.nn.tanh(hs[i])           
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;conv2dtrans&#34;&gt; conv2d_transpose() &lt;/h4&gt;

&lt;p&gt;The inverse convolution function looks very similar to the forward convolution function. We&amp;rsquo;ve had to make sure that different versions of TensorFlow work here - in newer versions, the correct function is located at &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose&#34; title=&#34;tf.nn.conv2d_transpose&#34;&gt;tf.nn.conv2d_transpose&lt;/a&gt; where as in older ones we must use &lt;code&gt;tf.nn.deconv2d&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;input_&lt;/code&gt;:         a vector (of noise) with dim=batch_size x z_dim&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output_shape&lt;/code&gt;:   the final shape of the generated image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k_h&lt;/code&gt;, &lt;code&gt;k_w&lt;/code&gt;:       the height and width of the kernels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d_h&lt;/code&gt;, &lt;code&gt;d_w&lt;/code&gt;:       the stride of the kernel horiz and vert.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;an image (upscaled from the initial data)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Deconv isn&#39;t an accurate word, but is a handy shortener,
so we&#39;ll use that. This is for the generator that has to make
the image from some randomly sampled data
&amp;quot;&amp;quot;&amp;quot;
def conv2d_transpose(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,
                     name=&amp;quot;conv2d_transpose&amp;quot;, with_w=False):
    with tf.variable_scope(name):
        w = tf.get_variable(&#39;w&#39;, [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],
                            initializer=tf.random_normal_initializer(stddev=stddev))

        try:
            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        # Support for verisons of TensorFlow before 0.7.0
        except AttributeError:
            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,
                                strides=[1, d_h, d_w, 1])

        biases = tf.get_variable(&#39;biases&#39;, [output_shape[-1]], initializer=tf.constant_initializer(0.0))
        # deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())
        deconv = tf.nn.bias_add(deconv, biases)

        if with_w:
            return deconv, w, biases
        else:
            return deconv    
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;buildmodel&#34;&gt; build_model() &lt;/h4&gt;

&lt;p&gt;The &lt;code&gt;build_model()&lt;/code&gt; method bring together the image data and the generator and discriminator methods. This is the &amp;lsquo;graph&amp;rsquo; for TensorFlow to follow. It contains some &lt;code&gt;tf.placeholder&lt;/code&gt; pieces which we must supply attributes to when we finally train the model.&lt;/p&gt;

&lt;p&gt;We will need to know whether the model is in training or inference mode throughout our code, so we have a placeholder for that variable. We also need a placeholder for the image data itself because there will be a different batch of data being injected at each epoch. These are our &lt;code&gt;real_images&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When we inject the &lt;code&gt;z&lt;/code&gt; vectors into the GAN (served by another palceholder) we will also produce some monitoring output for TensorBoard. By adding &lt;code&gt;tf.summary.histogram()&lt;/code&gt; we are able to keep track of how the different &lt;code&gt;z&lt;/code&gt; vectors look at each epoch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def build_model(self):
        self.is_training = tf.placeholder(tf.bool, name=&#39;is_training&#39;)
        self.images = tf.placeholder(
            tf.float32, [None] + self.image_shape, name=&#39;real_images&#39;)
        self.lowres_images = tf.reduce_mean(tf.reshape(self.images,
            [self.batch_size, self.lowres_size, self.lowres,
             self.lowres_size, self.lowres, self.c_dim]), [2, 4])
        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=&#39;z&#39;)
        self.z_sum = tf.summary.histogram(&amp;quot;z&amp;quot;, self.z)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, lets tell the graph to take the injected &lt;code&gt;z&lt;/code&gt; vector an turn it into an image with our &lt;code&gt;generator&lt;/code&gt;. We&amp;rsquo;ll also produce a lowres version of this image. Now, put the &amp;lsquo;real_images&amp;rsquo; into the &lt;code&gt;discriminator&lt;/code&gt;, which gives back our probabilities and the final-layer data (the logits). We then &lt;code&gt;reuse&lt;/code&gt; the same discriminator parameters to test the fake image from the generator. Here we also output some histograms of the probabilities of the &amp;lsquo;real_image&amp;rsquo; and the fake image. We will also output the current fake image from the generator to TensorBoard.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;        self.G = self.generator(self.z)
        self.lowres_G = tf.reduce_mean(tf.reshape(self.G,
            [self.batch_size, self.lowres_size, self.lowres,
             self.lowres_size, self.lowres, self.c_dim]), [2, 4])
        self.D, self.D_logits = self.discriminator(self.images)

        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)

        self.d_sum = tf.summary.histogram(&amp;quot;d&amp;quot;, self.D)
        self.d__sum = tf.summary.histogram(&amp;quot;d_&amp;quot;, self.D_)
        self.G_sum = tf.summary.image(&amp;quot;G&amp;quot;, self.G)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now for some of the necessary calculations needed to be able to update the network. Let&amp;rsquo;s find the &amp;lsquo;loss&amp;rsquo; on the current outputs. We will utilise a very efficient loss function here the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits&#34; title=&#34;tf.nn.sigmoid_cross_entropy_with_logits&#34;&gt;tf.nn.sigmoid_cross_entropy_with_logits&lt;/a&gt;. We want to calculate a few things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;how well did the discriminator do at letting &lt;em&gt;true&lt;/em&gt; images through (i.e. comparing &lt;code&gt;D&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;how often was the discriminator fooled by the generator  (i.e. comparing &lt;code&gt;D_&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;how often did the generator fail at making realistic images (i.e. comparing &lt;code&gt;D_&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&amp;rsquo;ll add the discriminator losses up (1 + 2) and create a TensorBoard summary statistic (a &lt;code&gt;scalar&lt;/code&gt; value) for the discriminator and generator losses in this epoch. These are what we will optimise during training.&lt;/p&gt;

&lt;p&gt;To keep everything tidy, we&amp;rsquo;ll group the discriminator and generator variables into &lt;code&gt;d_vars&lt;/code&gt; and &lt;code&gt;g_vars&lt;/code&gt; respectively.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;        self.d_loss_real = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,
                                                    labels=tf.ones_like(self.D)))
        self.d_loss_fake = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                                    labels=tf.zeros_like(self.D_)))
        self.g_loss = tf.reduce_mean(
            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                                    labels=tf.ones_like(self.D_)))

        self.d_loss_real_sum = tf.summary.scalar(&amp;quot;d_loss_real&amp;quot;, self.d_loss_real)
        self.d_loss_fake_sum = tf.summary.scalar(&amp;quot;d_loss_fake&amp;quot;, self.d_loss_fake)

        self.d_loss = self.d_loss_real + self.d_loss_fake

        self.g_loss_sum = tf.summary.scalar(&amp;quot;g_loss&amp;quot;, self.g_loss)
        self.d_loss_sum = tf.summary.scalar(&amp;quot;d_loss&amp;quot;, self.d_loss)

        t_vars = tf.trainable_variables()

        self.d_vars = [var for var in t_vars if &#39;d_&#39; in var.name]
        self.g_vars = [var for var in t_vars if &#39;g_&#39; in var.name]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We don&amp;rsquo;t want t lose our progress, so lets make sure we setup the &lt;code&gt;tf.Saver()&lt;/code&gt; function just keeping the most recent variables each time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;        self.saver = tf.train.Saver(max_to_keep=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;save&#34;&gt; save() &lt;/h4&gt;

&lt;p&gt;When we want to save a checkpoint (i.e. save all of the weights we&amp;rsquo;ve learned) we will call this function. It will check whether the output directory exists, if not it will create it. Then it wll call the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/Saver#save&#34; title=&#34;tf.train.Saver.save&#34;&gt;&lt;code&gt;tf.train.Saver.save()&lt;/code&gt;&lt;/a&gt; function which takes in the current session &lt;code&gt;sess&lt;/code&gt;, the save directory, model name and keeps track of the number of steps that&amp;rsquo;ve been done.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def save(self, checkpoint_dir, step):
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
            
        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name), global_step=step)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;load&#34;&gt; load() &lt;/h4&gt;

&lt;p&gt;Equally, if we&amp;rsquo;ve already spent a long time learning weights, we don&amp;rsquo;t want to start from scratch every time we want to push the network further. This function will load the most recent checkpoint in the save directory. TensorFlow has build-in functions for checking out the most recent checkpoint. If there is no checkpoint available, the function returns false and the appropriate action is taken by the main method that called it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def load(self, checkpoint_dir):
        print(&amp;quot; [*] Reading checkpoints...&amp;quot;)
        
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            self.saver.restore(self.sess, ckpt.model_checkpoint_path)
            return True
        else:
            return False
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;train&#34;&gt; train() &lt;/h4&gt;

&lt;p&gt;The all-important &lt;code&gt;train()&lt;/code&gt; method. This is where the magic happens. When we call &lt;code&gt;DCGAN.train(config)&lt;/code&gt; the networks will begin their fight and train. We will discuss the &lt;code&gt;config&lt;/code&gt; argument later on, but succinctly: it&amp;rsquo;s a list of all hyperparameters TensorFlow will use in the network. Here&amp;rsquo;s how &lt;code&gt;train()&lt;/code&gt; works:&lt;/p&gt;

&lt;p&gt;First we give the trainer the data (using our &lt;code&gt;dataset_files&lt;/code&gt; function) and make sure that it&amp;rsquo;s randomly shuffled. We want to make sure that the images next to each other have nothing in common so that we can truly randomly sample them. There&amp;rsquo;s also a check here `&lt;code&gt;assert(len(data) &amp;gt; 0)&lt;/code&gt; to make sure that we don&amp;rsquo;t pass in an empty directory&amp;hellip; that wouln&amp;rsquo;t be useful to learn from.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train(self, config):
	data = dataset_files(config.dataset)
	np.random.shuffle(data)
	assert(len(data) &amp;gt; 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re going to use the adaptive non-convex optimization method &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer&#34; title=&#34;tf.train.AdamOptimizer&#34;&gt;&lt;code&gt;tf.train.AdamOptimizer()&lt;/code&gt;&lt;/a&gt; from &lt;a href=&#34;https://arxiv.org/pdf/1412.6980.pdf&#34; title=&#34;Adam: A Method for Stochastic Optimization&#34;&gt;Kingma &lt;em&gt;et al&lt;/em&gt; (2014)&lt;/a&gt; to train out networks. Let&amp;rsquo;s set this up for the discriminator (&lt;code&gt;d_optim&lt;/code&gt;) and the generator (&lt;code&gt;g_optim&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.d_loss, var_list=self.d_vars)
	g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1).minimize(self.g_loss, var_list=self.g_vars)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we will initialize all variables in the network (depending on TensorFlow version) and generate some &lt;code&gt;tf.summary&lt;/code&gt; variables for TensorBoard which group together all of the summaries that we want to keep track of.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	try:
	    tf.global_variables_initializer().run()
	except:
	    tf.initialize_all_variables().run()
	    
	self.g_sum = tf.summary.merge([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])
	self.d_sum = tf.summary.merge([self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])
	self.writer = tf.summary.FileWriter(&amp;quot;./logs&amp;quot;, self.sess.graph)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So here&amp;rsquo;s the part where we now sample this well-known distribution $p_z$ to get the noise vector $z$. We&amp;rsquo;re using a &lt;code&gt;np.random.uniform&lt;/code&gt; distribution. Keep a look out for this when we&amp;rsquo;re watching the network in TensorBoard, we told the GAN &lt;code&gt;class&lt;/code&gt; to output the histogram of $z$ vectors that are sampled from $p_z$. So they should all approximate to a uniform distribution.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re also going to sample the input &lt;em&gt;real&lt;/em&gt; image files we shuffled earlier taking &lt;code&gt;sample_size&lt;/code&gt; images through to the training process. We will use these later on to assess the loss functions every now and again when we output some examples.&lt;/p&gt;

&lt;p&gt;We need to load in the data using the function &lt;code&gt;get_image()&lt;/code&gt; that we wrote into &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt; during the &lt;a href=&#34;/post/GAN3&#34; title=&#34;MLNotebook: GAN3&#34;&gt;last tutorial&lt;/a&gt;. After loading the images, lets make sure that they&amp;rsquo;re all in one &lt;code&gt;np.array&lt;/code&gt; ready to be used.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	sample_z = np.random.uniform(-1, 1, size=(self.sample_size, self.z_dim))

	sample_files = data[0:self.sample_size]
	sample = [get_image(sample_file, self.image_size, is_crop=self.is_crop) for sample_file in sample_files]
	sample_images = np.array(sample).astype(np.float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set the epoch counter and get the start time (it can be frustrating if we can&amp;rsquo;t see how long things are taking). We also want to be sure to load any previous checkpoints from TensorFlow before we start again from scratch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	counter = 1
	start_time = time.time()

	if self.load(self.checkpoint_dir):
	    print(&amp;quot;&amp;quot;&amp;quot; An existing model was found - delete the directory or specify a new one with --checkpoint_dir &amp;quot;&amp;quot;&amp;quot;)
	else:
	    print(&amp;quot;&amp;quot;&amp;quot; No model found - initializing a new one&amp;quot;&amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s the actual training bit taking place.  &lt;code&gt;For&lt;/code&gt; each &lt;code&gt;epoch&lt;/code&gt; that we&amp;rsquo;ve assigned in &lt;code&gt;config&lt;/code&gt;, we create two minibatches: a sampling of real images, and those generated from the $z$ vector. We then update the &lt;code&gt;discriminator&lt;/code&gt; network before updating the &lt;code&gt;generator&lt;/code&gt;. We also write these loss values to the TensorBoard summary. There are two things to notice:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;By calling &lt;code&gt;sess.run()&lt;/code&gt; with specified variables in the first (or &lt;code&gt;fetch&lt;/code&gt; attribute) we are able to keep the generator steady whilst updating the discriminator, and vice versa.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The generator is updated twice. This is to make sure that the discriminator loss function does not just converge to zero very quickly.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	for epoch in xrange(config.epoch):
	    data = dataset_files(config.dataset)
	    batch_idxs = min(len(data), config.train_size) // self.batch_size
	    
	    for idx in xrange(0, batch_idxs):
		batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]
		batch = [get_image(batch_file, self.image_size, is_crop=self.is_crop) for batch_file in batch_files]
		batch_images = np.array(batch).astype(np.float32)
		
		batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]).astype(np.float32)
		
		#update D network
		_, summary_str = self.sess.run([d_optim, self.d_sum],
		                               feed_dict={self.images: batch_images, self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)
		
		#update G network
		_, summary_str = self.sess.run([g_optim, self.g_sum],
		                               feed_dict={self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)
		
		#run g_optim twice to make sure that d_loss does not go to zero
		_, summary_str = self.sess.run([g_optim, self.g_sum],
		                               feed_dict={self.z: batch_z, self.is_training: True})
		self.writer.add_summary(summary_str, counter)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get the errors needed for backpropagation, we evaluate &lt;code&gt;d_loss_fake&lt;/code&gt;, &lt;code&gt;d_loss_real&lt;/code&gt; and &lt;code&gt;g_loss&lt;/code&gt;. We run the $z$ vector through the graph to get the fake loss and the generator loss, and use the real &lt;code&gt;batch_images&lt;/code&gt; for the real loss.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;		errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.is_training: False})
		errD_real = self.d_loss_real.eval({self.images: batch_images, self.is_training: False})
		errG = self.g_loss.eval({self.z: batch_z, self.is_training: False})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s get some output to &lt;code&gt;stdout&lt;/code&gt; for the user. The current epoch and progress through the minibatches is output at each new minibatch. Every 100 minibatches we&amp;rsquo;re going to evaluate the current generator &lt;code&gt;self.G&lt;/code&gt; and calculate the loss against the small set of images we sampled earlier. We will output the result of the generator and use our &lt;code&gt;save_images()&lt;/code&gt; function to create that image array we worked on in the last tutorial.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;		counter += 1
		print(&amp;quot;Epoch [{:2d}] [{:4d}/{:4d}] time: {:4.4f}, d_loss: {:.8f}&amp;quot;.format(
		        epoch, idx, batch_idxs, time.time() - start_time, errD_fake + errD_real, errG))
		
		if np.mod(counter, 100) == 1:
		    samples, d_loss, g_loss = self.sess.run([self.G, self.d_loss, self.g_loss], 
		                                            feed_dict={self.z: sample_z, self.images: sample_images, self.is_training: False})
		    save_images(samples, [8,8], &#39;./samples/train_{:02d}-{:04d}.png&#39;.format(epoch, idx))
		    print(&amp;quot;[Sample] d_loss: {:.8f}, g_loss: {:.8f}&amp;quot;.format(d_loss, g_loss))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we need to save the current weights from our networks.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;		if np.mod(counter, 500) == 2:
		    self.save(config.checkpoint_dir, counter)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion&#34;&gt; Conclusion &lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s it! We&amp;rsquo;ve completed the &lt;code&gt;gantut_gan.py&lt;/code&gt; and &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; files. Checkout the completed files below:&lt;/p&gt;

&lt;p&gt;Completed versions of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_trainer.py&#34; title=&#34;gantut_trainer.py&#34;&gt;gantut_trainer.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_imgfuncs_complete.py&#34; title=&#34;gantut_imgfuncs_complete.py&#34;&gt;gantut_imgfuncs_complete.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_datafuncs_complete.py&#34; title=&#34;gantut_datafuncs_complete.py&#34;&gt;gantut_datafuncs_complete.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_gan_complete.py&#34; title=&#34;gantut_gan_complete.py&#34;&gt;gantut_gan_complete.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By following this tutorial series we should now have:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A background in how GANs work&lt;/li&gt;
&lt;li&gt;Necessary data, fullly pre-processed and ready to use&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt; for loading data into the neworks&lt;/li&gt;
&lt;li&gt;A GAN &lt;code&gt;class&lt;/code&gt; with the necessary methods in &lt;code&gt;gantut_gan.py&lt;/code&gt; and the &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; we need to do the computations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the final part of the series, we will run this network and take a look at the outputs in TensorBoard.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generative Adversarial Network (GAN) in TensorFlow - Part 3</title>
      <link>/post/GAN3/</link>
      <pubDate>Thu, 13 Jul 2017 09:16:32 +0100</pubDate>
      
      <guid>/post/GAN3/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re ready to code! In &lt;a href=&#34;/content/post/GAN1 &amp;quot;GAN Tutorial - Part 1&#34;&gt;Part 1&lt;/a&gt; we looked at how GANs work and &lt;a href=&#34;/content/post/GAN2 &amp;quot;GAN Tutorial - Part 2&#34;&gt;Part 2&lt;/a&gt; showed how to get the data ready. In this Part, we will begin creating the functions that handle the image data including some pre-procesing and data normalisation.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#intro&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagefuncs&#34;&gt;Image Functions&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#importfuncs&#34;&gt;Importing Functions&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#imread&#34;&gt;imread()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transform&#34;&gt;transform()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centercrop&#34;&gt;center_crop()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getimage&#34;&gt;get_image()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#savingfuncs&#34;&gt;Saving Functions&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#invtransform&#34;&gt;inverse_transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#merge&#34;&gt;merge()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imsave&#34;&gt;imsave()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saveimages&#34;&gt;save_images()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;intro&#34;&gt; Introduction &lt;/h2&gt; 

&lt;p&gt;In the &lt;a href=&#34;/content/post/GAN2 &amp;quot;GAN Tutorial - Part 2&#34;&gt;previous post&lt;/a&gt; we downloaded and pre-processed our training data. There were also links to the skeleton code we will be using in the remainder of the tutorial, here they are again:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_imgfuncs.py&#34; title=&#34;gantut_imgfuncs.py&#34;&gt;&lt;code&gt;gantut_imgfuncs.py&lt;/code&gt;&lt;/a&gt;: holds the image-related functions&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_datafuncs.py&#34; title=&#34;gantut_datafuncs.py&#34;&gt;&lt;code&gt;gantut_datafuncs.py&lt;/code&gt;&lt;/a&gt;: contains the data-related functions&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_gan.py&#34; title=&#34;gantut_gan.py&#34;&gt;&lt;code&gt;gantut_gan.py&lt;/code&gt;&lt;/a&gt;: is where we define the GAN &lt;code&gt;class&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_trainer.py&#34; title=&#34;gantut_trainer.py&#34;&gt;&lt;code&gt;gantut_trainer.py&lt;/code&gt;&lt;/a&gt;: is the script that we will call in order to train the GAN&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Again, the code is based from other sources, particularly the respository by &lt;a href=&#34;https://github.com/carpedm20/DCGAN-tensorflow&#34; title=&#34;carpedm20/DCGAN-tensorflow&#34;&gt;carpedm20&lt;/a&gt; and &lt;a href=&#34;http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks&#34; title=&#34;bamos.github.io&#34;&gt;B. Amos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now, if your folder structure that looks something like this then we&amp;rsquo;re ready to go:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;~/GAN
  |- raw
    |-- 00001.jpg
    |-- ...
  |- aligned
    |-- 00001.jpg
    |-- ...
  |- gantut_imgfuncs.py
  |- gantut_datafuncs.py
  |- gantut_gan.py
  |- gantut_trainer.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;imagefuncs&#34;&gt; Image Functions &lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re going to want to be able to read-in a set of images. We will also want to be able to output some generated images. We will also add in a fail-safe cropping/transformation procedure in-case we want to make sure we have the right input format. The skeleton code &lt;code&gt;gantut_imgfuncs.py&lt;/code&gt; contains the definition headers for these functions, we will fill them in as we go along.&lt;/p&gt;

&lt;h3 id=&#34;importfuncs&#34;&gt; Importing Functions &lt;/h3&gt;

&lt;p&gt;These are the functions needed to get the data from the hard-disk into our network. They are called like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;get_image&lt;/code&gt; which calls&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imread&lt;/code&gt; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;transform&lt;/code&gt; which calls&lt;/li&gt;
&lt;li&gt;&lt;code&gt;center_crop&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;imread&#34;&gt; imread() &lt;/h4&gt;

&lt;p&gt;We are dealing with standard image files and our GAN will support &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt; and &lt;code&gt;.png&lt;/code&gt; as input. For these kind of files, Python already has well-developed tools: specifically we can use the &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.imread.html&#34; title=&#34;imread documentation&#34;&gt;scipy.misc.imread&lt;/a&gt; function from the &lt;code&gt;scipy.misc&lt;/code&gt; library. This is a one-liner and is already written in the skeleton code.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;path&lt;/code&gt;: location of the image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; Reads in the image (part of get_image function)
&amp;quot;&amp;quot;&amp;quot;
def imread(path):
    return scipy.misc.imread(path, mode=&#39;RGB&#39;).astype(np.float)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;transform&#34;&gt; transform() &lt;/h4&gt;
[to top][100]

This function we will have to write into the skeleton. We are including this to make sure that the image data are all of the same dimensions. So this function will need to take in the image, the desired width (the output will be square) and whether to perform the cropping or not. We may have already cropped our images (as we have) because we&#39;ve done some registration/alignment etc.

We do a check on whether we want to crop the image, if we do then call the `center_crop` function, other wise, just take the `image` as it is.

Before returning our cropped (or uncropped) image, we are going to perform normalisation. Currently the pixels have intensity values in the range $[0 \ 255]$ for each channel (reg, green, blue). It is best not to have this kind of skew on our data, so we will normalise our images to have intensity values in the range $[-1 \ 1]$ by dividing by the mean of the maximum range (127.5) and subtracting 1. i.e. image/127.5 - 1. 

We will define the cropping function next, but note that the returned image is a simply a `numpy` array.

*Inputs*

* `image`:      the image data to be transformed
* `npx`:        the size of the transformed image [`npx` x `npx`]
* `is_crop`:    whether to preform cropping too [`True` or `False`]

*Returns*

* the cropped, normalised image

```python
&#34;&#34;&#34; Transforms the image by cropping and resizing and 
normalises intensity values between -1 and 1
&#34;&#34;&#34;
def transform(image, npx=64, is_crop=True):
    if is_crop:
        cropped_image = center_crop(image, npx)
    else:
        cropped_image = image
    return np.array(cropped_image)/127.5 - 1.
```

&lt;hr&gt;

&lt;h4 id=&#34;centercrop&#34;&gt; center_crop() &lt;/h4&gt;

&lt;p&gt;Lets perform the cropping of the images (if requested). Usually we deal with square images, say $[64 \times 64]$. We can add a quick option to change that with short &lt;code&gt;if&lt;/code&gt; statements looking at the &lt;code&gt;crop_w&lt;/code&gt; argument to this function. We take the current height and width (&lt;code&gt;h&lt;/code&gt; and &lt;code&gt;w&lt;/code&gt;) from the &lt;code&gt;shape&lt;/code&gt; of the image &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To find the location of the centre of the image around which to take the square crop, we take half the result of &lt;code&gt;h - crop_h&lt;/code&gt; and &lt;code&gt;w - crop_w&lt;/code&gt;, making sure to round both to get a definite pixel value. However, it&amp;rsquo;s not guaranteed (depending on the image dimensions) that we will end up with a nice $[64 \times 64]$ image. Let&amp;rsquo;s fix that at the end.&lt;/p&gt;

&lt;p&gt;As before, &lt;code&gt;scipy&lt;/code&gt; has some efficient functions that we may as well use. &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imresize.html&#34; title=&#34;imresize documentation&#34;&gt;&lt;code&gt;scipy.misc.imresize&lt;/code&gt;&lt;/a&gt; takes in an image array and the desired size and outputs a resized image. We can give it our array, which may not be a nice square image due to the initial image dimensions, and &lt;code&gt;imresize&lt;/code&gt; will perform interpolation (bilinear by default) to make sure we get a nice square image at the end.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;:      the input image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;crop_h&lt;/code&gt;: the height of the crop region&lt;/li&gt;
&lt;li&gt;&lt;code&gt;crop_w&lt;/code&gt;: if None crop width = crop height&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resize_w&lt;/code&gt;: the width of the resized image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the cropped image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; Crops the input image at the centre pixel
&amp;quot;&amp;quot;&amp;quot;
def center_crop(x, crop_h, crop_w=None, resize_w=64):
    if crop_w is None:
        crop_w = crop_h
    h, w = x.shape[:2]
    j = int(round((h - crop_h)/2.))
    i = int(round((w - crop_w)/2.))
    return scipy.misc.imresize(x[j:j+crop_h, i:i+crop_w],
                               [resize_w, resize_w])
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;getimage&#34;&gt; get_image() &lt;/h4&gt;

&lt;p&gt;The &lt;code&gt;get_image&lt;/code&gt; function is a wrapper that will call the &lt;code&gt;imread&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; functions. It is the function that we&amp;rsquo;ll call to get the data rather than doing two separate function calls in the main GAN &lt;code&gt;class&lt;/code&gt;. This is a one-liner and is already written in the skeleton code.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Parameters&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;is_crop&lt;/code&gt;:    whether to crop the image or not [True or False]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;image_path&lt;/code&gt;: location of the image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image_size&lt;/code&gt;: width (in pixels) of the output image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the cropped image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; Loads the image and crops it to &#39;image_size&#39;
&amp;quot;&amp;quot;&amp;quot;
def get_image(image_path, image_size, is_crop=True):
    return transform(imread(image_path), image_size, is_crop)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h3 id=&#34;savingfuncs&#34;&gt; Saving Functions &lt;/h3&gt;

&lt;p&gt;When we&amp;rsquo;re training our network, we will want to see some of the results. The previous functions all deal with getting images from storage &lt;em&gt;into&lt;/em&gt; the networks. We now want to take some images &lt;em&gt;out&lt;/em&gt;. The functions are called like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;save_images&lt;/code&gt; which calls&lt;/li&gt;
&lt;li&gt;&lt;code&gt;inverse_transform&lt;/code&gt; and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imsave&lt;/code&gt; which calls&lt;/li&gt;
&lt;li&gt;&lt;code&gt;merge&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;invtransform&#34;&gt; inverse_transform() &lt;/h4&gt;

&lt;p&gt;Firstly, let&amp;rsquo;s put the intensities back into the skewed range, we&amp;rsquo;ll just go from $[-1 \ 1]$ to $[0 \ 1]$ here.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;images&lt;/code&gt;:     the image to be transformed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the transformed image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; This turns the intensities back to a normal range
&amp;quot;&amp;quot;&amp;quot;
def inverse_transform(images):
    return (images+1.)/2.
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;merge&#34;&gt; merge() &lt;/h4&gt;

&lt;p&gt;We will create an array of several example images from the network which we can output every now and again to see how things are progressing. We need some &lt;code&gt;images&lt;/code&gt; to go in and a &lt;code&gt;size&lt;/code&gt; which will say how many images in width and height the array should be.&lt;/p&gt;

&lt;p&gt;First get the height &lt;code&gt;h&lt;/code&gt; and width &lt;code&gt;w&lt;/code&gt; of the &lt;code&gt;images&lt;/code&gt; from their &lt;code&gt;shape&lt;/code&gt; (we assume they&amp;rsquo;re all the same size becuase we will have already used our previous functions to make this happen). &lt;strong&gt;Note&lt;/strong&gt; that &lt;code&gt;images&lt;/code&gt; is a collection of images where each &lt;code&gt;image&lt;/code&gt; has the same &lt;code&gt;h&lt;/code&gt; and &lt;code&gt;w&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We define &lt;code&gt;img&lt;/code&gt; to be the final image array and initialise it to all zeros. Notice that there is a &amp;lsquo;3&amp;rsquo; on the end to denote the number of channels as these are RGB images. This will still work for grayscale images.&lt;/p&gt;

&lt;p&gt;Next we will iterate through each &lt;code&gt;image&lt;/code&gt; in &lt;code&gt;images&lt;/code&gt; and put it into place. The &lt;code&gt;%&lt;/code&gt; operator is the modulo which returns the remainder of the division between two numbers. &lt;code&gt;//&lt;/code&gt; is the floor division operator which returns the integer result of division rounded down. So this will move along the top row of the array (remembering Python indexing starts at 0) and move down placing the image at each iteration.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;images&lt;/code&gt;:     the set of input images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size&lt;/code&gt;:       [height, width] of the array&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;an array of images as a single image&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; Takes a set of &#39;images&#39; and creates an array from them.
&amp;quot;&amp;quot;&amp;quot; 
def merge(images, size):
    h, w = images.shape[1], images.shape[2]
    img = np.zeros((int(h * size[0]), int(w * size[1]), 3))
    for idx, image in enumerate(images):
        i = idx % size[1]
        j = idx // size[1]
        img[j*h:j*h+h, i*w:i*w+w, :] = image
        
    return img
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;imsave&#34;&gt; imsave() &lt;/h4&gt;

&lt;p&gt;Our image array &lt;code&gt;img&lt;/code&gt; now has intensity values in $[0 \ 1]$ lets make this the proper image range $[0 \ 255]$ before getting the integer values as an image array with &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imsave.html&#34; title=&#34;imsave documentation&#34;&gt;&lt;code&gt;scipy.misc.imsave&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;images&lt;/code&gt;: the set of input images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size&lt;/code&gt;:   [height, width] of the array&lt;/li&gt;
&lt;li&gt;&lt;code&gt;path&lt;/code&gt;:   the save location&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Returns&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;an image saved to disk&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; Takes a set of `images` and calls the merge function. Converts
the array to image data and saves to disk.
&amp;quot;&amp;quot;&amp;quot;
def imsave(images, size, path):
    img = merge(images, size)
    return scipy.misc.imsave(path, (255*img).astype(np.uint8))
&lt;/code&gt;&lt;/pre&gt;

&lt;hr&gt;

&lt;h4 id=&#34;saveimages&#34;&gt; save_images() &lt;/h4&gt;

&lt;p&gt;Finally, let&amp;rsquo;s create the wrapper to pull this together:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inputs&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;images&lt;/code&gt;: the images to be saves&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size&lt;/code&gt;: the size of the img array [width height]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image_path&lt;/code&gt;: where the array is to be stored on disk&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot; takes an image and saves it to disk. Redistributes
intensity values [-1 1] from [0 255]
&amp;quot;&amp;quot;&amp;quot;
def save_images(images, size, image_path):
    return imsave(inverse_transform(images), size, image_path)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;conclusion&#34;&gt; Conclusion &lt;/h3&gt;

&lt;p&gt;In this post, we&amp;rsquo;ve dealt with all of the functions that are needed to import image data into our network and also some that will create outputs so we can see what&amp;rsquo;s going on. We&amp;rsquo;ve made sure that we can import any image-size  and it will be dealt with correctly.&lt;/p&gt;

&lt;p&gt;Make sure that we&amp;rsquo;ve imported &lt;code&gt;scpipy.misc&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; to this script:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import scipy.misc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The complete script can be found &lt;a href=&#34;/docs/GAN/gantut_imgfuncs_complete.py&#34; title=&#34;gantut_imgfuncs_complete.py&#34;&gt;here&lt;/a&gt;. In the next post, we will be working on the GAN itself and building the &lt;code&gt;gantut_datafuncs.py&lt;/code&gt; functions as we go.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generative Adversarial Network (GAN) in TensorFlow - Part 2</title>
      <link>/post/GAN2/</link>
      <pubDate>Wed, 12 Jul 2017 11:59:45 +0100</pubDate>
      
      <guid>/post/GAN2/</guid>
      <description>&lt;p&gt;This tutorial will provide the data that we will use when training our Generative Adversarial Networks. It will also take an overview on the structure of the necessary code for creating a GAN and provide some skeleton code which we can work on in the next post. If you&amp;rsquo;re not up to speed on GANs, please do read the brief introduction in &lt;a href=&#34;/post/GAN1&#34; title=&#34;GAN Part 1 - Some Background and Mathematics&#34;&gt;Part 1&lt;/a&gt; of this series on Generative Adversarial Networks.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;intro&#34;&gt; Introduction &lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve looked at &lt;a href=&#34;/post/GAN1&#34; title=&#34;GAN Part 1 - Some Background and Mathematics&#34;&gt;how a GAN works&lt;/a&gt;  and how it is trained, but how do we implement this in Python? There are several stages to this task:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create some initial functions that will read in our training data&lt;/li&gt;
&lt;li&gt;Create some functions that will perform the steps in the CNN&lt;/li&gt;
&lt;li&gt;Write a &lt;code&gt;class&lt;/code&gt; that will hold our GAN and all of its important methods&lt;/li&gt;
&lt;li&gt;Put these together in a script that we can run to train the GAN&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The way I&amp;rsquo;d like to go through this process (in the next post) is by taking the network piece by piece as it would be called by the program. I think this is important to help to understand the flow of the data through the network. The code that I&amp;rsquo;ve used for the basis of these tutorials is from &lt;a href=&#34;https://github.com/carpedm20/DCGAN-tensorflow&#34; title=&#34;carpedm20/DCGAN-tensorflow&#34;&gt;carpedm20&amp;rsquo;s DCGAN-tensorflow repository&lt;/a&gt;, with a lot of influence from other sources including &lt;a href=&#34;http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks&#34; title=&#34;bamos.github.io&#34;&gt;this blog from B. Amos&lt;/a&gt;. I&amp;rsquo;m hoping that by  putting this together in several posts, and fleshing out the code, it will become clearer.&lt;/p&gt;

&lt;h2 id=&#34;skeletons&#34;&gt; Skeleton Code &lt;/h2&gt;

&lt;p&gt;We will structure our code into 4 separate &lt;code&gt;.py&lt;/code&gt; files. Each file represents one of the 4 stages set out above:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_imgfuncs.py&#34; title=&#34;gantut_imgfuncs.py&#34;&gt;&lt;code&gt;gantut_imgfuncs.py&lt;/code&gt;&lt;/a&gt;: holds the image-related functions&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_datafuncs.py&#34; title=&#34;gantut_datafuncs.py&#34;&gt;&lt;code&gt;gantut_datafuncs.py&lt;/code&gt;&lt;/a&gt;: contains the data-related functions&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_gan.py&#34; title=&#34;gantut_gan.py&#34;&gt;&lt;code&gt;gantut_gan.py&lt;/code&gt;&lt;/a&gt;: is where we define the GAN &lt;code&gt;class&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/docs/GAN/gantut_trainer.py&#34; title=&#34;gantut_trainer.py&#34;&gt;&lt;code&gt;gantut_trainer.py&lt;/code&gt;&lt;/a&gt;: is the script that we will call in order to train the GAN&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For our project, let&amp;rsquo;s use the working directory &lt;code&gt;~/GAN&lt;/code&gt;. Download these skeletons using the links above into `~/GAN&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;If you look through each of these files, you will see that they contain only a comment for each function/class and the line defining each function/method. Each of these will have to be completed when we go through the next couple of posts. In the remainder of this post, we will take a look at the dataset that we will be using and prepare the images.&lt;/p&gt;

&lt;h2 id=&#34;dataset&#34;&gt; Dataset&lt;/h2&gt;

&lt;p&gt;We clearly need to have some training data to hand to be able to make this work. Several posts have used databases of faces or even the MNIST digit-classification dataset. In our tutorial, we will be using faces - I find this very interesting as it allows the computer to create photo-realistic images of people that don&amp;rsquo;t actually exist!&lt;/p&gt;

&lt;p&gt;To get the dataset prepared we need to download it, and then pre-process the images so that they will be small enough to use in our GAN.&lt;/p&gt;

&lt;h3 id=&#34;dataset-download&#34;&gt; Download &lt;/h3&gt;

&lt;p&gt;We are going to use the &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; title=&#34;CelebA&#34;&gt;CelebA&lt;/a&gt; databse. Here is a direct link to the GoogleDrive which stores the data: &lt;a href=&#34;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&#34;&gt;https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg&lt;/a&gt;. You will want to go to the &amp;ldquo;img&amp;rdquo; folder and download the &lt;a href=&#34;https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM&#34; title=&#34;img_align_celeba.zip&#34;&gt;&amp;ldquo;img_align_celeba.zip&amp;rdquo;&lt;/a&gt; file. Direct download link should be:&lt;/p&gt;

&lt;div align=&#34;center&#34;&gt;
&lt;a href=&#34;https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM&#34; title=&#34;img_align_celeba.zip&#34;&gt;img_align_celeba.zip (1.3GB)&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Download and extract this folder into &lt;code&gt;~/GAN/raw_images&lt;/code&gt; to find it contains 200,000+ examples of celebrity faces. Even though the &lt;code&gt;.zip&lt;/code&gt; says &amp;lsquo;align&amp;rsquo; in the name, we still need to resize the images and thus may need to realign them too.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;http://mmlab.ie.cuhk.edu.hk/projects/celeba/overview.png&#34; width=&#34;75%&#34; title=&#34;CelebA Database&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: Examples from the CelebA Database. Source: &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; alt=&#34;CelebA&#34;&gt;CelebA&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 is=&#34;dataset-process&#34;&gt; Processing &lt;/h3&gt;

&lt;p&gt;To process this volume of images, we need an automated method for resizing and cropping. We will use &lt;a href=&#34;http://cmusatyalab.github.io/openface/&#34; title=&#34;OpenFace&#34;&gt;OpenFace&lt;/a&gt;. Specifically, there&amp;rsquo;s a small tool we will want to use from this.&lt;/p&gt;

&lt;p&gt;Open a terminal, navigate to or create your working directory (we&amp;rsquo;ll use &lt;code&gt;~/GAN&lt;/code&gt; and follow the instructions below to clone OpenFace and get the Python wrapping sorted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ~/GAN
git clone https://github.com/cmusatyalab/openface.git openface
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cloning complete, move into the &lt;code&gt;openface&lt;/code&gt; folder and install the requirements (handily they&amp;rsquo;re in requirements.txt, so do this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./openface
sudo pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Installation complete (make sure you use sudo to get the permissions to install). Next we want to install the models that we can use with Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./models/get-models.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This make take a short while. When this is done, you may want to update Scipy. This is because the requirements.txt wants a previous version to the most recent. Easily fixed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pip install --upgrade scipy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have access to the Python tool that will do the aligning and cropping of our faces. This is an important step to ensure that all images going into the network are the same dimensions, but also so that the network can learn the faces well (there&amp;rsquo;s no point in having eyes at the bottom of an image, or a face that&amp;rsquo;s half out of the field of view).&lt;/p&gt;

&lt;p&gt;In our working directory `~/GAN&amp;rsquo;, do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./openface/util/align-dlib.py ./raw_images align innerEyesAndBottomLip ./aligned --size 64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will &lt;code&gt;align&lt;/code&gt; all of the &lt;code&gt;innerEyesAndBottomLip&lt;/code&gt; of the images in &lt;code&gt;./raw_images&lt;/code&gt;, crop them to &lt;code&gt;64&lt;/code&gt; x &lt;code&gt;64&lt;/code&gt; and put them in &lt;code&gt;./aligned&lt;/code&gt;. This will take a long time (for 200,000+ images!).&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;/img/CNN/resized_celeba.png&#34; width=&#34;50%&#34; title=&#34;Cropped and Resized CelebA&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: Examples of aligned, cropped and resized images from the &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; alt=&#34;CelebA&#34;&gt;CelebA&lt;/a&gt; database.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;That&amp;rsquo;s it! Now we will have a good training set to use with our network. We also have the skeletons that we can build up to form our GAN. Our next post will look at the functions that will read-in the images for use with the GAN and begin to work on the GAN &lt;code&gt;class&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generative Adversarial Network (GAN) in TensorFlow - Part 1</title>
      <link>/post/GAN1/</link>
      <pubDate>Tue, 11 Jul 2017 09:15:54 +0100</pubDate>
      
      <guid>/post/GAN1/</guid>
      <description>&lt;p&gt;We&amp;rsquo;ve seen that CNNs can learn the content of an image for classification purposes, but what else can they do? This tutorial will look at the Generative Adversarial Network (GAN) which is able to learn from a set of images and create an entirely new &amp;lsquo;fake&amp;rsquo; image which isn&amp;rsquo;t in the training set. Why? By the end of this tutorial you&amp;rsquo;ll get know why this might be done and how to do it.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;intro&#34;&gt;  Introduction &lt;/h2&gt;

&lt;p&gt;Generative Adversarial Networks (GANs) were proposed by Ian Goodfellow &lt;em&gt;et al&lt;/em&gt; in 2014 at annual the Neural Information and Processing Systems (NIPS) conference. The original paper &lt;a href=&#34;https://arxiv.org/pdf/1406.2661&#34; title=&#34;Generative Adversarial Nets 2014&#34;&gt;is available on Arxiv&lt;/a&gt; along with a later tutorial by Goodfellow delivered at NIPS in 2016 &lt;a href=&#34;https://arxiv.org/pdf/1701.00160&#34; title=&#34;NIPS 2016 Tutorial: Generative Adversarial Networks&#34;&gt;here&lt;/a&gt;. I&amp;rsquo;ve read both of these (and others) as well as taking a look at other tutorials but sometimes things just weren&amp;rsquo;t clear enough for me. &lt;a href=&#34;http://bamos.github.io/2016/08/09/deep-completion/#ml-heavy-generative-adversarial-net-gan-building-blocks&#34; title=&#34;bamos.github.io&#34;&gt;This blog from B. Amos&lt;/a&gt; has been helpful in getting my thoughts organised on this series, and hopefully I can build on this a little and make things more concrete.&lt;/p&gt;

&lt;h3&gt;What&#39;s a GAN?&lt;/h3&gt;

&lt;p&gt;GANs  are used in a number of ways, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to generate new images based upon some training data. For our tutorial, we will train with a database of faces and ask the network to produce a new face.&lt;/li&gt;
&lt;li&gt;to do &amp;lsquo;inpainting&amp;rsquo; or &amp;lsquo;image completion&amp;rsquo;. This is where part of a scene may be missing and we wish to recover the full image. It could be that we want to remove parts of the image e.g. people, and fill-in the background.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two components in a GAN which try to work against each other (hence the &amp;lsquo;adversarial&amp;rsquo; part).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Generator (&lt;em&gt;G&lt;/em&gt;) starts off by creating a very noisy image based upon some random input data. Its job is to try to come up with images that are as real as possible.&lt;/li&gt;
&lt;li&gt;The Discriminator (&lt;em&gt;D&lt;/em&gt;) is trying to determine whether an image is real or fake.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Though these two are the primary components of the network, we also need to write some functions for importing data and dealing with the training of this two-stage network. Part 1 of this tutorial will go through some background and mathematics, in Part 2 we will do some general housekeeping and get us prepared to write the main model of our network in Part 3.&lt;/p&gt;

&lt;h2 id=&#34;maths&#34;&gt; Background &lt;/h2&gt;

&lt;p&gt;There are a number of situations where you may want to use a GAN. A common task is for image completion or &amp;lsquo;in-painting&amp;rsquo;. This would be where we have an image and would like to remove some obstruction or imperfection by replacing it with the background. Maybe there&amp;rsquo;s a lovely holiday photo of beautiful scenery, but there are some people you don&amp;rsquo;t know spoiling the view. Figure 1 shows an example of the result of image completion using PhotoShop on such an image.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;https://farm5.staticflickr.com/4115/4756059924_e26ae12e46_b.jpg&#34; width=&#34;100%&#34; alt=&#34;Image Completion Example&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: Removal of unwated parts of a scene with image completion. Source: &lt;a href=&#34;https://www.flickr.com/photos/littleredelf/4756059924/in/photostream/&#34; alt=&#34;littleredelf&#34;&gt;Flickr:littleredelf&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;We have a couple of options if we want to try and do this kind of image completion ourselves. Let&amp;rsquo;s say we draw around an area we want to change:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If we&amp;rsquo;ve never seen a beach or the sky before, well we may just have to use the neighbouring pixels to inform our in-filling. If we&amp;rsquo;re feeling fancy, we would look a little further afield and use that information too ( i.e. is there just sky around the area, or is there something else).&lt;/li&gt;
&lt;li&gt;Or&amp;hellip; we could look at the image as a whole and try to see what would fit best. For this we would have to use our knowledge of similar scenes we&amp;rsquo;ve observed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is the difference between using (1) contextual and (2) perceptual information. But before we look more heavily into this, let&amp;rsquo;s take a look at the idea behind a GAN.&lt;/p&gt;

&lt;h2 id=&#34;gan&#34;&gt; Generative Adversarial Networks &lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve said that there are two components in a GAN, the &lt;em&gt;generator&lt;/em&gt; and the &lt;em&gt;discriminator&lt;/em&gt;. Here, we&amp;rsquo;ll look more closely at what they do.&lt;/p&gt;

&lt;p&gt;Our purpose is to create images which are as realistic as possible. So much so, that they are able to fool not only humans, but the computer that has generated them. You will often see GANs being compared to money counterfeiting: our generator is trying to create fake money whilst our discriminator is trying to tell the difference between the real and fake bills. How does this work?&lt;/p&gt;

&lt;p&gt;Say we have an image $x$ which our discriminator $D$ is analysing. $D(x)$ gives a low value near to 0 if the image looks normal or &amp;lsquo;natural&amp;rsquo; and a higher value near to 1 if it thinks the image is fake - this could mean it is very noisy for example. The generator $G$ takes a vector $z$ that has been randomly sampled from a very simple, but well known, distribution e.g. a uniform or normal distribution. The image that is produced by $G(z)$ should help to train the function at $D$. We alternate showing the discriminator a real image (which will change its parameters to give a low output) and then an image from $G$ (which will change $D$ to give a higher output). At the same time, we want $G$ to also be learning to produce more realistic images which are more likely to fool $D$. We want $G$ to &lt;em&gt;minimise&lt;/em&gt; the output of $D$ whilst $D$ is trying to &lt;em&gt;maximise&lt;/em&gt; the same thing. They are playing a &lt;a href=&#34;https://en.wikipedia.org/wiki/Minimax&#34; title=&#34;Wiki: minimax&#34;&gt;&amp;lsquo;minimax&amp;rsquo;&lt;/a&gt; game against each other, which is where we get the term &amp;lsquo;adversarial&amp;rsquo; training.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;/img/CNN/gan1.png&#34; width=&#34;100%&#34; alt=&#34;GAN&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: Generative Adversarial Network concept. Simple, known distribution $p_z$ from which the vector $z$ is drawn. Generator $G(z)$ generates an image. Discriminator tries to determine if image came from $G$ or from the true, unknown distribution $p_{data}$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s keep going with the maths&amp;hellip;&lt;/p&gt;

&lt;p&gt;This kind of network has a lot of latent (hidden) variables that need to be found. But we can start from a strong position by using a distribution that we know very well like a uniform distribution.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;known&lt;/strong&gt; distribution we denote $p_z$ We will randomly draw a vector $z$ from $p_z$.&lt;/li&gt;
&lt;li&gt;We know that our data must have some distribution but we do &lt;strong&gt;not&lt;/strong&gt; know this. We&amp;rsquo;ll call this $p_{data}$&lt;/li&gt;
&lt;li&gt;Our generator will try to learn its own distribution $p_g$. Our goal is for $p_g = p_{data}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have two networks to train, $D$ and $G$:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We want to &lt;em&gt;minimise&lt;/em&gt; $D(x)$ if $x$ is drawn from our true distribution $p_{data}$ i.e. &lt;em&gt;minimise&lt;/em&gt; $D(x)$ if it&amp;rsquo;s not.&lt;/li&gt;
&lt;li&gt;and &lt;em&gt;maximise&lt;/em&gt; $D(G(z))$ i.e. &lt;em&gt;minimise&lt;/em&gt; $1 - D(G(z))$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More formally:&lt;/p&gt;

&lt;div&gt;$$
\min_{G}\max_{D} V(D, G) = \mathbb{E}_{x\sim p_{data}} \left[ \log D(x)  \right]+ \mathbb{E}_{z\sim p_{z}} \left[ \log \left( 1 - D(G(z)) \right) \right]

$$
&lt;/div&gt;

&lt;p&gt;Where $\mathbb{E}$ is the expectation. The advantage of working with neural networks is that we can easily compute gradients and use backpropagation to perform training. This is because the generator and the discriminator are defined by the multi-layer perceptron (MLP) parameters $\theta_g$ and $\theta_d$ respectively.&lt;/p&gt;

&lt;p&gt;We will train the networks (the $G$ and the $D$) one at a time, fixing the weights of one whilst training the other. From the GAN paper by Goodfellow &lt;em&gt;et al&lt;/em&gt; we get the &lt;em&gt;pseudo&lt;/em&gt; code for this procedure:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;/img/CNN/ganalgorithm.png&#34; width=&#34;100%&#34; alt=&#34;GAN&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: &lt;i&gt;pseudo&lt;/i&gt; code for GAN training. With $k=1$ this equates to training $D$ then $G$ one after the other. Adapted from &lt;a href=&#34;https://arxiv.org/pdf/1406.2661&#34; title=&#34;Goodfellow et al. 2014&#34;&gt;Goodfellow &lt;i&gt;et al.&lt;/i&gt; 2014&lt;/a&gt;.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Notice that with $k=1$ we are training $D$ then $G$ one after the other. What is the training actually doing? Fig. 4 shows the distribution $p_g$ of the generator in green. Notice that with each training step, the $p_g$ becomes more like the true distribution of the image data $p_{data}$ in black. After each alternation, the error is backpropagated to udate the weights on the network that is not being held fixed. The discriminator eventually reaches its &lt;em&gt;lowest maximum&lt;/em&gt; where it is no longer able to tell the difference between the true and fake images.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img src=&#34;/img/CNN/ganalgographs.png&#34; width=&#34;100%&#34; alt=&#34;GAN&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 4&lt;/font&gt;: Initially (a) the generator&#39;s and true data distributions (green and black) are not very similar. (b) the discriminator (blue) is updated with generator held constant. (c) Generator is updated with discriminator held constant, until (d) $p_g$ and $p_{data}$ are most alike. Adapted from &lt;a href=&#34;https://arxiv.org/pdf/1406.2661&#34; title=&#34;Goodfellow et al. 2014&#34;&gt;Goodfellow &lt;i&gt;et al.&lt;/i&gt; 2014&lt;/a&gt;.
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;nextsteps&#34;&gt; What&#39;s Next?&#34;&lt;/h2&gt;

&lt;p&gt;That really is it. The basics of a GAN are just a game between two networks, the generator $G$, which produces images from some latent variables $z$, and the discriminator $D$ which tries to detect the faked images.&lt;/p&gt;

&lt;p&gt;Implementing this in Python seems old-hat to many and there are many pre-built solutions available. The work in this tutorial series will mostly follow the base-code from &lt;a href=&#34;https://github.com/carpedm20/DCGAN-tensorflow&#34; title=&#34;carpedm20/DCGAN-tensorflow&#34;&gt;carpedm20&amp;rsquo;s DCGAN-tensorflow repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the next post, we&amp;rsquo;ll get ourselves organised, make sure we have some dependencies, create some files and get our training data sorted.&lt;/p&gt;

&lt;p&gt;As always, if there&amp;rsquo;s anything wrong or that doesn&amp;rsquo;t make send &lt;strong&gt;please&lt;/strong&gt; get in contact and let me know. A comment here is great.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Simple Neural Network - Mathematics</title>
      <link>/post/neuralnetwork/</link>
      <pubDate>Mon, 06 Mar 2017 17:04:53 +0000</pubDate>
      
      <guid>/post/neuralnetwork/</guid>
      <description>&lt;p&gt;This is the first part of a series of tutorials on Simple Neural Networks (NN). Tutorials on neural networks (NN) can be found all over the internet. Though many of them are the same, each is written (or recorded) slightly differently. This means that I always feel like I learn something new or get a better understanding of things with every tutorial I see. I&amp;rsquo;d like to make this tutorial as clear as I can, so sometimes the maths may be simplistic, but hopefully it&amp;rsquo;ll give you a good unserstanding of what&amp;rsquo;s going on.  &lt;strong&gt;Please&lt;/strong&gt; let me know if any of the notation is incorrect or there are any mistakes - either comment or use the contact page on the left.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#nnarchitecture&#34;&gt;Neural Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferFunction&#34;&gt;Transfer Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedforward&#34;&gt;Feed-forward&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#error&#34;&gt;Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationGrads&#34;&gt;Back Propagation - the Gradients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias&#34;&gt;Bias&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backPropagationAlgorithm&#34;&gt;Back Propagaton - the Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;nnarchitecture&#34;&gt;1. Neural Network Architecture &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By now, you may well have come across diagrams which look very similar to the one below. It shows some input node, connected to some output node via an intermediate node in what is called a &amp;lsquo;hidden layer&amp;rsquo; - &amp;lsquo;hidden&amp;rsquo; because in the use of NN only the input and output is of concern to the user, the &amp;lsquo;under-the-hood&amp;rsquo; stuff may not be interesting to them. In real, high-performing NN there are usually more hidden layers.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;Simple NN&#34; width=40% src=&#34;/img/simpleNN/simpleNN.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: A simple 2-layer NN with 2 features in the input layer, 3 nodes in the hidden layer and two nodes in the output layer.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;When we train our network, the nodes in the hidden layer each perform a calculation using the values from the input nodes. The output of this is passed on to the nodes of the next layer. When the output hits the final layer, the &amp;lsquo;output layer&amp;rsquo;, the results are compared to the real, known outputs and some tweaking of the network is done to make the output more similar to the real results. This is done with an algorithm called &lt;em&gt;back propagation&lt;/em&gt;. Before we get there, lets take a closer look at these calculations being done by the nodes.&lt;/p&gt;

&lt;h2 id=&#34;transferFunction&#34;&gt;2. Transfer Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At each node in the hidden and output layers of the NN, an &lt;em&gt;activation&lt;/em&gt; or &lt;em&gt;transfer&lt;/em&gt; function is executed. This function takes in the output of the previous node, and multiplies it by some &lt;em&gt;weight&lt;/em&gt;. These weights are the lines which connect the nodes. The weights that come out of one node can all be different, that is they will &lt;em&gt;activate&lt;/em&gt; different neurons. There can be many forms of the transfer function, we will first look at the &lt;em&gt;sigmoid&lt;/em&gt; transfer function as it seems traditional.&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;The sigmoid function&#34; width=50% src=&#34;/img/simpleNN/sigmoid.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: The sigmoid function.
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As you can see from the figure, the sigmoid function takes any real-valued input and maps it to a real number in the range $(0 \ 1)$ - i.e. between, but not equal to, 0 and 1. We can think of this almost like saying &amp;lsquo;if the value we have maps to an output near 1, this node fires, if it maps to an output near 0, the node does not fire&amp;rsquo;. The equation for this sigmoid function is:&lt;/p&gt;

&lt;div id=&#34;eqsigmoidFunction&#34;&gt;$$
\sigma ( x ) = \frac{1}{1 + e^{-x}}
$$&lt;/div&gt;

&lt;p&gt;We need to have the derivative of this transfer function so that we can perform back propagation later on. This is the process where by the connections in the network are updated to tune the performance of the NN. We&amp;rsquo;ll talk about this in more detail later, but let&amp;rsquo;s find the derivative now.&lt;/p&gt;

&lt;div&gt;
$$
\begin{align*}
\frac{d}{dx}\sigma ( x ) &amp;= \frac{d}{dx} \left( 1 + e^{ -x }\right)^{-1}\\
&amp;=  -1 \times -e^{-x} \times \left(1 + e^{-x}\right)^{-2}= \frac{ e^{-x} }{ \left(1 + e^{-x}\right)^{2} } \\
&amp;= \frac{\left(1 + e^{-x}\right) - 1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{\left(1 + e^{-x}\right) }{\left(1 + e^{-x}\right)^{2}} - \frac{1}{\left(1 + e^{-x}\right)^{2}} 
= \frac{1}{\left(1 + e^{-x}\right)} - \left( \frac{1}{\left(1 + e^{-x}\right)} \right)^{2} \\[0.5em]
&amp;= \sigma ( x ) - \sigma ( x ) ^ {2}
\end{align*}
$$&lt;/div&gt;

&lt;p&gt;Therefore, we can write the derivative of the sigmoid function as:&lt;/p&gt;

&lt;div id=&#34;eqdsigmoid&#34;&gt;$$
\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)
$$&lt;/div&gt;

&lt;p&gt;The sigmoid function has the nice property that its derivative is very simple: a bonus when we want to hard-code this into our NN later on. Now that we have our activation or transfer function selected, what do we do with it?&lt;/p&gt;

&lt;h2 id=&#34;feedforward&#34;&gt;3. Feed-forward &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;During a feed-forward pass, the network takes in the input values and gives us some output values. To see how this is done, let&amp;rsquo;s first consider a 2-layer neural network like the one in Figure 1. Here we are going to refer to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$i$ - the $i^{\text{th}}$ node of the input layer $I$&lt;/li&gt;
&lt;li&gt;$j$ - the $j^{\text{th}}$ node of the hidden layer $J$&lt;/li&gt;
&lt;li&gt;$k$ - the $k^{\text{th}}$ node of the input layer $K$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The activation function at a node $j$ in the hidden layer takes the value:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{j} &amp;= \xi_{1} w_{1j} + \xi_{2} w_{2j} \\[0.5em]
&amp;= \sum_{i \in I} \xi_{i} w_{i j}

\end{align}
$$&lt;/div&gt;

&lt;p&gt;where $\xi_{i}$ is the value of the $i^{\text{th}}$ input node and $w_{i j}$ is the weight of the connection between $i^{\text{th}}$ input node and the $j^{\text{th}}$ hidden node. &lt;strong&gt;In short:&lt;/strong&gt; at each hidden layer node, multiply each input value by the connection received by that node and add them together.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the weights are initisliased when the network is setup. Sometimes they are all set to 1, or often they&amp;rsquo;re set to some small random value.&lt;/p&gt;

&lt;p&gt;We apply the activation function on $x_{j}$ at the $j^{\text{th}}$ hidden node and get:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{j} &amp;= \sigma(x_{j}) \\
&amp;= \sigma(  \xi_{1} w_{1j} + \xi_{2} w_{2j})
\end{align}
$$&lt;/div&gt;

&lt;p&gt;$\mathcal{O}_{j}$ is the output of the $j^{\text{th}}$ hidden node. This is calculated for each of the $j$ nodes in the hidden layer. The resulting outputs now become the input for the next layer in the network. In our case, this is the final output later. So for each of the $k$ nodes in $K$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\mathcal{O}_{k} &amp;= \sigma(x_{k}) \\
&amp;= \sigma \left( \sum_{j \in J}  \mathcal{O}_{j} w_{jk}  \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;As we&amp;rsquo;ve reached the end of the network, this is also the end of the feed-foward pass. So how well did our network do at getting the correct result $\mathcal{O}_{k}$? As this is the training phase of our network, the true results will be known an we cal calculate the error.&lt;/p&gt;

&lt;h2 id=&#34;error&#34;&gt;4. Error &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We measure error at the end of each foward pass. This allows us to quantify how well our network has performed in getting the correct output. Let&amp;rsquo;s define $t_{k}$ as the expected or &lt;em&gt;target&lt;/em&gt; value of the $k^{\text{th}}$ node of the output layer $K$. Then the error $E$ on the entire output is:&lt;/p&gt;

&lt;div id=&#34;eqerror&#34;&gt;$$
\text{E} = \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;Dont&amp;rsquo; be put off by the random &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; in front there, it&amp;rsquo;s been manufactured that way to make the upcoming maths easier. The rest of this should be easy enough: get the residual (difference between the target and output values), square this to get rid of any negatives and sum this over all of the nodes in the output layer.&lt;/p&gt;

&lt;p&gt;Good! Now how does this help us? Our aim here is to find a way to tune our network such that when we do a forward pass of the input data, the output is exactly what we know it should be. But we can&amp;rsquo;t change the input data, so there are only two other things we can change:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the weights going into the activation function&lt;/li&gt;
&lt;li&gt;the activation function itself&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will indeed consider the second case in another post, but the magic of NN is all about the &lt;em&gt;weights&lt;/em&gt;. Getting each weight i.e. each connection between nodes, to be just the perfect value, is what back propagation is all about. The back propagation algorithm we will look at in the next section, but lets go ahead and set it up by considering the following: how much of this error $E$ has come from each of the weights in the network?&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re asking, what is the proportion of the error coming from each of the $W_{jk}$ connections between the nodes in layer $J$ and the output layer $K$. Or in mathematical terms:&lt;/p&gt;

&lt;div&gt;$$
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{\partial{}}{\partial{W_{jk}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;If you&amp;rsquo;re not concerned with working out the derivative, skip this highlighted section.&lt;/p&gt;

&lt;div class=&#34;highlight_section&#34;&gt;

To tackle this we can use the following bits of knowledge: the derivative of the sum is equal to the sum of the derivatives i.e. we can move the derivative term inside of the summation:

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \sum_{k \in K} \frac{\partial{}}{\partial{W_{jk}}} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the weight $w_{1k}$ does not affect connection $w_{2k}$ therefore the change in $W_{jk}$ with respect to any node other than the current $k$ is zero. Thus the summation goes away:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \frac{1}{2} \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;apply the power rule knowing that $t_{k}$ is a constant:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}} &amp;=  \frac{1}{2} \times 2 \times \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right) \\
 &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{k}\right)
\end{align}
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the leftover derivative is the chage in the output values with respect to the weights. Substituting $ \mathcal{O}_{k} = \sigma(x_{k}) $ and the sigmoid derivative $\sigma^{\prime}( x ) = \sigma (x ) \left( 1 - \sigma ( x ) \right)$:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}} =  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( x_{k}\right)
$$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;the final derivative, the input value $x_{k}$ is just $\mathcal{O}_{j} W_{jk}$ i.e. output of the previous layer times the weight to this layer. So the change in  $\mathcal{O}_{j} w_{jk}$ with respect to $w_{jk}$ just gives us the output value of the previous layer $ \mathcal{O}_{j} $ and so the full derivative becomes:&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ 
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  &amp;=  \left( \mathcal{O}_{k} - t_{k} \right) \sigma (x ) \left( 1 - \sigma ( x ) \right) \frac{\partial{}}{\partial{W_{jk}}}  \left( \mathcal{O}_{j} W_{jk} \right) \\[0.5em]
&amp;=\left( \mathcal{O}_{k} - t_{k} \right) \sigma (x )  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j} 
\end{align}
$$&lt;/div&gt;

&lt;p&gt;We can replace the sigmoid function with the output of the layer
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;The derivative of the error function with respect to the weights is then:&lt;/p&gt;

&lt;div id=&#34;derror&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  =\left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right) \mathcal{O}_{j}
$$&lt;/div&gt;

&lt;p&gt;We group the terms involving $k$ and define:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;

&lt;p&gt;And therefore:&lt;/p&gt;

&lt;div id=&#34;derrorjk&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{jk}}}  = \mathcal{O}_{j} \delta_{k} 
$$&lt;/div&gt;

&lt;p&gt;So we have an expression for the amount of error, called &amp;lsquo;deta&amp;rsquo; ($\delta_{k}$), on the weights from the nodes in $J$ to each node $k$ in $K$. But how does this help us to improve out network? We need to back propagate the error.&lt;/p&gt;

&lt;h2 id=&#34;backPropagationGrads&#34;&gt;5. Back Propagation - the gradients&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Back propagation takes the error function we found in the previous section, uses it to calculate the error on the current layer and updates the weights to that layer by some amount.&lt;/p&gt;

&lt;p&gt;So far we&amp;rsquo;ve only looked at the error on the output layer, what about the hidden layer? This also has an error, but the error here depends on the output layer&amp;rsquo;s error too (because this is where the difference between the target $t_{k}$ and output $\mathcal{O}_{k}$ can be calculated). Lets have a look at the error on the weights of the hidden layer $W_{ij}$:&lt;/p&gt;

&lt;div&gt;$$ \frac{\partial{\text{E}}}{\partial{W_{ij}}} =  \frac{\partial{}}{\partial{W_{ij}}}  \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}$$&lt;/div&gt;

&lt;p&gt;Now, unlike before, we cannot just drop the summation as the derivative is not directly acting on a subscript $k$ in the summation. We should be careful to note that the output from every node in $J$ is actually connected to each of the nodes in $K$ so the summation should stay. But we can still use the same tricks as before: lets use the power rule again and move the derivative inside (because the summation is finite):&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;=  \frac{1}{2} \times 2 \times  \frac{\partial{}}{\partial{W_{ij}}}   \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)  \mathcal{O}_{k} \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} \mathcal{O}_{k}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Again, we substitute $\mathcal{O}_{k} = \sigma( x_{k})$ and its derivative and revert back to our output notation:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (\sigma(x_{k}) )\\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \sigma(x_{k}) \left( 1 - \sigma(x_{k}) \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k}) \\
&amp;= \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) \frac{\partial{}}{\partial{W_{ij}}} (x_{k})
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;This still looks familar from the output layer derivative, but now we&amp;rsquo;re struggling with the derivative of the input to $k$ i.e. $x_{k}$ with respect to the weights from $I$ to $J$. Let&amp;rsquo;s use the chain rule to break apart this derivative in terms of the output from $J$:&lt;/p&gt;

&lt;div&gt; $$
\frac{\partial{ x_{k}}}{\partial{W_{ij}}} = \frac{\partial{ x_{k}}}{\partial{\mathcal{O}_{j}}}\frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}
$$&lt;/div&gt;

&lt;p&gt;The change of the input to the $k^{\text{th}}$ node with respect to the output from the $j^{\text{th}}$ node is down to a product with the weights, therefore this derivative just becomes the weights $W_{jk}$. The final derivative has nothing to do with the subscript $k$ anymore, so we&amp;rsquo;re free to move this around - lets put it at the beginning:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{\mathcal{O}_{j}}}{\partial{W_{ij}}}  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Lets finish the derivatives, remembering that the output of the node $j$ is just $\mathcal{O}_{j} = \sigma(x_{j}) $ and we know the derivative of this function too:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \frac{\partial{}}{\partial{W_{ij}}}\sigma(x_{j})  \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \sigma(x_{j}) \left( 1 - \sigma(x_{j}) \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk} \\
&amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \frac{\partial{x_{j} }}{\partial{W_{ij}}} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;The final derivative is straightforward too, the derivative of the input to $j$ with repect to the weights is just the previous input, which in our case is $\mathcal{O}_{i}$,&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right) \mathcal{O}_{k} \left( 1 - \mathcal{O}_{k} \right) W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;Almost there! Recall that we defined $\delta_{k}$ earlier, lets sub that in:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\frac{\partial{\text{E}}}{\partial{W_{ij}}} &amp;= \mathcal{O}_{j} \left( 1 - \mathcal{O}_{j} \right)  \mathcal{O}_{i} \sum_{k \in K} \delta_{k} W_{jk}
 \end{align}
 $$&lt;/div&gt;
 

&lt;p&gt;To clean this up, we now define the &amp;lsquo;delta&amp;rsquo; for our hidden layer:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;

&lt;p&gt;Thus, the amount of error on each of the weights going into our hidden layer:&lt;/p&gt;

&lt;div id=&#34;derrorij&#34;&gt;$$ 
\frac{\partial{\text{E}}}{\partial{W_{ij}}}  = \mathcal{O}_{i} \delta_{j} 
$$&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the reason for the name &lt;em&gt;back&lt;/em&gt; propagation is that we must calculate the errors at the far end of the network and work backwards to be able to calculate the weights at the front.&lt;/p&gt;

&lt;h2 id=&#34;bias&#34;&gt;6.  Bias &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets remind ourselves what happens inside our hidden layer nodes:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInsideNoBias.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;Each feature $\xi_{i}$ from the input layer $I$ is multiplied by some weight $w_{ij}$&lt;/li&gt;
&lt;li&gt;These are added together to get $x_{i}$ the total, weighted input from the nodes in $I$&lt;/li&gt;
&lt;li&gt;$x_{i}$ is passed through the activation, or transfer, function $\sigma(x_{i})$&lt;/li&gt;
&lt;li&gt;This gives the output $\mathcal{O}_{j}$ for each of the $j$ nodes in hidden layer $J$&lt;/li&gt;
&lt;li&gt;$\mathcal{O}_{j}$ from each of the $J$ nodes becomes $\xi_{j}$ for the next layer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we talk about the &lt;em&gt;bias&lt;/em&gt; term in NN, we are talking about an additional parameter that is inluded in the summation of step 2 above. The bias term is usually denoted with the symbol $\theta$ (theta). It&amp;rsquo;s function is to act as a threshold for the activation (transfer) function. It is given the value of 1 and is not connected to anything else. As such, this means that any derivative of the node&amp;rsquo;s output with respect to the bias term would just give a constant, 1. This allows us to just think of the bias term as an output from the node with the value of 1. This will be updated later during backpropagation to change the threshold at which the node fires.&lt;/p&gt;

&lt;p&gt;Lets update the equation for $x_{i}$:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
x_{i} &amp;= \xi_{1j} w_{1j} + \xi_{2j} w_{2j} + \theta_{j} \\[0.5em]
\sigma( x_{i} ) &amp;= \sigma \left( \sum_{i \in I} \left( \xi_{ij} w_{ij} \right) + \theta_{j} \right)
\end{align}
$$&lt;/div&gt;

&lt;p&gt;and put it on the diagram:&lt;/p&gt;

&lt;div class=&#34;figure_container&#34;&gt;
    &lt;div class=&#34;figure_images&#34;&gt;
    &lt;img title=&#34;Simple NN&#34;  width=50% src=&#34;/img/simpleNN/nodeInside.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;figure_caption&#34;&gt;
        &lt;font color=&#34;blue&#34;&gt;Figure 3&lt;/font&gt;: The insides of a hidden layer node, $j$.
    &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;backPropagationAlgorithm&#34;&gt;7. Back Propagation - the algorithm&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now we have all of the pieces! We&amp;rsquo;ve got the initial outputs after our feed-forward, we have the equations for the delta terms (the amount by which the error is based on the different weights) and we know we need to update our bias term too. So what does it look like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Input the data into the network and feed-forward&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;output&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{k} = \mathcal{O}_{k}  \left( 1 - \mathcal{O}_{k}  \right)  \left( \mathcal{O}_{k} - t_{k} \right)
$$&lt;/div&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For each of the &lt;em&gt;hidden layer&lt;/em&gt; nodes calculate:&lt;/p&gt;

&lt;div&gt;$$
\delta_{j} = \mathcal{O}_{i} \left( 1 - \mathcal{O}_{j} \right)   \sum_{k \in K} \delta_{k} W_{jk}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calculate the changes that need to be made to the weights and bias terms:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
\Delta W &amp;= -\eta \ \delta_{l} \ \mathcal{O}_{l-1} \\
\Delta\theta &amp;= -\eta \ \delta_{l}
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update the weights and biases across the network:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}
W + \Delta W &amp;\rightarrow W \\
\theta + \Delta\theta &amp;\rightarrow \theta
\end{align}
$$&lt;/div&gt;
    &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, $\eta$ is just a small number that limit the size of the deltas that we compute: we don&amp;rsquo;t want the network jumping around everywhere. The $l$ subscript denotes the deltas and output for that layer $l$. That is, we compute the delta for each of the nodes in a layer and vectorise them. Thus we can compute the element-wise product with the output values of the previous layer and get our update $\Delta W$ for the weights of the current later. Similarly with the bias term.&lt;/p&gt;

&lt;p&gt;This algorithm is looped over and over until the error between the output and the target values is below some set threshold. Depending on the size of the network i.e. the number of layers and number of nodes per layer, it can take a long time to complete one &amp;lsquo;epoch&amp;rsquo; or run through of this algorithm.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Some of the ideas and notation in this tutorial comes from the good videos by &lt;a href=&#34;https://www.youtube.com/playlist?list=PL29C61214F2146796&#34; title=&#34; NN Videos&#34;&gt;Ryan Harris&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>