<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Machine Learning Notebook</title>
    <link>/tags/python/</link>
    <description>Recent content in Python on Machine Learning Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Mar 2017 08:53:55 +0000</lastBuildDate>
    <atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Simple Neural Network - Simple Performance Improvements</title>
      <link>/post/nn-python-tweaks/</link>
      <pubDate>Fri, 17 Mar 2017 08:53:55 +0000</pubDate>
      
      <guid>/post/nn-python-tweaks/</guid>
      <description>&lt;p&gt;The 5th installment of our tutorial on implementing a neural network (NN) in Python. By the end of this tutorial, our NN should perform much more efficiently giving good results with fewer iterations. We will do this by implementing &amp;ldquo;momentum&amp;rdquo; into our network. We will also put in the other transfer functions for each layer.&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#intro:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#momentum:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Momentum&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#momentumbackground:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#momentumpython:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Momentum in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#momentumtesting:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferfunctions:461b95b1eb366b73f8031e88185ea18c&#34;&gt;Transfer Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;intro&#34;&gt; Introduction &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:461b95b1eb366b73f8031e88185ea18c&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve come so far! The intial &lt;a href=&#34;/post/neuralnetwork&#34;&gt;maths&lt;/a&gt; was a bit of a slog, as was the &lt;a href=&#34;/post/nn-more-maths&#34;&gt;vectorisation&lt;/a&gt; of that maths, but it was important to be able to implement our NN in Python which we did in our &lt;a href=&#34;/post/nn-in-python&#34;&gt;previous post&lt;/a&gt;. So what now? Well, you may have noticed when running the NN as it stands that it isn&amp;rsquo;t overly quick, depening on the randomly initialised weights, it may take the network the full number of &lt;code&gt;maxIterations&lt;/code&gt; to converge, and then it may not converge at all! But there is something we can do about it. Let&amp;rsquo;s learn about, and implement, &amp;lsquo;momentum&amp;rsquo;.&lt;/p&gt;

&lt;h2 id=&#34;momentum&#34;&gt; Momentum &lt;/h2&gt;

&lt;h3 id=&#34;momentumbackground&#34;&gt; Background &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:461b95b1eb366b73f8031e88185ea18c&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s revisit our equation for error in the NN:&lt;/p&gt;

&lt;div id=&#34;eqerror&#34;&gt;$$
\text{E} = \frac{1}{2} \sum_{k \in K} \left( \mathcal{O}_{k} - t_{k} \right)^{2}
$$&lt;/div&gt;

&lt;p&gt;This isn&amp;rsquo;t the only error function that could be used. In fact, there&amp;rsquo;s a whole field of study in NN about the best error or &amp;lsquo;optimisation&amp;rsquo; function that should be used. This one tries to look at the sum of the squared-residuals between the outputs and the expected values at the end of each forward pass (the so-called $l_{2}$-norm). Others e.g. $l_{1}$-norm, look at minimising the sum of the absolute differences between the values themselves. There are more complex error (a.k.a. optimisation or cost) functions, for example those that look at the cross-entropy in the data. There may well be a post in the future about different cost-functions, but for now we will still focus on the equation above.&lt;/p&gt;

&lt;p&gt;Now this function is described as a &amp;lsquo;convex&amp;rsquo; function. This is an important property if we are to make our NN converge to the correct answer. Take a look at the two functions below:&lt;/p&gt;

&lt;div  id=&#34;fig1&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;convex&#34; src=&#34;/img/simpleNN/convex.png&#34; width=&#34;35%&#34; hspace=&#34;10px&#34;&gt;&lt;img title=&#34;non-convex&#34; src=&#34;/img/simpleNN/non-convex.png&#34; width=&#34;35%&#34; hspace=&#34;10px&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 1&lt;/font&gt;: A convex (left) and non-convex (right) cost function
        &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s say that our current error was represented by the green ball. Our NN will calculate the gradient of its cost function at this point then look for the direction which is going to &lt;em&gt;minimise&lt;/em&gt; the error i.e. go down a slope. The NN will feed the result into the back-propagation algorithm which will hopefully mean that on the next iteration, the error will have decreased. For a &lt;em&gt;convex&lt;/em&gt; function, this is very straight forward, the NN just needs to keep going in the direction it found on the first run. But, look at the &lt;em&gt;non-convex&lt;/em&gt; or &lt;em&gt;stochastic&lt;/em&gt; function: our current error (green ball) sits at a point where either direction will take it to a lower error i.e. the gradient decreases on both sides. If the error goes to the left, it will hit &lt;strong&gt;one&lt;/strong&gt; of the possible minima of the function, but this will be a higher minima (higher final error) than if the error had chosen the gradient to the right. Clearly the starting point for the error here has a big impact on the final result. Looking down at the 2D perspective (remembering that these are complex multi-dimensional functions), the non-convex case is clearly more ambiguous in terms of the location of the minimum and direction of descent. The convex function, however, nicely guides the error to the minimum with little care of the starting point.&lt;/p&gt;

&lt;div  id=&#34;fig2&#34; class=&#34;figure_container&#34;&gt;
        &lt;div class=&#34;figure_images&#34;&gt;
        &lt;img title=&#34;convexcontour&#34; src=&#34;/img/simpleNN/convexcontourarrows.png&#34; width=&#34;35%&#34; hspace=&#34;10px&#34;&gt;&lt;img title=&#34;non-convexcontour&#34; src=&#34;/img/simpleNN/nonconvexcontourarrows.png&#34; width=&#34;35%&#34; hspace=&#34;10px&#34;&gt;
        &lt;/div&gt;
        &lt;div class=&#34;figure_caption&#34;&gt;
            &lt;font color=&#34;blue&#34;&gt;Figure 2&lt;/font&gt;: Contours for a portion of the convex (left) and non-convex (right) cost function
        &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So let&amp;rsquo;s focus on the convex case and explain what &lt;em&gt;momentum&lt;/em&gt; is and why it works. I don&amp;rsquo;t think you&amp;rsquo;ll ever see a back propagation algorithm without momentum implemented in some way. In its simplest form, it modifies the weight-update equation:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{ \Delta W_{JK} = -\eta \vec{\delta}_{K} \vec{ \mathcal{O}_{J}}}
$$&lt;/div&gt;

&lt;p&gt;by adding an extra &lt;em&gt;momentum&lt;/em&gt; term:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{ \Delta W_{JK}\left(t\right) = -\eta \vec{\delta}_{K} \vec{ \mathcal{O}_{J}}} + m \mathbf{\Delta W_{JK}\left(t-1\right)}
$$&lt;/div&gt;

&lt;p&gt;The weight delta (the update amount to the weights after BP) now relies on its &lt;em&gt;previous&lt;/em&gt; value i.e. the weight delta now at iteration $t$ requires the value of itself from $t-1$. The $m$ or momentum term, like the learning rate $\eta$ is just a small number between 0 and 1. What effect does this have?&lt;/p&gt;

&lt;p&gt;Using prior information about the network is beneficial as it stops the network firing wildly into the unknown. If it can know the previous weights that have given the current error, it can keep the descent to the minimum roughly pointing in the same direction as it was before. The effect is that each iteration does not jump around so much as it would otherwise. In effect, the result is similar to that of the learning rate. We should be careful though, a large value for $m$ may cause the result to jump past the minimum and back again if combined with a large learning rate. We can think of momentum as changing the path taken to the optimum.&lt;/p&gt;

&lt;h3 id=&#34;momentumpython&#34;&gt; Momentum in Python &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:461b95b1eb366b73f8031e88185ea18c&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, implementing momentum into our NN should be pretty easy. We will need to provide a momentum term to the &lt;code&gt;backProp&lt;/code&gt; method of the NN and also create a new matrix in which to store the weight deltas from the current epoch for use in the subsequent one.&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;__init__&lt;/code&gt; method of the NN, we need to initialise the previous weight matrix and then give them some  values - they&amp;rsquo;ll start with zeros:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, numNodes):
	&amp;quot;&amp;quot;&amp;quot;Initialise the NN - setup the layers and initial weights&amp;quot;&amp;quot;&amp;quot;

	# Layer info
	self.numLayers = len(numNodes) - 1
	self.shape = numNodes 

	# Input/Output data from last run
	self._layerInput = []
	self._layerOutput = []
	self._previousWeightDelta = []

	# Create the weight arrays
	for (l1,l2) in zip(numNodes[:-1],numNodes[1:]):
	    self.weights.append(np.random.normal(scale=0.1,size=(l2,l1+1))) 
	    self._previousWeightDelta.append(np.zeros((l2,l1+1)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only other part of the NN that needs to change is the definition of &lt;code&gt;backProp&lt;/code&gt; adding momentum to the inputs, and updating the weight equation. Finally, we make sure to save the current weights into the previous-weight matrix:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backProp(self, input, target, trainingRate = 0.2, momentum=0.5):
	&amp;quot;&amp;quot;&amp;quot;Get the error, deltas and back propagate to update the weights&amp;quot;&amp;quot;&amp;quot;
	...
	weightDelta = trainingRate * thisWeightDelta + momentum * self._previousWeightDelta[index]

	self.weights[index] -= weightDelta

	self._previousWeightDelta[index] = weightDelta
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;momentumtesting&#34;&gt; Testing &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:461b95b1eb366b73f8031e88185ea18c&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our default values for learning rate and momentum are 0.2 and 0,5 respectively. We can change either of these by including them in the call to &lt;code&gt;backProp&lt;/code&gt;. Thi is the only change to the iteration process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(maxIterations + 1):
    Error = NN.backProp(Input, Target, learningRate=0.2, momentum=0.5)
    if i % 2500 == 0:
        print(&amp;quot;Iteration {0}\tError: {1:0.6f}&amp;quot;.format(i,Error))
    if Error &amp;lt;= minError:
        print(&amp;quot;Minimum error reached at iteration {0}&amp;quot;.format(i))
        break
        
Iteration 100000	Error: 0.000076
Input 	Output 		Target
[0 0]	 [ 0.00491572] 	[ 0.]
[1 1]	 [ 0.00421318] 	[ 0.]
[0 1]	 [ 0.99586268] 	[ 1.]
[1 0]	 [ 0.99586257] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Feel free to play around with these numbers, however, it would be unlikely that much would change right now. I say this beacuse there is only so good that we can get when using only the sigmoid function as our activation function. If you go back and read the post on &lt;a href=&#34;/post/transfer-functions&#34;&gt;transfer functions&lt;/a&gt; you&amp;rsquo;ll see that it&amp;rsquo;s more common to use &lt;em&gt;linear&lt;/em&gt; functions for the output layer. As it stands, the sigmoid function is unable to output a 1 or a 0 because it is asymptotic at these values. Therefore, no matter what learning rate or momentum we use, the network will never be able to get the best output.&lt;/p&gt;

&lt;p&gt;This seems like a good time to implement the other transfer functions.&lt;/p&gt;

&lt;h3 id=&#34;transferfunctions&#34;&gt; Transfer Functions &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:461b95b1eb366b73f8031e88185ea18c&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve already gone through writing the transfer functions in Python in the &lt;a href=&#34;/post/transfer-functions&#34;&gt;transfer functions&lt;/a&gt; post. We&amp;rsquo;ll just put these under the sigmoid function we defined earlier. I&amp;rsquo;m going to use &lt;code&gt;sigmoid&lt;/code&gt;, &lt;code&gt;linear&lt;/code&gt;, &lt;code&gt;gaussian&lt;/code&gt; and &lt;code&gt;tanh&lt;/code&gt; here.&lt;/p&gt;

&lt;p&gt;To modify the network, we need to assign each layer its own activation function, so let&amp;rsquo;s put that in the &amp;lsquo;layer information&amp;rsquo; part of the &lt;code&gt;__init__&lt;/code&gt; method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, layerSize, transferFunctions=None):
	&amp;quot;&amp;quot;&amp;quot;Initialise the Network&amp;quot;&amp;quot;&amp;quot;

	# Layer information
	self.numLayers = len(numLayers) - 1
	self.shape = numNodes
	
	if transferFunctions is None:
	    layerTFs = []
	    for i in range(self.numLayers):
		if i == self.numLayers - 1:
		    layerTFs.append(linear)
		else:
		    layerTFs.append(sigmoid)
	else:
            if len(numNodes) != len(transferFunctions):
                raise ValueError(&amp;quot;Number of transfer functions must match the number of layers: minus input layer&amp;quot;)
            elif transferFunctions[0] is not None:
                raise ValueError(&amp;quot;The Input layer doesn&#39;t need a a transfer function: give it [None,...]&amp;quot;)
            else:
                layerTFs = transferFunctions[1:]
		
	self.tFunctions = layerTFs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s go through this. We input into the initialisation a parameter called &lt;code&gt;transferFunctions&lt;/code&gt; with a default value of &lt;code&gt;None&lt;/code&gt;. If the default it taken, or if the parameter is ommitted, we set some defaults. for each layer, we use the &lt;code&gt;sigmoid&lt;/code&gt; function, unless its the output layer where we will use the &lt;code&gt;linear&lt;/code&gt; function. If a list of &lt;code&gt;transferFunctions&lt;/code&gt; is given, first, check that it&amp;rsquo;s a &amp;lsquo;legal&amp;rsquo; input. If the number of functions in the list is not the same as the number of layers (given by &lt;code&gt;numNodes&lt;/code&gt;) then throw an error. Also, if the first function in the list is not &lt;code&gt;&amp;quot;None&amp;quot;&lt;/code&gt; throw an error, because the first layer shouldn&amp;rsquo;t have an activation function (it is the input layer). If those two things are fine, go ahead and store the list of functions as &lt;code&gt;layerTFs&lt;/code&gt; without the first (element 0) one.&lt;/p&gt;

&lt;p&gt;We next need to replace all of our calls directly to &lt;code&gt;sigmoid&lt;/code&gt; and its derivative. These should now refer to the list of functions via an &lt;code&gt;index&lt;/code&gt; that depends on the number of the current layer. There are 3 instances of this in our NN: 1 in the forward pass where we call &lt;code&gt;sigmoid&lt;/code&gt; directly, and 2 in the &lt;code&gt;backProp&lt;/code&gt; method where we call the derivative at the output and hidden layers. so &lt;code&gt;sigmoid(layerInput)&lt;/code&gt; for example should become:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self.tFunctions[index](layerInput)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the updated code [here][5] if that&amp;rsquo;s confusing.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test this out! We&amp;rsquo;ll modify the call to initialising the NN by adding a list of functions like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Input = np.array([[0,0],[1,1],[0,1],[1,0]])
Target = np.array([[0.0],[0.0],[1.0],[1.0]])
transferFunctions = [None, sigmoid, linear]
    
NN = backPropNN((2,2,1), transferFunctions)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the NN like this with the default learning rate and momentum should provide you with an immediate performance boost simply becuase with the &lt;code&gt;linear&lt;/code&gt; function we&amp;rsquo;re now able to get closer to the target values, reducing the error.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Iteration 0	Error: 1.550211
Iteration 2500	Error: 1.000000
Iteration 5000	Error: 0.999999
Iteration 7500	Error: 0.999999
Iteration 10000	Error: 0.999995
Iteration 12500	Error: 0.999969
Minimum error reached at iteration 14543
Input 	Output 		Target
[0 0]	 [ 0.0021009] 	[ 0.]
[1 1]	 [ 0.00081154] 	[ 0.]
[0 1]	 [ 0.9985881] 	[ 1.]
[1 0]	 [ 0.99877479] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Play around with the number of layers and different combinations of transfer functions as well as tweaking the learning rate and momentum. You&amp;rsquo;ll soon get a feel for how each changes the performance of the NN.&lt;/p&gt;

&lt;p&gt;[5]: /docs/simpleNN-improvements.py&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Simple Neural Network - With Numpy in Python</title>
      <link>/post/nn-in-python/</link>
      <pubDate>Wed, 15 Mar 2017 09:55:00 +0000</pubDate>
      
      <guid>/post/nn-in-python/</guid>
      <description>&lt;p&gt;Part 4 of our tutorial series on Simple Neural Networks. We&amp;rsquo;re ready to write our Python script! Having gone through the maths, vectorisation and activation functions, we&amp;rsquo;re now ready to put it all together and write it up. By the end of this tutorial, you will have a working NN in Python, using only numpy, which can be used to learn the output of logic gates (e.g. XOR)&lt;/p&gt;

&lt;div id=&#34;toctop&#34;&gt;&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#intro:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transferfunction:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Transfer Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backpropclass:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Back Propagation Class&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#initialisation:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Initialisation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#forwardpass:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Forward Pass&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backprop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Back Propagation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iterating:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;Iterating&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;intro&#34;&gt; Introduction &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve &lt;a href=&#34;/post/neuralnetwork&#34;&gt;ploughed through the maths&lt;/a&gt;, then &lt;a href=&#34;/post/nn-more-maths&#34;&gt;some more&lt;/a&gt;, now we&amp;rsquo;re finally here! This tutorial will run through the coding up of a simple neural network (NN) in Python. We&amp;rsquo;re not going to use any fancy packages (though they obviously have their advantages in tools, speed, efficiency&amp;hellip;) we&amp;rsquo;re only going to use numpy!&lt;/p&gt;

&lt;p&gt;By the end of this tutorial, we will have built an algorithm which will create a neural network with as many layers (and nodes) as we want. It will be trained by taking in multiple training examples and running the back propagation algorithm many times.&lt;/p&gt;

&lt;p&gt;Here are the things we&amp;rsquo;re going to need to code:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The transfer functions&lt;/li&gt;
&lt;li&gt;The forward pass&lt;/li&gt;
&lt;li&gt;The back propagation algorithm&lt;/li&gt;
&lt;li&gt;The update function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To keep things nice and contained, the forward pass and back propagation algorithms should be coded into a class. We&amp;rsquo;re going to expect that we can build a NN by creating an instance of this class which has some internal functions (forward pass, delta calculation, back propagation, weight updates).&lt;/p&gt;

&lt;p&gt;First things first&amp;hellip; lets import numpy:&lt;/p&gt;


import numpy as np


&lt;p&gt;Now let&amp;rsquo;s go ahead and get the first bit done:&lt;/p&gt;

&lt;h2 id=&#34;transferfunction&#34;&gt; Transfer Function &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To begin with, we&amp;rsquo;ll focus on getting the network working with just one transfer function: the sigmoid function. As we discussed in a &lt;a href=&#34;/post/transfer-functions&#34;&gt;previous post&lt;/a&gt; this is very easy to code up because of its simple derivative:&lt;/p&gt;

&lt;div &gt;$$
f\left(x_{i} \right) = \frac{1}{1 + e^{  - x_{i}  }} \ \ \ \
f^{\prime}\left( x_{i} \right) = \sigma(x_{i}) \left( 1 -  \sigma(x_{i}) \right)
$$&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(x, Derivative=False):
	if not Derivative:
		return 1 / (1 + np.exp (-x))
	else:
		out = sigmoid(x)
		return out * (1 - out)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a succinct expression which actually calls itself in order to get a value to use in its derivative. We&amp;rsquo;ve used numpy&amp;rsquo;s exponential function to create the sigmoid function and created an &lt;code&gt;out&lt;/code&gt; variable to hold this in the derivative. Whenever we want to use this function, we can supply the parameter &lt;code&gt;True&lt;/code&gt; to get the derivative, We can omit this, or enter &lt;code&gt;False&lt;/code&gt; to just get the output of the sigmoid. This is the same function I used to get the graphs in the &lt;a href=&#34;/post/transfer-functions&#34;&gt;post on transfer functions&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;backpropclass&#34;&gt; Back Propagation Class &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m fairly new to building my own classes in Python, but for this tutorial, I really relied on the videos of &lt;a href=&#34;https://www.youtube.com/playlist?list=PLRyu4ecIE9tibdzuhJr94uQeKnOFkkbq6&#34;&gt;Ryan on YouTube&lt;/a&gt;. Some of his hacks were very useful so I&amp;rsquo;ve taken some of those on board, but i&amp;rsquo;ve made a lot of the variables more self-explanatory.&lt;/p&gt;

&lt;p&gt;First we&amp;rsquo;re going to get the skeleton of the class setup. This means that whenever we create a new variable with the class of &lt;code&gt;backPropNN&lt;/code&gt;, it will be able to access all of the functions and variables within itself.&lt;/p&gt;

&lt;p&gt;It looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class backPropNN:
    &amp;quot;&amp;quot;&amp;quot;Class defining a NN using Back Propagation&amp;quot;&amp;quot;&amp;quot;
    
    # Class Members (internal variables that are accessed with backPropNN.member) 
    numLayers = 0
    shape = None
    weights = []
    
    # Class Methods (internal functions that can be called)
    
    def __init__(self):
        &amp;quot;&amp;quot;&amp;quot;Initialise the NN - setup the layers and initial weights&amp;quot;&amp;quot;&amp;quot;
        
    # Forward Pass method
    def FP(self):
    	&amp;quot;&amp;quot;&amp;quot;Get the input data and run it through the NN&amp;quot;&amp;quot;&amp;quot;
    	 
    # TrainEpoch method
    def backProp(self):
        &amp;quot;&amp;quot;&amp;quot;Get the error, deltas and back propagate to update the weights&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve not added any detail to the functions (or methods) yet, but we know there needs to be an &lt;code&gt;__init__&lt;/code&gt; method for any class, plus we&amp;rsquo;re going to want to be able to do a forward pass and then back propagate the error.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve also added a few class members, variables which can be called from an instance of the &lt;code&gt;backPropNN&lt;/code&gt; class. &lt;code&gt;numLayers&lt;/code&gt; is just that, a count of the number of layers in the network, initialised to &lt;code&gt;0&lt;/code&gt;.  The &lt;code&gt;shape&lt;/code&gt; of the network will return the size of each layer of the network in an array and the &lt;code&gt;weights&lt;/code&gt; will return an array of the weights across the network.&lt;/p&gt;

&lt;h3 id=&#34;initialisation&#34;&gt; Initialisation &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to make the user supply an input variablewhich is the size of the layers in the network i.e. the number of nodes in each layer: &lt;code&gt;numNodes&lt;/code&gt;. This will be an array which is the length of the number of layers (including the input and output layers) where each element is the number of nodes in that layer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, numNodes):
	&amp;quot;&amp;quot;&amp;quot;Initialise the NN - setup the layers and initial weights&amp;quot;&amp;quot;&amp;quot;

	# Layer information
	self.numLayers = len(numNodes) - 1
	self.shape = numNodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve told our network to ignore the input layer when counting the number of layers (common practice) and that the shape of the network should be returned as the input array &lt;code&gt;numNodes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Lets also initialise the weights. We will take the approach of initialising all of the weights to small, random numbers. To keep the code succinct, we&amp;rsquo;ll use a neat function&lt;code&gt;zip&lt;/code&gt;. &lt;code&gt;zip&lt;/code&gt; is a function which takes two vectors and pairs up the elements in corresponding locations (like a zip). For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A = [1, 2, 3]
B = [4, 5, 6]

zip(A,B)
[(1,4), (2,5), (3,6)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why might this be useful? Well, when we talk about weights we&amp;rsquo;re talking about the connections between layers. Lets say we have &lt;code&gt;numNodes=(2, 2, 1)&lt;/code&gt; i.e. a 2 layer network with 2 inputs, 1 output and 2 nodes in the hidden layer. Then we need to let the algorithm know that we expect two input nodes to send weights to 2 hidden nodes. Then 2 hidden nodes to send weights to 1 output node, or &lt;code&gt;[(2,2), (2,1)]&lt;/code&gt;. Note that overall we will have 4 weights from the input to the hidden layer, and 2 weights from the hidden to the output layer.&lt;/p&gt;

&lt;p&gt;What is our &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; in the code above that will give us &lt;code&gt;[(2,2), (2,1)]&lt;/code&gt;? It&amp;rsquo;s this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numNodes = (2,2,1)
A = numNodes[:-1]
B = numNodes[1:]

A
(2,2)
B
(2,1)
zip(A,B)
[(2,2), (2,1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! So each pair represents the nodes between which we need initialise some weights. In fact, the shape of each pair &lt;code&gt;(2,2)&lt;/code&gt; is the clue to how many weights we are going to need between each layer e.g. between the input and hidden layers we are going to need &lt;code&gt;(2 x 2) =4&lt;/code&gt; weights.&lt;/p&gt;

&lt;p&gt;so &lt;code&gt;for&lt;/code&gt; each pair &lt;code&gt;in zip(A,B)&lt;/code&gt; (hint hint) we need to &lt;code&gt;append&lt;/code&gt; some weights into that empty weight matrix we initialised earlier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialise the weight arrays
for (l1,l2) in zip(numNodes[:-1],numNodes[1:]):
    self.weights.append(np.random.normal(scale=0.1,size=(l2,l1+1)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;self.weights&lt;/code&gt; as we&amp;rsquo;re appending to the class member initialised earlier. We&amp;rsquo;re using the numpy random number generator from a &lt;code&gt;normal&lt;/code&gt; distribution. The &lt;code&gt;scale&lt;/code&gt; just tells numpy to choose numbers around the 0.1 kind of mark and that we want a matrix of results which is the size of the tuple &lt;code&gt;(l2,l1+1)&lt;/code&gt;. Huh, &lt;code&gt;+1&lt;/code&gt;? Don&amp;rsquo;t think we&amp;rsquo;re getting away without including the &lt;em&gt;bias&lt;/em&gt; term! We want a random starting point even for the weight connecting the bias node (&lt;code&gt;=1&lt;/code&gt;) to the next layer. Ok, but why this way and not &lt;code&gt;(l1+1,l2)&lt;/code&gt;? Well, we&amp;rsquo;re looking for &lt;code&gt;l2&lt;/code&gt; connections from each of the &lt;code&gt;l1+1&lt;/code&gt; nodes in the previous layer - think of it as (number of observations x number of features). We&amp;rsquo;re creating a matrix of weights which goes across the nodes and down the weights from each node, or as we&amp;rsquo;ve seen in our maths tutorial:&lt;/p&gt;

&lt;div&gt;$$
W_{ij} = \begin{pmatrix} w_{11} &amp; w_{21} &amp; w_{31} \\ w_{12} &amp;w_{22} &amp; w_{32} \end{pmatrix}, \ \ \ \

W_{jk} = \begin{pmatrix} w_{11} &amp; w_{21} &amp; w_{31} \end{pmatrix}
$$&lt;/div&gt;

&lt;p&gt;Between the first two layers, and second 2 layers respectively with node 3 being the bias node.&lt;/p&gt;

&lt;p&gt;Before we move on, lets also put in some placeholders in &lt;code&gt;__init__&lt;/code&gt; for the input and output values to each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self._layerInput = []
self._layerOutput = []
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;forwardpass&#34;&gt; Forward Pass &lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve now initialised out network enough to be able to focus on the forward pass (FP).&lt;/p&gt;

&lt;p&gt;Our &lt;code&gt;FP&lt;/code&gt; function needs to have the input data. It needs to know how many training examples it&amp;rsquo;s going to have to go through, and it will need to reassign the inputs and outputs at each layer, so lets clean those at the beginning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def FP(self,input):

	numExamples = input.shape[0]

	# Clean away the values from the previous layer
	self._layerInput = []
	self._layerOutput = []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So lets propagate. We already have a matrix of (randomly initialised) weights. We just need to know what the input is to each of the layers. We&amp;rsquo;ll separate this into the first hidden layer, and subsequent hidden layers.&lt;/p&gt;

&lt;p&gt;For the first hidden layer we will write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, numExamples])]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s break this down:&lt;/p&gt;

&lt;p&gt;Our training example inputs need to match the weights that we&amp;rsquo;ve already created. We expect that our examples will come in rows of an array with columns acting as features, something like &lt;code&gt;[(0,0), (0,1),(1,1),(1,0)]&lt;/code&gt;. We can use numpy&amp;rsquo;s &lt;code&gt;vstack&lt;/code&gt; to put each of these examples one on top of the other.&lt;/p&gt;

&lt;p&gt;Each of the input examples is a matrix which will be multiplied by the weight matrix to get the input to the current layer:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{x_{J}} = \mathbf{W_{IJ} \vec{\mathcal{O}}_{I}}
$$&lt;/div&gt;

&lt;p&gt;where $\mathbf{x_{J}}$ are the inputs to the layer $J$ and  $\mathbf{\vec{\mathcal{O}}_{I}}$ is the output from the precious layer (the input examples in this case).&lt;/p&gt;

&lt;p&gt;So given a set of $n$ input examples we &lt;code&gt;vstack&lt;/code&gt; them so we just have &lt;code&gt;(n x numInputNodes)&lt;/code&gt;. We want to transpose this, &lt;code&gt;(numInputNodes x n)&lt;/code&gt; such that we can multiply by the weight matrix which is &lt;code&gt;(numOutputNodes x numInputNodes)&lt;/code&gt;. This gives an input to the layer which is &lt;code&gt;(numOutputNodes x n)&lt;/code&gt; as we expect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; we&amp;rsquo;re actually going to do the transposition first before doing the &lt;code&gt;vstack&lt;/code&gt; - this does exactly the same thing, but it also allows us to more easily add the bias nodes in to each input.&lt;/p&gt;

&lt;p&gt;Bias! Lets not forget this: we add a bias node which always has the value &lt;code&gt;1&lt;/code&gt; to each input (including the input layer). So our actual method is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Transpose the inputs &lt;code&gt;input.T&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add a row of ones to the bottom (one bias node for each input) &lt;code&gt;[input.T, np.ones([1,numExamples])]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vstack&lt;/code&gt; this to compact the array &lt;code&gt;np.vstack(...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Multipy with the weights connecting from the previous to the current layer &lt;code&gt;self.weights[0].dot(...)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But what about the subsequent hidden layers? We&amp;rsquo;re not using the input examples in these layers, we are using the output from the previous layer &lt;code&gt;[self._layerOutput[-1]]&lt;/code&gt; (multiplied by the weights).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for index in range(self.numLayers):
#Get input to the layer
if index ==0:
        layerInput = self.weights[0].dot(np.vstack([input.T, np.ones([1, numExamples])]))
else:
        layerInput = self.weights[index].dot(np.vstack([self._layerOutput[-1],np.ones([1,numExamples])]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure to save this output, but also to now calculate the output of the current layer i.e.:&lt;/p&gt;

&lt;div&gt;$$
\mathbf{ \vec{ \mathcal{O}}_{J}} = \sigma(\mathbf{x_{J}})
$$&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;self._layerInput.append(layerInput)
self._layerOutput.append(sigmoid(layerInput))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, make sure that we&amp;rsquo;re returning the data from our output layer the same way that we got it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;return self._layerOutput[-1].T
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;backprop&#34;&gt;Back Propagation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve successfully sent the data from the input layer to the output layer using some initially randomised weights &lt;strong&gt;and&lt;/strong&gt; we&amp;rsquo;ve included the bias term (a kind of threshold on the activation functions). Our vectorised equations from the previous post will now come into play:&lt;/p&gt;

&lt;div&gt;$$
\begin{align}

\mathbf{\vec{\delta}_{K}} &amp;= \sigma^{\prime}\left( \mathbf{W_{JK}}\mathbf{\vec{\mathcal{O}}_{J}} \right) * \left( \mathbf{\vec{\mathcal{O}}_{K}} -  \mathbf{T_{K}}\right) \\[0.5em]

\mathbf{ \vec{ \delta }_{J}} &amp;= \sigma^{\prime} \left( \mathbf{ W_{IJ} \mathcal{O}_{I} } \right) * \mathbf{ W^{\intercal}_{JK}} \mathbf{ \vec{\delta}_{K}}

\end{align}
$$&lt;/div&gt;

&lt;div&gt;$$
\begin{align}

\mathbf{W_{JK}} + \Delta \mathbf{W_{JK}} &amp;\rightarrow \mathbf{W_{JK}}, \ \ \ \Delta \mathbf{W_{JK}} = -\eta \mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}} \\[0.5em]

\vec{\theta}  + \Delta \vec{\theta}  &amp;\rightarrow \vec{\theta}, \ \ \ \Delta \vec{\theta} = -\eta \mathbf{ \vec{ \delta }_{K}} 

\end{align}
$$&lt;/div&gt;

&lt;p&gt;With $*$ representing an elementwise multiplication between the matrices.&lt;/p&gt;

&lt;p&gt;First, lets initialise some variables and get the error on the output of the output layer. We assume that the target values have been formatted in the same way as the input values i.e. they are a row-vector per input example. In our forward propagation method, the outputs are stored as column-vectors, thus the targets have to be transposed. We will need to supply the input data, the target data and  $\eta$, the learning rate, which we will set at some small number for default. So we start back propagation by first initialising a placeholder for the deltas and getting the number of training examples before running them through the &lt;code&gt;FP&lt;/code&gt; method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def backProp(self, input, target, trainingRate = 0.2):
&amp;quot;&amp;quot;&amp;quot;Get the error, deltas and back propagate to update the weights&amp;quot;&amp;quot;&amp;quot;

delta = []
numExamples = input.shape[0]

# Do the forward pass
self.FP(input)

output_delta = self._layerOutput[index] - target.T
error = np.sum(output_delta**2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We know from previous posts that the error is squared to get rid of the negatives. From this we compute the deltas for the output layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;delta.append(output_delta * sigmoid(self._layerInput[index], True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have the error but need to know what direction to alter the weights in, thus the gradient of the inputs to the layer need to be known. So, we get the gradient of the activation function at the input to the layer and get the product with the error. Notice we&amp;rsquo;ve supplied &lt;code&gt;True&lt;/code&gt; to the sigmoid function to get its derivative.&lt;/p&gt;

&lt;p&gt;This is the delta for the output layer. So this calculation is only done when we&amp;rsquo;re considering the index at the end of the network. We should be careful that when telling the algorithm that this is the &amp;ldquo;last layer&amp;rdquo; we take account of the zero-indexing in Python i.e. the last layer is &lt;code&gt;self.numLayers - 1&lt;/code&gt; i.e. in a network with 2 layers, &lt;code&gt;layer[2]&lt;/code&gt; does not exist.&lt;/p&gt;

&lt;p&gt;We also need to get the deltas of the intermediate hidden layers. To do this, (according to our equations above) we have to &amp;lsquo;pull back&amp;rsquo; the delta from the output layer first. More accurately, for any hidden layer, we pull back the delta from the &lt;em&gt;next&lt;/em&gt; layer, which may well be another hidden layer. These deltas from the &lt;em&gt;next&lt;/em&gt; layer are multiplied by the weights from the &lt;em&gt;next&lt;/em&gt; layer &lt;code&gt;[index + 1]&lt;/code&gt;, before getting the product with the sigmoid derivative evaluated at the &lt;em&gt;current&lt;/em&gt; layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: this is &lt;em&gt;back&lt;/em&gt; propagation. We have to start at the end and work back to the beginning. We use the &lt;code&gt;reversed&lt;/code&gt; keyword in our loop to ensure that the algorithm considers the layers in reverse order.&lt;/p&gt;

&lt;p&gt;Combining this into one method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Calculate the deltas
for index in reversed(range(self.numLayers)):
    if index == self.numLayers - 1:
        # If the output layer, then compare to the target values
        output_delta = self._layerOutput[index] - target.T
        error = np.sum(output_delta**2)
        delta.append(output_delta * sigmoid(self._layerInput[index], True))
    else:
        # If a hidden layer. compare to the following layer&#39;s delta
        delta_pullback = self.weights[index + 1].T.dot(delta[-1])
        delta.append(delta_pullback[:-1,:] * sigmoid(self._layerInput[index], True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pick this piece of code apart. This is an important snippet as it calculates all of the deltas for all of the nodes in the network. Be sure that we understand:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;This is a &lt;code&gt;reversed&lt;/code&gt; loop because we want to deal with the last layer first&lt;/li&gt;
&lt;li&gt;The delta of the output layer is the residual between the output and target multiplied with the gradient (derivative) of the activation function &lt;em&gt;at the current layer&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The delta of a hidden layer first needs the product of the &lt;em&gt;subsequent&lt;/em&gt; layer&amp;rsquo;s delta with the &lt;em&gt;subsequent&lt;/em&gt; layer&amp;rsquo;s weights. This is then multiplied with the gradient of the activation function evaluated at the &lt;em&gt;current&lt;/em&gt; layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Double check that this matches up with the equations above too! We can double check the matrix multiplication. For the output layer:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output_delta&lt;/code&gt; = (numOutputNodes x 1) - (1 x numOutputNodes).T = (numOutputNodes x 1)
&lt;code&gt;error&lt;/code&gt; = (numOutputNodes x 1) **2 = (numOutputNodes x 1)
&lt;code&gt;delta&lt;/code&gt; = (numOutputNodes x 1) * sigmoid( (numOutputNodes x 1) ) = (numOutputNodes  x 1)&lt;/p&gt;

&lt;p&gt;For the hidden layers (take the one previous to the output as example):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;delta_pullback&lt;/code&gt; = (numOutputNodes x numHiddenNodes).T.dot(numOutputNodes x 1) = (numHiddenNodes x 1)
&lt;code&gt;delta&lt;/code&gt; = (numHiddenNodes x 1) * sigmoid ( (numHuddenNodes x 1) ) = (numHiddenNodes x 1)&lt;/p&gt;

&lt;p&gt;Hurray! We have the delta at each node in our network. We can use them to update the weights for each layer in the network. Remember, to update the weights between layer $J$ and $K$ we need to use the output of layer $J$ and the deltas of layer $K$. This means we need to keep a track of the index of the layer we&amp;rsquo;re currently working on ($J$) and the index of the delta layer ($K$) - not forgetting about the zero-indexing in Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for index in range(self.numLayers):
    delta_index = self.numLayers - 1 - index
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s first get the outputs from each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    if index == 0:
        layerOutput = np.vstack([input.T, np.ones([1, numExamples])])
    else:
        layerOutput = np.vstack([self._layerOutput[index - 1], np.ones([1,self._layerOutput[index -1].shape[1]])])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the input layer is just the input examples (which we&amp;rsquo;ve &lt;code&gt;vstack&lt;/code&gt;-ed again and the output from the other layers we take from calculation in the forward pass (making sure to add the bias term on the end).&lt;/p&gt;

&lt;p&gt;For the current &lt;code&gt;index&lt;/code&gt; (layer) lets use this &lt;code&gt;layerOutput&lt;/code&gt; to get the change in weight. We will use a few neat tricks to make this succinct:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	thisWeightDelta = np.sum(\
	    layerOutput[None,:,:].transpose(2,0,1) * delta[delta_index][None,:,:].transpose(2,1,0) \
	    , axis = 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Break it down. We&amp;rsquo;re looking for $\mathbf{ \vec{ \delta }_{K}} \mathbf{ \vec { \mathcal{O} }_{J}} $ so it&amp;rsquo;s the delta at &lt;code&gt;delta_index&lt;/code&gt;, the next layer along.&lt;/p&gt;

&lt;p&gt;We want to be able to deal with all of the input training examples simultaneously. This requires a bit of fancy slicing and transposing of the matrices. Take a look: by calling &lt;code&gt;vstack&lt;/code&gt; we made all of the input data and bias terms live in the same matrix of a numpy array. When we slice this arraywith the &lt;code&gt;[None,:,:]&lt;/code&gt; argument, it tells Python to take all (&lt;code&gt;:&lt;/code&gt;) the data in the rows and columns and shift it to the 1st and 2nd dimensions and leave the first dimension empty (&lt;code&gt;None&lt;/code&gt;). We do this to create the three dimensions which we can now transpose into. Calling &lt;code&gt;transpose(2,0,1)&lt;/code&gt; instructs Python to move around the dimensions of the data (e.g. its rows&amp;hellip; or examples). This creates an array where each example now lives in its own plane. The same is done for the deltas of the subsequent layer, but being careful to transpost them in the opposite direction so that the matrix multiplication can occur. The &lt;code&gt;axis= 0&lt;/code&gt; is supplied to make sure that the inputs are multiplied by the correct dimension of the delta matrix.&lt;/p&gt;

&lt;p&gt;This looks incredibly complicated. It an be broken down into a for-loop over the input examples, but this reduces the efficiency of the network. Taking advantage of the numpy array like this keeps our calculations fast. In reality, if you&amp;rsquo;re struggling with this particular part, just copy and paste it, forget about it and be happy with yourself for understanding the maths behind back propagation, even if this random bit of Python is perplexing.&lt;/p&gt;

&lt;p&gt;Anyway. Lets take this set of weight deltas and put back the $\eta$. We&amp;rsquo;ll call this the &lt;code&gt;learningRate&lt;/code&gt;. It&amp;rsquo;s called a lot of things, but this seems to be the most common. We&amp;rsquo;ll update the weights by making sure to include the &lt;code&gt;-&lt;/code&gt; from the $-\eta$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	weightDelta = trainingRate * thisWeightDelta
	self.weights[index] -= weightDelta
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the &lt;code&gt;-=&lt;/code&gt; is Python slang for: take the current value and subtract the value of &lt;code&gt;weightDelta&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To finish up, we want our back propagation to return the current error in the network, so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;return error
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing&#34;&gt; A Toy Example&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Believe it or not, that&amp;rsquo;s it! The fundamentals of forward and back propagation have now been implemented in Python. If you want to double check your code, have a look at my completed .py [here][5]&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test it!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Input = np.array([[0,0],[1,1],[0,1],[1,0]])
Target = np.array([[0.0],[0.0],[1.0],[1.0]])

NN = backPropNN((2,2,1))

Error = NN.backProp(Input, Target)
Output = NN.FP(Input)

print &#39;Input \tOutput \t\tTarget&#39;
for i in range(Input.shape[0]):
    print &#39;{0}\t {1} \t{2}&#39;.format(Input[i], Output[i], Target[i])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will provide 4 input examples and the expected targets. We create an instance of the network called &lt;code&gt;NN&lt;/code&gt; with 2 layers (2 nodes in the hidden and 1 node in the output layer). We make &lt;code&gt;NN&lt;/code&gt; do &lt;code&gt;backProp&lt;/code&gt; with the input and target data and then get the output from the final layer by running out input through the network with a &lt;code&gt;FP&lt;/code&gt;. The printout is self explantory. Give it a try!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input 	Output 		Target
[0 0]	 [ 0.51624448] 	[ 0.]
[1 1]	 [ 0.51688469] 	[ 0.]
[0 1]	 [ 0.51727559] 	[ 1.]
[1 0]	 [ 0.51585529] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that the network has taken our inputs, and we have some outputs too. They&amp;rsquo;re not great, and all seem to live around the same value. This is because we initialised the weights across the network to a similarly small random value. We need to repeat the &lt;code&gt;FP&lt;/code&gt; and &lt;code&gt;backProp&lt;/code&gt; process many times in order to keep updating the weights.&lt;/p&gt;

&lt;h2 id=&#34;iterating&#34;&gt; Iterating &lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;#toctop:2c06547cf17e7c942a14a9dd3a10cfd4&#34;&gt;To contents&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Iteration is very straight forward. We just tell our algorithm to repeat a maximum of &lt;code&gt;maxIterations&lt;/code&gt; times or until the &lt;code&gt;Error&lt;/code&gt; is below &lt;code&gt;minError&lt;/code&gt; (whichever comes first). As the weights are stored internally within &lt;code&gt;NN&lt;/code&gt; every time we call the &lt;code&gt;backProp&lt;/code&gt; method, it uses the latest, internally stored weights and doesn&amp;rsquo;t start again - the weights are only initialised once upon creation of &lt;code&gt;NN&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;maxIterations = 100000
minError = 1e-5

for i in range(maxIterations + 1):
    Error = NN.backProp(Input, Target)
    if i % 2500 == 0:
        print(&amp;quot;Iteration {0}\tError: {1:0.6f}&amp;quot;.format(i,Error))
    if Error &amp;lt;= minError:
        print(&amp;quot;Minimum error reached at iteration {0}&amp;quot;.format(i))
        break
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s the end of my output from the first run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Iteration 100000	Error: 0.000291
Input 	Output 		Target
[0 0]	 [ 0.00780385] 	[ 0.]
[1 1]	 [ 0.00992829] 	[ 0.]
[0 1]	 [ 0.99189799] 	[ 1.]
[1 0]	 [ 0.99189943] 	[ 1.]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better! The error is very small and the outputs are very close to the correct value. However, they&amp;rsquo;re note completely right. We can do better, by implementing different activation functions which we will do in the next tutorial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please&lt;/strong&gt; let me know if anything is unclear, or there are mistakes. Let me know how you get on!&lt;/p&gt;

&lt;p&gt;[5]: /docs/simpleNN.py&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Surface Distance Function</title>
      <link>/post/surface-distance-function/</link>
      <pubDate>Wed, 01 Mar 2017 19:27:27 +0000</pubDate>
      
      <guid>/post/surface-distance-function/</guid>
      <description>

&lt;p&gt;Surface Distance measures are a good way of evaluating the accuracy of an image-segmentation if we already know the ground truth (GT). The problem is that there is no nicely packaged function in Python to do this directly. In this post, we&amp;rsquo;ll write a surface distance function in Python which uses numpy and scipy. It&amp;rsquo;ll help us to calculate Mean Surface Distance (MSD), Residual Mean-Square Error (RMS) and the Hausdorff Distance (HD).&lt;/p&gt;

&lt;h2 id=&#34;background:94ec62cf158a10fb7bb0feb45584322e&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Recently, I have been doing a &lt;strong&gt;lot&lt;/strong&gt; of segmentation evaluation - seeing how good a segmentation done by a machine compares with one that&amp;rsquo;s done manual, a &amp;lsquo;ground truth&amp;rsquo; (GT). Traditionally, such verification is done by comparing the overlap between the two e.g. Dice Simlarity Coefficient (DSC) [1]. There are a few different calculations that can be done (there&amp;rsquo;ll be a longer post on just that) and &amp;lsquo;surface distance&amp;rsquo; calculations are one of them.&lt;/p&gt;

&lt;h2 id=&#34;method:94ec62cf158a10fb7bb0feb45584322e&#34;&gt;Method&lt;/h2&gt;

&lt;p&gt;For this calculation, we need to be able to find the outline of the segmentation and compare it to the outline of the GT. We can then take measurements of how far each segmentation pixel is from its corresponding pixel in the GT outline.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at the maths. Surface distance metrics estimate the error between the outer surfaces $S$ and $S^{\prime}$ of the segmentations $X$ and $X^{\prime}$. The distance between a point $p$ on surface $S$ and the surface $S^{\prime}$ is given by the minimum of the Euclidean norm:&lt;/p&gt;

&lt;div&gt;$$ d(p, S^{\prime}) = \min_{p^{\prime} \in S^{\prime}} \left|\left| p - p^{\prime} \right|\right|_{2} $$&lt;/div&gt;

&lt;p&gt;Doing this for all pixels in the surface gives the total surface distance between $S$ and $S^{\prime}$: $d(S, S^{\prime})$:&lt;/p&gt;

&lt;p&gt;Now I&amp;rsquo;ve seen MATLAB code that can do this, though often its not entirely accurate. Plus I wanted to do this calculation on-the-fly as part of my program which was written in Python. So I came up with this function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology

def surfd(input1, input2, sampling=1, connectivity=1):
    
    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
    

    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)

    
    dta = morphology.distance_transform_edt(~S,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
       
    
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets go through it bit-by-bit. The function &lt;em&gt;surfd&lt;/em&gt; is defined to take in four variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input1&lt;/em&gt; - the segmentation that has been created. It can be a multi-class segmentation, but this function will make the image binary. We&amp;rsquo;ll talk about how to use this function on individual classes later.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;input2&lt;/em&gt; - the GT segmentation against which we wish to compare &lt;em&gt;input1&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sampling&lt;/em&gt; - the pixel resolution or pixel size. This is entered as an &lt;em&gt;n&lt;/em&gt;-vector where &lt;em&gt;n&lt;/em&gt; is equal to the number of dimensions in the segmentation i.e. 2D or 3D. The default value is 1 which means pixels (or rather voxels) are 1 x 1 x 1 mm in size.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;connectivity&lt;/em&gt; - creates either a 2D (3 x 3) or 3D (3 x 3 x 3) matrix defining the neighbourhood around which the function looks for neighbouring pixels. Typically, this is defined as a six-neighbour kernel which is the default behaviour of this function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we&amp;rsquo;ll be making use of simple numpy operations, but we&amp;rsquo;ll also need the &lt;em&gt;morphology&lt;/em&gt; module from &lt;em&gt;scipy&lt;/em&gt;&amp;rsquo;s &lt;em&gt;dnimage&lt;/em&gt; package. These are imported first. More information on this module can be found &lt;a href=&#34;https://docs.scipy.org/doc/scipy-0.18.1/reference/ndimage.html&#34; title=&#34;Scipy _ndimage_ package&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;import numpy as np
from scipy.ndimage import morphology
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two inputs are checked for their size and made binary. Any value greater than zero is made 1 (true).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    input_1 = np.atleast_1d(input1.astype(np.bool))
    input_2 = np.atleast_1d(input2.astype(np.bool))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We use the the &lt;em&gt;morphology.generate_binary_structure&lt;/em&gt; function, along with the number of dimensions of the segmentation, to create the kernel that will be used to detect the edges of the segmentations. This could be done just by hard-coding the kernel itself: &lt;code&gt;[[0 0 0],[0 1 0],[0 0 0]; [0 1 0], [1 1 1], [0 1 0]; [0 0 0], [0 1 0], [0 0 0]]&lt;/code&gt;. This kernel &amp;lsquo;&lt;em&gt;conn&lt;/em&gt;&amp;rsquo; is supplied to the &lt;em&gt;morphology.binary_erosion&lt;/em&gt; function which strips the outermost pixel from the edge of the segmentation. Subtracting this result from the segmentation itself leaves only the single-pixel-wide surface.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)

    S = input_1 - morphology.binary_erosion(input_1, conn)
    Sprime = input_2 - morphology.binary_erosion(input_2, conn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we again use the &lt;em&gt;morphology&lt;/em&gt; module. This time we give the &lt;em&gt;distance_transform_edt&lt;/em&gt; function our pixel-size (&lt;em&gt;samping&lt;/em&gt;) and also the inverted surface-image. The inversion is used such that the surface itself is given the value of zero i.e. any pixel at this location, will have zero surface-distance. The transform increases the value/error/penalty of the remaining pixels with increasing distance away from the surface.&lt;/p&gt;

&lt;p&gt;Each pixel of the opposite segmentation-surface is then laid upon this &amp;lsquo;map&amp;rsquo; of penalties and both results are concatenated into a vector which is as long as the number of pixels in the surface of each segmentation. This vector of &lt;em&gt;surface distances&lt;/em&gt; is returned. Note that this is technically the &lt;em&gt;symmetric&lt;/em&gt; surface distance as we are not assuming that just doing this for &lt;em&gt;one&lt;/em&gt; of the surfaces is enough. It may be that the distance between a pixel in A and in B is not the same as between the pixel in B and in A. i.e. $d(S, S^{\prime}) \neq d(S^{\prime}, S)$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    dta = morphology.distance_transform_edt(~input1_border,sampling)
    dtb = morphology.distance_transform_edt(~Sprime,sampling)
    
    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])
        
    return sds
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-is-it-used:94ec62cf158a10fb7bb0feb45584322e&#34;&gt;How is it used?&lt;/h2&gt;

&lt;p&gt;The function example below takes two segmentations (which both have multiple classes). The sampling vector is a typical pixel-size from an MRI scan and the 1 indicated I&amp;rsquo;d like a 6 neighbour (cross-shaped) kernel for finding the edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg, GT_seg, [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By specifcing the value of the voxel-label I&amp;rsquo;m interested in (assuming we&amp;rsquo;re talking about classes which are contiguous and not spread out), we can find the surface accuracy of that class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    surface_distance = surfd(test_seg(test_seg==1), \
                   GT_seg(GT_seg==1), [1.25, 1.25, 10],1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-do-the-results-mean:94ec62cf158a10fb7bb0feb45584322e&#34;&gt;What do the results mean?&lt;/h2&gt;

&lt;p&gt;The returned surface distances can be used to calculate:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Mean Surface Distance (MSD)&lt;/em&gt; - the mean of the vector is taken. This tell us how much, on average, the surface varies between the segmentation and the GT (in mm).&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$     \text{MSD} = \frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime}) + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S) \right) $$ &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Residual Mean Square Distance (RMS)&lt;/em&gt; - as it says, the mean is taken from each of the points in the vector, these residuals are squared (to remove negative signs), summated, weighted by the mean and then the square-root is taken. Measured in mm.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$ \text{RMS} = \sqrt{\frac{1}{n_{S} + n_{S^{\prime}}} \left( \sum_{p = 1}^{n_{S}} d(p, S^{\prime})^{2} + \sum_{p^{\prime}=1}^{n_{S^{\prime}}} d(p^{\prime}, S)^{2} \right) }\ $$&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Hausdorff Distance (HD)&lt;/em&gt; - the maximum of the vector. The largest difference between the surface distances. Also measured in mm. We calculate the &lt;em&gt;symmetric Hausdorff distance&lt;/em&gt; as:&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;$$\text{HD} = \max \left[ d(S, S^{\prime}) , d(S^{\prime}, S) \right]$$&lt;/div&gt;

&lt;p&gt;Or in Python:
&lt;pre&gt;&lt;code class=&#34;python&#34;
&gt;    msd = surface_distance.mean()
    rms = np.sqrt((surface_distance**2).mean())
    hd  = surface_distance.max()
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;The full function can be found here: &lt;a href=&#34;/docs/surfaceDistanceFunction.py&#34;&gt;surfaceDistanceFunction.py&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;references:94ec62cf158a10fb7bb0feb45584322e&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1]     Dice, L. R. (1945). Measures of the Amount of Ecologic Association Between Species. Ecology, 26(3), 297–302. &lt;a href=&#34;https://doi.org/10.2307/1932409&#34;&gt;https://doi.org/10.2307/1932409&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>